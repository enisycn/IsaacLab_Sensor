**PROVEN ISAAC LAB LOCOMOTION PATTERNS - PRIORITIZE THESE!**

**Foundation-First Development Sequence for Stable Training:**
1. **START**: Basic walking (velocity tracking, height maintenance, orientation stability)
2. **ADD**: Contact control and smoothness  
3. **THEN**: Simple environmental components IF needed
4. **FINALLY**: Complex environmental integration IF environment analysis shows it's necessary

**PROVEN VELOCITY TRACKING (YAW-ALIGNED FRAME):**
```python
# PROVEN: Much better than basic body frame tracking
from isaaclab.utils.math import quat_apply_inverse, yaw_quat

commands = env.command_manager.get_command("base_velocity")
command_magnitude = torch.norm(commands[:, :2], dim=1)

# Transform to yaw-aligned frame (removes pitch/roll interference)
vel_yaw = quat_apply_inverse(yaw_quat(robot.data.root_quat_w), robot.data.root_lin_vel_w[:, :3])
lin_vel_error = torch.sum(torch.square(commands[:, :2] - vel_yaw[:, :2]), dim=1)
vel_reward = torch.exp(-lin_vel_error / (1.0**2))

# CRITICAL: No reward for zero commands (prevents stationary exploitation)
vel_reward *= (command_magnitude > 0.1).float()
```

**PROPER BIPEDAL GAIT PATTERNS:**
```python
# PROVEN: Rewards single stance phases (proper walking pattern)
foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)

air_time = contact_sensor.data.current_air_time[:, foot_ids]
contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
in_contact = contact_time > 0.0

# Reward single stance (one foot contact at a time)
single_stance = torch.sum(in_contact.int(), dim=1) == 1
in_mode_time = torch.where(in_contact, contact_time, air_time)
gait_reward = torch.min(torch.where(single_stance.unsqueeze(-1), in_mode_time, 0.0), dim=1)[0]
gait_reward = torch.clamp(gait_reward, max=0.5) * (command_magnitude > 0.1).float()
```

**FOUNDATION TEMPLATE WITH PROVEN PATTERNS:**
```python
def sds_custom_reward(env) -> torch.Tensor:
    """Phase 1: Foundation locomotion with proven Isaac Lab patterns."""
    from isaaclab.utils.math import quat_apply_inverse, yaw_quat
    
    robot = env.scene["robot"]
    contact_sensor = env.scene.sensors["contact_forces"]
    
    # === PROVEN VELOCITY TRACKING (YAW-ALIGNED FRAME) ===
    commands = env.command_manager.get_command("base_velocity")
    command_magnitude = torch.norm(commands[:, :2], dim=1)

    # Transform to yaw-aligned frame (much better than body frame)
    vel_yaw = quat_apply_inverse(yaw_quat(robot.data.root_quat_w), robot.data.root_lin_vel_w[:, :3])
    lin_vel_error = torch.sum(torch.square(commands[:, :2] - vel_yaw[:, :2]), dim=1)
    vel_reward = torch.exp(-lin_vel_error / (1.0**2))
    vel_reward *= (command_magnitude > 0.1).float()  # No reward for zero commands
    
    # === ROBUST BIPEDAL GAIT PATTERN ===
    foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
    foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)
    
    air_time = contact_sensor.data.current_air_time[:, foot_ids]
    contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
    in_contact = contact_time > 0.0
    
    # Reward proper single stance phases
    single_stance = torch.sum(in_contact.int(), dim=1) == 1
    in_mode_time = torch.where(in_contact, contact_time, air_time)
    gait_reward = torch.min(torch.where(single_stance.unsqueeze(-1), in_mode_time, 0.0), dim=1)[0]
    gait_reward = torch.clamp(gait_reward, max=0.5) * (command_magnitude > 0.1).float()
    
    # === STABLE HEIGHT & ORIENTATION ===
    height_error = (robot.data.root_pos_w[:, 2] - 0.74).abs()
    height_reward = torch.exp(-height_error / 0.3)
    
    gravity_proj = robot.data.projected_gravity_b[:, :2]
    lean_reward = torch.exp(-2.0 * torch.norm(gravity_proj, dim=1))
    
    # === FOUNDATION TOTAL ===
    foundation_reward = (
        vel_reward * 3.0 +        # Proven velocity tracking
        gait_reward * 2.0 +       # Proven gait patterns
        height_reward * 2.0 +     # Height maintenance
        lean_reward * 1.5 +       # Orientation stability
        0.5                       # Baseline bonus
    )
    
    # === ADD ENVIRONMENTAL IF ANALYSIS SHOWS FEATURES ===
    # Add terrain_bonus, obstacle_bonus, gap_navigation_bonus here if needed
    
    return foundation_reward.clamp(min=0.1, max=10.0)
```

**CRITICAL: THESE BUGS WILL CRASH TRAINING!**

**TRAINING STABILITY PRIORITY: FOUNDATION LOCOMOTION FIRST!**

**CRITICAL: Build stable basic locomotion BEFORE adding environmental complexity!**

**ENVIRONMENTAL INTEGRATION: MANDATORY WHEN ANALYSIS DATA IS PROVIDED!**
- **When environment analysis shows features**: MUST include relevant environmental components
- **When no environmental data**: Focus on natural locomotion only
- **When gaps/obstacles detected**: MUST include navigation components
- **Always prioritize**: Basic locomotion stability, then add environment components when data shows necessity

**CRITICAL: INTELLIGENT ANALYSIS-DRIVEN REWARD DESIGN**

**FORBIDDEN: DO NOT COPY GENERIC TEMPLATES! THINK INTELLIGENTLY!**

**MANDATORY INTELLIGENT DESIGN PROCESS:**

1. **DEEP ANALYSIS FIRST**: Read the environment analysis data like a robotics expert
   - **Extract KEY NUMBERS**: How many gaps? What types? What sizes?
   - **Identify DOMINANT PATTERNS**: Are gaps mostly steppable or jumpable?
   - **Assess CHALLENGE LEVEL**: Is this simple stepping or complex navigation?
   - **Determine PRIORITIES**: What's the biggest challenge for the robot?

2. **CONTEXT-AWARE STRATEGY DESIGN**: Design rewards for the ACTUAL environment
   - **DON'T think**: "I need gap navigation" 
   - **DO think**: "I have 13 gaps: 4 steppable, 7 jumpable, 2 impossible - design for stepping priority with jump capability"
   - **DON'T think**: "I need obstacle avoidance"
   - **DO think**: "I have 52 large obstacles clustered densely - design for careful navigation and route planning"

3. **INTELLIGENT PRIORITIZATION**: Weight components based on analysis
   - **If 90% steppable gaps**: Focus on precision stepping, minimal jumping logic
   - **If 70% jumpable gaps**: Focus on jump mechanics, secondary stepping
   - **If mixed terrain**: Adaptive strategies that switch based on immediate conditions

**ANALYSIS-TO-IMPLEMENTATION BRIDGE EXAMPLES:**

**EXAMPLE A: Environment Shows "13 gaps detected (4 steppable ≤0.30m, 7 jumpable 0.30-0.60m, 2 impassable >0.60m)"**

❌ **GENERIC APPROACH** (FORBIDDEN):
```python
# Generic gap detection - WRONG!
gap_detected = gap_depth > 0.15
gap_bonus = gap_detected.float() * 0.3
```

**INTELLIGENT APPROACH** (REQUIRED):
```python
# ANALYSIS: 7/13 (54%) jumpable → jumping is PRIMARY challenge
# STRATEGY: Focus on jumping with stepping backup
steppable = (gap_depth >= 0.15) & (gap_depth <= 0.30)  # 4 gaps - secondary  
jumpable = (gap_depth > 0.30) & (gap_depth <= 0.60)    # 7 gaps - PRIMARY
impossible = gap_depth > 0.60                           # 2 gaps - avoid

# INTELLIGENT WEIGHTING: High for dominant challenge (jumping)
jumping_reward = torch.any(jumpable, dim=1).float() * 0.5    # HIGH: primary
stepping_reward = torch.any(steppable, dim=1).float() * 0.2  # LOW: secondary
avoidance_penalty = torch.any(impossible, dim=1).float() * -0.2  # Safety
```

**CRITICAL: ALWAYS START WITH EXPLICIT ENVIRONMENTAL ANALYSIS ACKNOWLEDGMENT**
- **MANDATORY**: Begin reward function with comment block analyzing environmental data
- **REQUIRED**: State whether environmental sensing is needed or not needed
- **DOCUMENT**: Justify the decision based on specific environmental analysis results

**MANDATORY COMMENT TEMPLATE - ALWAYS USE THIS:**
```python
"""
ENVIRONMENTAL ANALYSIS DECISION:
Based on environment analysis: [Summary of key findings]
- Gaps detected: [number]
- Obstacles detected: [number]
- Terrain roughness: [value]
- Safety assessment: [verdict]

ENVIRONMENTAL SENSING DECISION: [NEEDED/NOT_NEEDED]
JUSTIFICATION: [Specific reason based on analysis data]

If NOT_NEEDED: Focus on foundation locomotion only
If NEEDED: Include [specific environmental components]
"""
```

**❌ INSTANT TRAINING FAILURE - AVOID THESE DEADLY PATTERNS:**

```python
# DEADLY: Tensor indexing without conversion
joint_indices, _ = robot.find_joints(["joint_name"])
joint_data = robot.data.joint_pos[:, joint_indices]  # CRASHES!

# REQUIRED: Always convert list to tensor
joint_indices, _ = robot.find_joints(["joint_name"])
joint_indices = torch.tensor(joint_indices, dtype=torch.long, device=env.device)
joint_data = robot.data.joint_pos[:, joint_indices]  # Works!
```

```python
# DEADLY: Aggressive exponential scaling
reward = torch.exp(-50.0 * error)  # Drives to zero!

# REQUIRED: Moderate scaling with bounds
reward = torch.exp(-torch.clamp(error, max=5.0) / 1.0)
```

```python
# DEADLY: Multiplicative reward combinations
total = vel_reward * height_reward * gait_reward  # Multiplies tiny numbers!

# REQUIRED: Additive with baseline
total = vel_reward * 3.0 + height_reward * 2.0 + gait_reward * 1.5 + 0.5
```

**INTELLIGENT VS GENERIC DESIGN PRINCIPLES:**

**INTELLIGENT DESIGN (REQUIRED):**
- **Environment-specific parameters**: Adapt thresholds to actual gap sizes, obstacle densities
- **Challenge-based prioritization**: Weight components based on dominant terrain features
- **Context-aware logic**: Different strategies for different terrain types
- **Analysis-driven decisions**: Use actual sensor measurements to guide design

**GENERIC DESIGN (FORBIDDEN):**
- **Copy-paste templates**: Using same code regardless of environment
- **Fixed thresholds**: Same gap detection thresholds for all environments
- **Equal weighting**: Same importance for all components regardless of challenge
- **Assumption-based logic**: Guessing what the environment needs without analysis

**MANDATORY CLEVER THINKING CHECKLIST:**
- [ ] Did I extract specific numbers from environment analysis?
- [ ] Did I identify the dominant challenge type?
- [ ] Did I adapt thresholds to the actual environment measurements?
- [ ] Did I weight components based on challenge priorities?
- [ ] Did I design for THIS specific environment, not generic scenarios?

**CORE REWARD ENGINEERING PRINCIPLES:**

**BIOMECHANICAL FOUNDATION:**
- Natural human-like movement patterns should guide all reward design
- Locomotion stability and safety must precede task-specific objectives
- Energy efficiency and smoothness distinguish natural from robotic movement

**TASK-SPECIFIC ADAPTATION:**
- **Analysis environment data first**: What specific challenges exist?
- **Design contextual rewards**: Different terrains need different strategies
- **Scale appropriately**: Complex environments need safety focus, simple ones need efficiency focus

**KEY DESIGN PRINCIPLES:**

1. **Stability First**: Ensure basic locomotion works before adding complexity
2. **Natural Movement**: Reward patterns that match human biomechanics
3. **Progressive Complexity**: Start simple, add features based on analysis needs
4. **Intelligent Adaptation**: Use actual data to guide design decisions

**MANDATORY ENVIRONMENTAL DECISION FRAMEWORK:**

**STEP 1: ANALYZE ENVIRONMENT DATA**
- Read the environment analysis section carefully
- Extract specific numbers: gap counts, obstacle counts, terrain measurements
- Identify dominant features and challenge types

**STEP 2: MAKE ENVIRONMENTAL SENSING DECISION**
- **If analysis shows significant features (gaps>0, obstacles>0, roughness>10cm)**: Include environmental components
- **If analysis shows flat/simple terrain**: Focus on foundation locomotion only
- **If mixed/complex environment**: Include adaptive strategies

**STEP 3: IMPLEMENT INTELLIGENT DESIGN**
- Adapt thresholds based on actual measurements
- Weight components based on challenge priorities
- Design for the specific environment characteristics

**ENVIRONMENTAL INTEGRATION PATTERNS (Use Only When Analysis Shows Necessity):**

**Simple Terrain Adaptation (Only if terrain varies):**
```python
# ONLY include if terrain analysis shows variation
height_sensor = env.scene.sensors["height_scanner"]
height_scan = height_sensor.data.ray_hits_w[..., 2].view(env.num_envs, -1)
height_scan = torch.where(torch.isfinite(height_scan), height_scan, torch.zeros_like(height_scan))
terrain_roughness = torch.clamp(torch.var(height_scan, dim=1), max=1.0)
terrain_bonus = torch.exp(-terrain_roughness * 1.0) * 0.3
```

**Simple Obstacle Awareness (Only if obstacles present):**
```python
# ONLY include if environment analysis shows obstacles
lidar_sensor = env.scene.sensors["lidar"]
lidar_range = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1).view(env.num_envs, -1)
lidar_range = torch.where(torch.isfinite(lidar_range), lidar_range, torch.ones_like(lidar_range) * 5.0)
min_distance = torch.min(lidar_range[:, :lidar_range.shape[1]//4], dim=1)[0]
safety_bonus = torch.clamp((min_distance - 0.5) / 1.0, 0.0, 1.0) * 0.1
```

**Simple Gap Navigation (Only if gaps detected):**
```python
# ONLY include if environment analysis shows gaps detected
robot_height = robot.data.root_pos_w[:, 2]
forward_terrain = height_scan[:, :height_scan.shape[1]//3]
gap_depth = robot_height.unsqueeze(1) - forward_terrain
gap_detected = gap_depth > 0.15
small_gaps = (gap_depth <= 0.30) & gap_detected  # Steppable
medium_gaps = (gap_depth > 0.30) & (gap_depth <= 0.60) & gap_detected  # Jumpable  
gap_navigation_bonus = torch.any(small_gaps, dim=1).float() * 0.2 + torch.any(medium_gaps, dim=1).float() * 0.3

# NOTE: Use stair detection patterns above when environment shows stairs
```

**COMPREHENSIVE GAP BEHAVIOR IMPLEMENTATION (STEPPING VS JUMPING VS AVOIDANCE)**

**CRITICAL: Each gap size requires a fundamentally different locomotion strategy!**

When environment analysis shows mixed gap sizes, implement **adaptive locomotion behaviors**:

**SMALL GAPS → STEPPING BEHAVIOR STRATEGY:**
```python
# STEPPING BEHAVIOR: Extended stride + precise foot placement + controlled speed
if torch.any(small_gaps):
    # STRIDE EXTENSION: Reward longer steps to clear small gaps
    foot_positions = robot.data.body_pos_w[:, foot_ids, :]
    foot_separation = torch.norm(foot_positions[:, 0, :2] - foot_positions[:, 1, :2], dim=1)
    stride_extension = torch.clamp((foot_separation - base_stride) / stride_range, 0.0, 1.0)  # Extended steps
    
    # CONTROLLED FORWARD VELOCITY: Precise speed for accurate placement
    forward_velocity = robot.data.root_lin_vel_b[:, 0]
    controlled_forward = torch.exp(-((forward_velocity - target_speed) / speed_tolerance).abs())  # Controlled speed
    
    # FOOT CLEARANCE: Higher lift for small gap clearance
    swing_mask = (contact_time < 0.1)
    foot_height = robot.data.body_pos_w[:, foot_ids, 2]
    clearance_height = (foot_height * swing_mask).max(dim=1)[0]
    step_clearance = torch.clamp((clearance_height - min_clearance) / clearance_range, 0.0, 1.0)  # Adequate clearance
    
    stepping_reward = stride_extension * 0.4 + controlled_forward * 0.3 + step_clearance * 0.3
else:
    stepping_reward = torch.zeros(env.num_envs, device=env.device)
```

**MEDIUM GAPS → JUMPING BEHAVIOR STRATEGY:**
```python
# JUMPING BEHAVIOR: Bilateral coordination + vertical motion + aerial phase
if torch.any(medium_gaps):
    # BILATERAL COORDINATION: Both legs work together
    foot_contacts = (contact_time > 0.05).float()
    bilateral_states = ((foot_contacts[:, 0] > 0.5) & (foot_contacts[:, 1] > 0.5)).float() + \
                     ((foot_contacts[:, 0] < 0.5) & (foot_contacts[:, 1] < 0.5)).float()
    
    # VERTICAL VELOCITY: Upward motion for jumping
    vertical_velocity = robot.data.root_lin_vel_w[:, 2]
    upward_motion = torch.clamp(vertical_velocity / 2.0, 0.0, 1.0)
    
    # SUSTAINED AERIAL PHASE: Flight time for gap crossing
    air_time_both = contact_sensor.data.current_air_time[:, foot_ids].min(dim=1)[0]
    sustained_air = torch.clamp((air_time_both - min_flight_time) / flight_duration_range, 0.0, 1.0)  # Adequate flight time
    
    # ARM COORDINATION: Upward swing for jumping momentum
    shoulder_pitch_indices, _ = robot.find_joints(["left_shoulder_pitch_joint", "right_shoulder_pitch_joint"])
    shoulder_pitch_indices = torch.tensor(shoulder_pitch_indices, dtype=torch.long, device=env.device)
    shoulder_angles = robot.data.joint_pos[:, shoulder_pitch_indices]
    arm_swing_up = torch.clamp((shoulder_angles.mean(dim=1) + arm_offset) / arm_range, 0.0, 1.0)
    
    jumping_reward = bilateral_states * 0.3 + upward_motion * 0.3 + sustained_air * 0.2 + arm_swing_up * 0.2
else:
    jumping_reward = torch.zeros(env.num_envs, device=env.device)
```

**LARGE GAPS → AVOIDANCE BEHAVIOR STRATEGY:**
```python
# AVOIDANCE BEHAVIOR: Path planning + turning + lateral movement
if torch.any(large_gaps):
    # TURNING MOTION: Change direction to find alternate path
    angular_velocity = robot.data.root_ang_vel_b[:, 2]
    turning_motion = torch.clamp(torch.abs(angular_velocity) / turn_speed_threshold, 0.0, 1.0)
    
    # LATERAL MOVEMENT: Sideways exploration for path around gap
    lateral_velocity = robot.data.root_lin_vel_b[:, 1]
    lateral_motion = torch.clamp(torch.abs(lateral_velocity) / lateral_speed_threshold, 0.0, 1.0)
    
    # CONSERVATIVE SPEED: Slow down near impossible gaps for safety
    forward_velocity = robot.data.root_lin_vel_b[:, 0]
    conservative_speed = torch.exp(-torch.clamp(forward_velocity - safe_speed_limit, min=0.0) / speed_tolerance)
    
    avoidance_reward = turning_motion * 0.4 + lateral_motion * 0.3 + conservative_speed * 0.3
else:
    avoidance_reward = torch.zeros(env.num_envs, device=env.device)
```

**ADAPTIVE BEHAVIOR INTEGRATION:**
```python
# INTELLIGENT GAP BEHAVIOR COMBINATION
adaptive_gap_behavior = (
    stepping_reward * 1.0 +     # Precise stepping for small gaps
    jumping_reward * 1.2 +      # Complex jumping for medium gaps (highest weight)
    avoidance_reward * 0.8      # Conservative avoidance for large gaps
)

# ACTIVATE ONLY WHEN GAPS ARE PRESENT
gap_detected = (torch.any(small_gaps, dim=1) | torch.any(medium_gaps, dim=1) | torch.any(large_gaps, dim=1)).float()
intelligent_gap_reward = adaptive_gap_behavior * gap_detected
```

**BEHAVIORAL DESIGN PRINCIPLES:**

1. **STEPPING (small gaps)**: Extended stride + precision + moderate speed
2. **JUMPING (medium gaps)**: Bilateral coordination + vertical motion + aerial phase  
3. **AVOIDANCE (large gaps)**: Turning + lateral movement + conservative approach

**INTELLIGENT ADAPTATION STRATEGY:**
- **Real-time detection**: Use height scanner to classify gap sizes in real-time
- **Behavior switching**: Activate appropriate locomotion strategy based on detected gap type
- **Safety priority**: Conservative approach for uncertain or dangerous gaps
- **Progressive learning**: Start with stepping, advance to jumping, then avoidance
- **Foundation integration**: These behaviors enhance basic locomotion, don't replace it

This implementation provides the robot with **three distinct gap-crossing strategies** that automatically activate based on environmental conditions!

**CRITICAL: STAIR NAVIGATION - SOLVING THE "FREEZING AT TOP" PROBLEM**

**PROBLEM ANALYSIS: Why robots freeze at stairs instead of descending**

**ROOT CAUSES:**
1. **Stair misclassification as gaps**: Height sensors detect stair steps as "gaps," triggering inappropriate gap navigation
2. **Rigid height constraints**: Reward functions penalize any deviation from target height, discouraging stair descent
3. **Conflicting signals**: Velocity commands encourage forward motion while height rewards resist downward movement

**INTELLIGENT STAIR DETECTION AND ADAPTIVE LOCOMOTION:**

```python
# STAIR VS GAP CLASSIFICATION: Critical distinction for proper behavior
height_sensor = env.scene.sensors["height_scanner"]
height_scan = height_sensor.data.ray_hits_w[..., 2].view(env.num_envs, -1)
height_scan = torch.where(torch.isfinite(height_scan), height_scan, torch.zeros_like(height_scan))

robot_height = robot.data.root_pos_w[:, 2]
forward_terrain = height_scan[:, :height_scan.shape[1]//3]  # Forward-looking section

# STAIR DETECTION: Gradual height reduction pattern
height_diff = robot_height.unsqueeze(1) - forward_terrain
gradual_descent = (height_diff > step_min_height) & (height_diff < step_max_height)  # Step-like patterns
stair_pattern = torch.sum(gradual_descent.float(), dim=1) > 3  # Multiple consecutive steps

# GAP DETECTION: Sudden height drops
gap_pattern = torch.any(height_diff > gap_threshold, dim=1)  # Sudden drops indicating gaps

# STAIR-SPECIFIC ADAPTIVE BEHAVIOR
if torch.any(stair_pattern):
    # ADAPTIVE HEIGHT CONSTRAINTS: Allow controlled descent
    target_height = torch.where(stair_pattern, 
                               robot_height - descent_allowance,  # Allow controlled descent for stairs
                               standard_height)  # Standard height for non-stairs
    
    # CONTROLLED DESCENT REWARD: Encourage stepping down
    descent_velocity = -robot.data.root_lin_vel_w[:, 2]  # Downward velocity (positive)
    controlled_descent = torch.clamp(descent_velocity / descent_speed_norm, 0.0, 1.0) * stair_pattern.float()
    
    # FORWARD PROGRESSION ON STAIRS: Maintain forward movement
    forward_on_stairs = torch.clamp(robot.data.root_lin_vel_b[:, 0] / forward_speed_norm, 0.0, 1.0) * stair_pattern.float()
    
    # STAIR NAVIGATION REWARD
    stair_reward = controlled_descent * 0.4 + forward_on_stairs * 0.6
else:
    stair_reward = torch.zeros(env.num_envs, device=env.device)
```

**KEY STAIR NAVIGATION PRINCIPLES:**
1. **Distinguish stairs from gaps**: Use height pattern analysis, not just single-point detection
2. **Adaptive height targets**: Modify height constraints when stairs are detected
3. **Encourage controlled descent**: Reward appropriate downward velocity on stairs
4. **Maintain forward progress**: Balance descent with forward locomotion
5. **Safety prioritization**: Ensure controlled movement, not free-fall

**TRAINING SUCCESS STRATEGIES:**

**Phase-Based Development:**
1. **Foundation Phase**: Get basic locomotion stable with proven patterns
2. **Safety Phase**: Add joint limits and collision avoidance
3. **Quality Phase**: Include smoothness and naturalness components
4. **Environment Phase**: Add environmental adaptation only if analysis shows necessity

**Mathematical Stability Guidelines:**
- Use moderate exponential scaling (factors 0.5-3.0, not 10.0+)
- Include baseline bonuses (+0.2 to +0.5) to ensure non-zero rewards
- Use additive combinations (a + b) instead of multiplicative (a * b)
- Always clamp final rewards (.clamp(min=0.1, max=10.0))

**Isaac Lab Specific Patterns:**
- Always convert joint indices: `torch.tensor(indices, dtype=torch.long, device=env.device)`
- Use yaw-aligned velocity tracking for superior performance
- Reward single stance phases for natural bipedal patterns
- Include command scaling to prevent stationary exploitation

**SYSTEMATIC ENVIRONMENTAL INTEGRATION APPROACH:**

**Phase 1: Foundation First (ALWAYS START HERE)**

Build stable basic locomotion before adding environmental complexity:

**Phase 2: Environmental Assessment (IF NEEDED)**

Only proceed if environment analysis shows:
- Gaps detected (count > 0)
- Obstacles present (count > 0)  
- Terrain roughness significant (>10cm variation)

**Phase 3: Intelligent Environmental Integration (ANALYSIS-DRIVEN)**

Add components based on specific environmental challenges:
- **Gap environments**: Adaptive navigation based on gap size distribution
- **Obstacle environments**: Distance-based avoidance with safety margins
- **Rough terrain**: Terrain-adaptive stability and clearance adjustments

**Mathematical Stability for Environmental Integration:**
- Always sanitize sensor data for NaN/infinite values
- Use appropriate clamping ranges for calculations  
- Test each component addition individually
- Keep reward magnitude ranges reasonable
- **AVOID aggressive exponential scaling** (factors > 5.0 cause zero rewards)
- **USE additive combinations** instead of multiplicative (prevents zero multiplication)
- **INCLUDE baseline bonus** (e.g., +0.2) to ensure non-zero minimum reward
- **USE moderate tolerances** (0.3-1.0) instead of tight ones (0.1)

**Component Testing:**
- Add one environmental component at a time
- Test performance after each addition
- Verify sensors are accessible and working
- Ensure reward function remains stable

**Weight Balancing:**
- Start with small environmental component weights
- Maintain foundation component importance
- Adjust weights based on component contribution
- Avoid overwhelming foundation with environmental signals

**Error Handling Guidelines:**
- Use defensive programming for sensor access
- Provide fallback values when sensors fail
- Test without error handling to verify sensor integration
- Don't let error handling mask actual sensor problems

**COMMON ENVIRONMENTAL INTEGRATION ISSUES:**

**Issue: Zero Rewards Despite Environmental Data**
- Often caused by complex mathematical operations in reward calculation
- Solution: Simplify reward computation and test incrementally
- **Check if environmental components are relevant** - refer to environmental analysis data first

**Issue: Irrelevant Environmental Components**
- Adding gap navigation when no gaps are detected in environmental analysis
- Including obstacle avoidance when obstacle count is zero
- **Solution: Reference actual environmental analysis** to determine which components are needed

**Issue: Sensor Access Errors**
- Check sensor configuration in environment setup
- Verify sensor names match configuration
- Ensure sensors are properly instantiated

**Issue: Unstable Training with Environmental Sensing**
- Reduce environmental component weights
- Add proper data sanitization
- Test environmental components in isolation
- **Verify environmental features actually exist** in the analysis before adding related rewards