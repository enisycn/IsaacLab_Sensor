Isaac Lab Reward Function Format:

Your reward function must follow this exact structure with proper 4-space indentation:

def sds_custom_reward(env) -> torch.Tensor:
    """Your reward function description here."""
    # Access environment data (4-space indent)
    robot = env.scene["robot"]
    contact_sensor = env.scene.sensors["contact_forces"]
    commands = env.command_manager.get_command("base_velocity")
    
    # Initialize reward (4-space indent)
    reward = torch.zeros(env.num_envs, device=env.device)
    
    # IMPORTANT: For contact analysis, use this inline approach:
    # Get foot contact forces
    contact_forces = contact_sensor.data.net_forces_w  # [num_envs, num_bodies, 3]
    foot_ids, foot_names = contact_sensor.find_bodies(".*_foot")
    foot_forces = contact_forces[:, foot_ids, :]  # [num_envs, num_feet, 3]
    force_magnitudes = foot_forces.norm(dim=-1)  # [num_envs, num_feet]
    
    # Contact detection - analyze video to determine appropriate threshold
    # Different gaits require different thresholds: gentle gaits (0.5-2.0N), dynamic gaits (5.0-10.0N)
    contact_threshold = 1.0  # Adjust based on observed contact forces in video
    foot_contacts = (force_magnitudes > contact_threshold).float()  # Convert to float for partial credit
    
    # Note: Design contact rewards based on the observed gait pattern in the video
    
    # Available robot data:
    # robot.data.root_pos_w[:, 2] - height (z-coordinate)
    # robot.data.root_lin_vel_b[:, 0] - forward velocity (x-axis in body frame)
    # robot.data.root_lin_vel_w - linear velocity in world frame [num_envs, 3]
    # robot.data.root_ang_vel_b - angular velocity in body frame [num_envs, 3]
    # robot.data.root_ang_vel_w - angular velocity in world frame [num_envs, 3]
    # robot.data.root_quat_w - orientation quaternion [w,x,y,z]
    # robot.data.joint_pos - joint positions [num_envs, 12]
    # robot.data.joint_vel - joint velocities [num_envs, 12]
    # robot.data.applied_torque - actual applied joint torques [num_envs, 12]
    # WARNING: robot.data.joint_acc uses unstable finite differencing - use joint_vel for smoothness metrics
    
    # CRITICAL: DO NOT USE UNDEFINED VARIABLES!
    # ❌ NEVER: height_error = (height - GO1_SPECS["nominal_height"])  # GO1_SPECS not defined!
    # ❌ NEVER: target = ROBOT_CONFIG["target_height"]                 # ROBOT_CONFIG not defined!
    # ✅ CORRECT: Use literal values for robot specifications
    
    # UNITREE GO1 ROBOT SPECIFICATIONS (use these literal values):
    nominal_height = 0.34        # meters - Go1 nominal standing height
    height_min = 0.25           # meters - minimum safe height
    height_max = 0.45           # meters - maximum safe height  
    max_velocity = 3.0          # m/s - maximum safe forward velocity
    contact_threshold = 1.0     # Newtons - force threshold for foot contact detection
    
    # Available temporal contact data (for time-based analysis):
    # contact_sensor.data.current_air_time[:, foot_ids] - time each foot has been in air [num_envs, num_feet]
    # contact_sensor.data.last_air_time[:, foot_ids] - previous air time duration [num_envs, num_feet]
    # contact_sensor.data.current_contact_time[:, foot_ids] - time each foot has been in contact [num_envs, num_feet]
    # contact_sensor.data.last_contact_time[:, foot_ids] - previous contact duration [num_envs, num_feet]
    # contact_sensor.compute_first_contact(env.step_dt)[:, foot_ids] - boolean for new contacts [num_envs, num_feet]
    # These enable step timing rewards, gait pattern analysis, and natural locomotion behaviors
    
    # CRITICAL: Available contact sensor bodies for Unitree Go1:
    # ONLY these body names exist: ['FL_foot', 'FR_foot', 'RL_foot', 'RR_foot']
    # DO NOT try to find: thigh, shin, calf, hip, or any other body parts
    # ONLY use: contact_sensor.find_bodies(".*_foot") for foot contacts
    
    # CRITICAL: For tensor creation, always specify dtype and device:
    # For single vectors used with batch operations, expand to match env.num_envs:
    # up_vector = torch.tensor([0, 0, 1], dtype=torch.float32, device=env.device).expand(env.num_envs, 3)
    # projected_up = quat_apply_inverse(robot.data.root_quat_w, up_vector)
    # For batched dot products: uprightness = torch.sum(projected_up * up_vector, dim=-1)
    # 
    # NOT: torch.tensor([0, 0, 1])  # This creates Long tensors and causes errors!
    # NOT: up_vector = torch.tensor([0, 0, 1], dtype=torch.float32, device=env.device)  # Wrong shape for batch ops!
    # NOT: torch.dot(tensor1, tensor2)  # Only works with 1D tensors, not batched!
    # NOT: torch.clamp(tensor, device=device)  # device parameter not supported!
    
    # DIVISION BY ZERO SAFETY - CRITICAL PATTERNS:
    # 
    # ✅ CORRECT: Use max() for literal numbers
    # height_error = (robot.data.root_pos_w[:, 2] - target_height).abs()
    # height_reward = (1.0 - height_error / max(height_tolerance, 1e-6)).clamp(min=0.0, max=1.0)
    #
    # ✅ CORRECT: Use torch.clamp() for tensor variables  
    # target_vel = commands[:, 0]  # This is a tensor variable
    # reward_vel = actual_vel / torch.clamp(target_vel, min=1e-6)
    #
    # ❌ WRONG: torch.clamp() on literal numbers (CRASHES!)
    # height_reward = (1.0 - height_error / torch.clamp(0.15, min=1e-6))  # 0.15 is literal - CRASHES!
    
    # Your reward computation here (4-space indent)
    # Analyze video frames to determine:
    # - Appropriate height range and dynamics for the observed gait
    # - Required body orientation behavior (upright vs natural tilting)
    # - Contact timing and force patterns from the demonstration
    # - Speed and movement characteristics shown in the video
    
    # TASK-SPECIFIC GAIT REWARD PATTERNS:
    # For HOPPING: All legs synchronized coordination
    # num_contacts = foot_contacts.sum(dim=-1)
    # hop_contact_reward = ((num_contacts == 0) | (num_contacts == 4)).float()  # Only airborne or all-contact
    # hop_timing_reward = torch.where(all_airborne, current_air_time.mean(dim=1), 0.0)  # Reward sustained air phases
    # hop_reward = hop_contact_reward + hop_timing_reward + vertical_motion_emphasis
    # For TROTTING: diagonal_pairs = (foot_contacts[:, 0] & foot_contacts[:, 3]) | (foot_contacts[:, 1] & foot_contacts[:, 2]); trot_reward = diagonal_pairs.float()
    # For SEQUENTIAL GAITS: May use front-then-rear patterns for forward propulsion
    
    # CRITICAL: Always clamp final reward for numerical stability (4-space indent)
    return reward.clamp(min=0.0, max=10.0)

CRITICAL: Do NOT call external functions like extract_foot_contacts() or get_foot_contact_analysis().
Use the inline contact analysis code shown above.

CRITICAL: Always specify dtype=torch.float32 and device=env.device for tensor creation!

STABLE MATHEMATICAL PATTERNS (Use these for numerical stability):
Pattern 1 - Exponential Decay (for tracking targets):
# error = (robot.data.root_lin_vel_b[:, 0] - target_value).abs()
# reward_component = torch.exp(-scale_factor * error)  # scale_factor: 0.5 to 10.0

Pattern 2 - Bounded Linear (for contact rewards):
# num_contacts = foot_contacts.sum(dim=-1).float()
# contact_reward = (1.0 - (num_contacts - target_count).abs() / tolerance).clamp(min=0.0, max=1.0)

Pattern 3 - Boolean Masks (for gait patterns):
# gait_reward = ((num_contacts >= min_contacts) & (num_contacts <= max_contacts)).float()

Pattern 4 - Final Bounds (CRITICAL for PPO stability):
# return reward.clamp(min=0.0, max=10.0)  # Prevents training crashes

Pattern 5 - Division Safety (CRITICAL to prevent crashes):
# For literal numbers: safe_ratio = numerator / max(denominator_value, 1e-6)  # denominator_value is literal
# For tensor variables: safe_ratio = numerator / torch.clamp(tensor_var, min=1e-6)  # tensor_var computed from robot data

TIP: Normalize reward components to similar scales (0-1 range) for balanced learning.
TIP: Analyze video frames to understand the specific locomotion pattern before setting thresholds.

FORMATTING REQUIREMENTS:
- Use EXACTLY 4 spaces for each indentation level
- NEVER use 8 spaces, tabs, or inconsistent spacing
- Always add safety checks before any division operations
- Specify dtype=torch.float32 for all tensor creations