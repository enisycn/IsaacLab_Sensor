**CRITICAL DISCLAIMER: EXAMPLES ARE TECHNICAL REFERENCE ONLY!**

âš ï¸ **ALL NUMERICAL EXAMPLES, CODE SNIPPETS, AND REWARD PATTERNS IN THIS PROMPT ARE FOR TECHNICAL DEMONSTRATION ONLY.**
âš ï¸ **DO NOT COPY EXAMPLES DIRECTLY! ANALYZE THE ENVIRONMENT AND CREATE INTELLIGENT, CONTEXTUAL REWARDS.**

**ðŸš¨ TRAINING STABILITY WARNING: FOUNDATION-FIRST APPROACH REQUIRED ðŸš¨**

**CRITICAL: Basic locomotion must be stable BEFORE adding environmental components!**

**FOUNDATION-FIRST DEVELOPMENT SEQUENCE:**
1. **START**: Basic walking (velocity tracking, height maintenance, orientation stability)
2. **ADD**: Contact control and smoothness  
3. **THEN**: Simple environmental components IF needed
4. **FINALLY**: Complex environmental integration IF environment analysis shows it's necessary

**âš ï¸ ENVIRONMENTAL COMPONENTS ARE OPTIONAL, NOT MANDATORY âš ï¸**
- **ONLY include environmental sensing IF your analysis shows terrain variations, gaps, or obstacles**
- **FLAT walking environments**: Focus on natural locomotion, skip complex environmental sensing
- **SIMPLE terrains**: Basic terrain adaptation only
- **COMPLEX environments**: Gradual environmental integration after basic locomotion works

**STABLE TRAINING PRIORITIES:**
1. **Safety First**: Joint limits, orientation stability (prevent crashes)
2. **Basic Function**: Height, velocity tracking (achieve task goals)
3. **Movement Quality**: Smoothness, coordination (natural appearance)
4. **Environmental Adaptation**: ONLY if environment analysis indicates necessity

**ðŸ”¥ CRITICAL CODE STRUCTURE REQUIREMENT: ðŸ”¥**

**âŒ FORBIDDEN: DO NOT CREATE HELPER FUNCTIONS!**
- **NO** separate function definitions (def helper_function...)
- **NO** function calls outside of available Isaac Lab functions
- **ALL LOGIC MUST BE INLINE** within the sds_custom_reward function
- **USE ONLY** torch operations and provided Isaac Lab data access

**CORRECT PATTERN:**
```python
def sds_custom_reward(env) -> torch.Tensor:
    # ALL your logic goes here inline - no helper functions!
    reward = your_inline_calculations
    return reward.clamp(min=0.0, max=10.0)
```

**ðŸ§  ENVIRONMENT-DRIVEN REWARD DESIGN:**
- **Read environment analysis first:** What specific challenges exist in THIS environment?
- **Design contextual rewards:** Different terrains need completely different reward strategies
- **Think intelligently:** If no gaps exist, don't include gap-related rewards
- **Scale appropriately:** Complex environments need safety focus, simple ones need efficiency focus

**MANDATORY: GENTLE MOVEMENT ENFORCEMENT**

ALL generated reward functions MUST prioritize GENTLE, CONTROLLED, SUSTAINABLE movements:

Do not hardcode anything from examples, think and decide for task requirements.

**âš ï¸  ENVIRONMENTAL SENSOR INTEGRATION (WHEN RELEVANT)**

**IMPORTANT: Environmental integration should be based on environment analysis, not forced requirements!**

**ðŸ“‹ CLARIFICATION: Use existing sensor data WITHIN your reward function ONLY when relevant - do NOT generate observation configurations!**

**ENVIRONMENTAL COMPONENTS (INCLUDE ONLY IF ENVIRONMENT ANALYSIS SHOWS RELEVANCE):**
Include only the components that are relevant based on environment analysis:

**ðŸš¨ CRITICAL: WHEN ENVIRONMENT ANALYSIS SHOWS SIGNIFICANT FEATURES, INCLUDE ENVIRONMENTAL COMPONENTS! ðŸš¨**

**CLEAR DECISION RULES:**
- **IF environment analysis shows gaps detected (>0)**: INCLUDE gap navigation components
- **IF environment analysis shows obstacles detected (>0)**: INCLUDE obstacle avoidance components  
- **IF terrain roughness > 20cm**: INCLUDE terrain adaptation components
- **IF environment is truly flat with no features**: Focus foundation-only

1. **TERRAIN ANALYSIS (INCLUDE IF TERRAIN SHOWS VARIATION):**
   - **Include if**: Environment analysis shows terrain roughness > 20cm OR elevation changes
   - **Skip if**: Truly flat terrain with minimal variation
   ```python
   # INCLUDE if terrain analysis shows significant variation
   height_sensor = env.scene.sensors["height_scanner"]
   height_scan = height_sensor.data.ray_hits_w[..., 2].view(env.num_envs, -1)
   height_scan = torch.where(torch.isfinite(height_scan), height_scan, torch.zeros_like(height_scan))
   terrain_roughness = torch.clamp(torch.var(height_scan, dim=1), max=1.0)
   terrain_bonus = torch.exp(-terrain_roughness * 1.0) * 0.3
   ```

2. **OBSTACLE DETECTION (INCLUDE IF OBSTACLES ARE PRESENT):**
   - **Include if**: Environment analysis shows obstacles detected (count > 0)
   - **Skip if**: Clear environment with no obstacles
   ```python
   # INCLUDE if environment analysis shows obstacles
   lidar_sensor = env.scene.sensors["lidar"]
   lidar_range = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1).view(env.num_envs, -1)
   lidar_range = torch.where(torch.isfinite(lidar_range), lidar_range, torch.ones_like(lidar_range) * 10.0)
   min_obstacle_distance = torch.min(lidar_range[:, :lidar_range.shape[1] // 4], dim=1)[0]
   obstacle_bonus = torch.clamp((min_obstacle_distance - 1.0) / 2.0, min=0.0, max=0.5)
   ```

3. **GAP NAVIGATION (INCLUDE IF GAPS ARE DETECTED):**
   - **Include if**: Environment analysis shows gaps detected (count > 0)
   - **Adaptive strategy**: Use gap size distribution to determine navigation approach
   ```python
   # INCLUDE if environment analysis shows gaps detected
   # Real-time gap size detection for adaptive navigation
   robot_height = robot.data.root_pos_w[:, 2]
   forward_terrain = height_scan[:, :height_scan.shape[1]//3]
   gap_depth = robot_height.unsqueeze(1) - forward_terrain
   gap_detected = gap_depth > 0.15
   small_gaps = (gap_depth <= 0.30) & gap_detected  # Steppable
   medium_gaps = (gap_depth > 0.30) & (gap_depth <= 0.60) & gap_detected  # Jumpable
   gap_navigation_bonus = torch.any(small_gaps, dim=1).float() * 0.2 + torch.any(medium_gaps, dim=1).float() * 0.3
   ```

4. **ENVIRONMENTAL ADAPTATION (ONLY IF ENVIRONMENT ANALYSIS INDICATES NECESSITY):**
   Choose ONLY relevant components based on environment analysis:
   - **Flat terrain with no obstacles**: Skip environmental adaptation entirely
   - **Rough terrain**: Include terrain roughness adaptation
   - **Gap environments**: Include gap detection ONLY if gaps present
   - **Obstacle environments**: Include obstacle avoidance ONLY if obstacles present

5. **ENVIRONMENTAL SAFETY (ONLY IF ENVIRONMENTAL HAZARDS DETECTED):**
   - **Safe environments**: Focus on locomotion safety (joint limits, orientation)
   - **Hazardous environments**: Add environmental hazard avoidance

**VALIDATION CHECKLIST:**
Before submitting, verify your reward function includes:
- height_scan usage for terrain analysis
- lidar_range usage for obstacle detection
- At least one environmental adaptation mechanism
- Environmental safety considerations
- Proper integration with locomotion objectives

**ðŸ”§ ENVIRONMENTAL COMPONENT DEBUG PATTERN FOR GPT GUIDANCE:**

**CRITICAL: Always include observability in environmental reward functions:**

```python
def sds_custom_reward(env) -> torch.Tensor:
    """Environmental reward with systematic observability for GPT analysis."""
    
    # === FOUNDATION LOCOMOTION COMPONENTS ===
    foundation_reward = (
        survival_bonus +
        height_maintenance +
        velocity_tracking +
        orientation_stability +
        contact_control
    )
    
    # === ENVIRONMENTAL COMPONENTS (Calculate separately for monitoring) ===
    
    # Terrain adaptation (only if terrain analysis shows variation)
    terrain_bonus = torch.exp(-terrain_roughness * 2.0) - 1.0  # Convert to additive bonus
    
    # Obstacle avoidance (only if obstacles detected in analysis)  
    obstacle_bonus = torch.clamp(min_obstacle_distance - 1.0, min=0.0) * 0.2
    
    # Gap navigation (only if gaps present, adaptive based on gap sizes)
    gap_navigation_bonus = adaptive_gap_navigation_logic()  # Real-time gap size detection
    
    # Environmental reward (separable for debugging)
    environmental_reward = (
        terrain_bonus +           # Individual component visible
        obstacle_bonus +          # Can be monitored separately  
        gap_navigation_bonus      # Adaptive behavior observable
    )
    
    # === OBSERVABILITY FOR GPT GUIDANCE (MANDATORY) ===
    if hasattr(env, '_debug_counter'):
        env._debug_counter += 1
    else:
        env._debug_counter = 0
    
    if env._debug_counter % 200 == 0:
        print(f"[REWARD DEBUG] Foundation: {{foundation_reward.mean():.3f}}, "
              f"Environmental: {{environmental_reward.mean():.3f}}")
        print(f"  Terrain Bonus: {{terrain_bonus.mean():.3f}}, "
              f"Obstacle Bonus: {{obstacle_bonus.mean():.3f}}, "
              f"Gap Navigation: {{gap_navigation_bonus.mean():.3f}}")
    
    # === CLEAR SEPARATION FOR SYSTEMATIC IMPROVEMENT ===
    total_reward = foundation_reward + environmental_reward
    
    return torch.clamp(total_reward, min=0.1, max=10.0)
```

**ENVIRONMENTAL OBSERVABILITY PRINCIPLES:**

1. **SEPARATE CALCULATION**: Calculate environmental components independently from locomotion components
2. **ADDITIVE COMBINATION**: Use addition, not multiplication, so individual contributions are visible
3. **DEBUG OUTPUT**: Print component breakdown every 200 steps for GPT analysis
4. **INDIVIDUAL MONITORING**: Each environmental feature should be separately observable
5. **SYSTEMATIC INTEGRATION**: Add environmental components incrementally with performance monitoring

**ADAPTIVE GAP NAVIGATION PATTERN (For Mixed Gap Environments):**

```python
# Real-time gap size detection and adaptive navigation
height_sensor = env.scene.sensors["height_scanner"]
height_scan = height_sensor.data.ray_hits_w[..., 2].view(env.num_envs, -1)
height_scan = torch.where(torch.isfinite(height_scan), height_scan, torch.zeros_like(height_scan))

# Detect forward gaps and classify sizes dynamically
robot_height = robot.data.root_pos_w[:, 2]
forward_terrain = height_scan[:, :height_scan.shape[1]//3]  # Focus on forward area
gap_depth = robot_height.unsqueeze(1) - forward_terrain
gap_detected = gap_depth > 0.15  # Basic gap threshold

# Classify gap sizes and activate appropriate navigation
small_gaps = (gap_depth <= 0.30) & gap_detected  # Steppable gaps
medium_gaps = (gap_depth > 0.30) & (gap_depth <= 0.60) & gap_detected  # Jumpable gaps  
large_gaps = (gap_depth > 0.60) & gap_detected  # Avoidance required

# Adaptive reward based on detected gap type
gap_navigation_bonus = torch.zeros(env.num_envs, device=env.device)
if torch.any(small_gaps):
    gap_navigation_bonus += 0.2  # Reward normal walking for small gaps
if torch.any(medium_gaps):  
    gap_navigation_bonus += 0.3  # Reward jumping behavior for medium gaps
if torch.any(large_gaps):
    gap_navigation_bonus += 0.1  # Reward avoidance/route planning for large gaps

# Debug output for gap navigation monitoring
if env._debug_counter % 200 == 0:
    print(f"  Gap Types - Small: {{torch.any(small_gaps)}}, "
          f"Medium: {{torch.any(medium_gaps)}}, Large: {{torch.any(large_gaps)}}")
```

**BENEFITS OF ENVIRONMENTAL OBSERVABILITY:**

1. **GPT Can Provide Specific Feedback**: "Increase terrain adaptation weight, obstacle avoidance working well"
2. **Systematic Debugging**: When performance drops, identify which environmental component is the issue
3. **Incremental Improvement**: Tune individual environmental features based on their observed contributions
4. **Clear Understanding**: See exactly what environmental sensors contribute to overall reward
5. **Validation**: Confirm environmental components are active and contributing as expected

**COMMON ENVIRONMENTAL DEBUG PATTERNS:**

```python
# Pattern 1: Component contribution monitoring
if env._debug_counter % 200 == 0:
    print(f"[ENV] Terrain: {{terrain_component.mean():.3f}}, "
          f"Obstacle: {{obstacle_component.mean():.3f}}, "
          f"Total Environmental: {{environmental_reward.mean():.3f}}")

# Pattern 2: Feature detection confirmation  
if env._debug_counter % 500 == 0:
    print(f"[SENSORS] Terrain Roughness: {{terrain_roughness.mean():.3f}}, "
          f"Min Obstacle Dist: {{min_obstacle_distance.mean():.2f}}m")

# Pattern 3: Adaptive behavior validation
if env._debug_counter % 300 == 0:
    active_strategies = []
    if torch.any(small_gaps): active_strategies.append("Stepping")
    if torch.any(medium_gaps): active_strategies.append("Jumping") 
    if torch.any(large_gaps): active_strategies.append("Avoidance")
    print(f"[ADAPTATION] Active navigation strategies: {{active_strategies}}")
```

**ENVIRONMENTAL INTEGRATION VALIDATION FOR GPT:**
- âœ… Foundation vs environmental rewards are clearly separated
- âœ… Individual environmental components are observable through debug output
- âœ… Environmental features are validated to exist before including related components
- âœ… Adaptive behavior (like gap navigation) provides real-time strategy feedback
- âœ… Debug output enables GPT to provide specific, targeted feedback for improvement

**TASK-SPECIFIC IMPLEMENTATION REQUIREMENTS:**

**VERTICAL JUMP TASK IMPLEMENTATION:**
- **Height:** 10-25cm clearance (NEVER >30cm for basic jumps)
- **Velocity:** 0.5-1.5 m/s vertical (NEVER >2m/s for basic jumps)
- **Coordination:** Bilateral synchronization required
- **Landing:** Controlled impact absorption

**WALKING TASK IMPLEMENTATION:**
- **Height:** 0.74m maintained (body height consistency)
- **Velocity:** Natural speeds 0.5-2.0 m/s forward
- **Coordination:** Cross-pattern arm-leg coordination
- **Landing:** Controlled heel-to-toe contact
- **Contacts:** Feet should not drag. Lifting feet should 4-5 cm should be like human-like. 

**BACKFLIP TASK IMPLEMENTATION:**
- **Height:** 1.2m -1.5 minimum (sufficient for rotation)
- **Flight time:** 0.8-1.2 seconds (rotation duration)
- **Rotation:** 300-450Â°/second controlled (FULL 360Â° total - robot must land upright)
- **Coordination:** Phase-specific arm patterns

**MARCH TASK IMPLEMENTATION:**
- **Height:** 0.74m maintained with controlled knee lift
- **Velocity:** Deliberate cadence 60-90 steps/minute
- **Coordination:** Opposite arm-leg ceremonial patterns
- **Landing:** Precise foot placement

**SPRINT TASK IMPLEMENTATION:**
- **Height:** 0.74m maintained during high-speed movement
- **Velocity:** 3.0-6.0 m/s forward (higher than walking)
- **Coordination:** Reciprocal arm drive for propulsion
- **Landing:** Controlled high-speed ground contact

**PACE TASK IMPLEMENTATION:**
- **Height:** 0.74m maintained during lateral movement
- **Velocity:** 0.5-1.5 m/s lateral (controlled sideways)
- **Coordination:** Compensatory arms for lateral stability
- **Landing:** Controlled lateral foot placement

**TASK-SPECIFIC REWARD PATTERNS: Do not Hardcode directly just an example**
```python
# VERTICAL JUMP: Gentle height control
height_gain = current_height - baseline_height
gentle_jump_height = torch.where(
    (height_gain > 0.05) & (height_gain < 0.25),  # 5-25cm target
    torch.exp(-((height_gain - 0.15) / 0.05).abs()),  # Peak at 15cm
    torch.zeros_like(height_gain)  # Zero reward outside range
)

# WALKING: Height maintenance
target_height = 0.74  # G1 humanoid walking height
height_error = (robot.data.root_pos_w[:, 2] - target_height).abs()
height_reward = torch.exp(-height_error / 0.3)  # Maintain 0.74m with stable tolerance

# BACKFLIP: High jump for rotation
height_gain = current_height - baseline_height
backflip_height = torch.where(
    (height_gain > 0.8) & (height_gain < 1.2),  # 0.8-1.2m target
    torch.exp(-((height_gain - 1.0) / 0.2).abs()),  # Peak at 1.0m
    torch.zeros_like(height_gain)  # Zero reward outside range
)

# BACKFLIP: Flight time control
flight_time = contact_sensor.data.current_air_time[:, foot_ids].mean(dim=1)
flight_reward = torch.where(
    (flight_time > 0.8) & (flight_time < 1.2),  # 0.8-1.2s target
    torch.exp(-((flight_time - 1.0) / 0.2).abs()),  # Peak at 1.0s
    torch.zeros_like(flight_time)  # Zero reward outside range
)

# UNIVERSAL: Landing impact control
impact_penalty = torch.clamp((force_magnitude - 500.0) / 200.0, 0.0, 1.0)
```

**BIOMECHANICAL UNDERSTANDING FOR NATURAL WALKING DESIGN:**

When creating walking rewards, it's essential to understand the biomechanical principles that distinguish natural human walking from robotic movement patterns. Two critical areas often overlooked are foot clearance and arm coordination.

**FOOT CLEARANCE EDUCATION - UNDERSTANDING "DRAGGY FEET":**

**Why Foot Clearance Matters:**
Humans naturally lift their toes 10-25mm above ground during mid-swing phase. This isn't just functional - it's a fundamental safety mechanism and aesthetic requirement for natural-looking walking.

**Key Biomechanical Insights:**
- Peak clearance occurs during mid-swing, not at toe-off or heel contact
- The foot undergoes dorsiflexion (toes up) during swing to ensure clearance
- Adequate clearance indicates proper ankle, knee, and hip coordination
- Insufficient clearance creates "draggy" appearance and tripping risk

**Design Thinking for Foot Clearance:**
- Consider how to measure foot height during non-contact phases
- Think about the role of ankle positioning and dorsiflexion
- Account for both minimum clearance values and trajectory smoothness
- Balance clearance adequacy with energy efficiency

**ARM COORDINATION EDUCATION - UNDERSTANDING NATURAL PATTERNS:**

**Why Arm Swing Matters:**
Natural human walking uses cross-pattern coordination where the left arm swings forward when the right leg advances. This reduces energy cost by 5-12% and provides essential balance control.

**Key Biomechanical Insights:**
- Arms act as passive pendulums driven by shoulder rotation and momentum
- Cross-pattern coordination (opposite arm-leg movement) is fundamental to human walking
- Natural amplitude ranges are approximately 15-20Â° forward, 10-15Â° backward
- Arms and legs typically maintain 1:1 frequency coupling

**Common Robotic Problems to Understand:**
- "Negative side restriction" - arms staying behind body
- Same-side coordination - arms and legs moving together like a military march
- Bilateral synchrony - both arms moving in same direction
- Excessive constraint - arms held rigidly without natural motion

**Design Thinking for Natural Arm Movement:**
- Consider phase relationships between arm and leg movements
- Think about momentum transfer and energy efficiency
- Account for natural frequency coupling between arms and legs
- Balance active coordination with passive dynamics

**ENVIRONMENTAL AWARENESS EDUCATION - UNDERSTANDING TERRAIN-ADAPTIVE LOCOMOTION:**

**Why Environmental Awareness Matters:**
Real-world locomotion requires adapting to terrain variations, obstacles, and environmental constraints. Reward functions that only work on flat surfaces fail to produce robust, generalizable behaviors.

**Key Environmental Sensing Insights:**
- Height scan data provides detailed terrain elevation mapping around the robot
- LiDAR range data offers comprehensive distance measurements for obstacle detection
- Terrain variance indicates surface roughness and gap density
- Forward-looking sensor analysis enables predictive behavior adaptation

**Environmental Adaptation Principles:**
- **Terrain-Adaptive Target Adjustment:** Modify target values based on local terrain characteristics
- **Gap-Aware Clearance Adaptation:** Adjust movement patterns when terrain challenges are detected
- **Predictive Obstacle Response:** Adapt behavior based on upcoming terrain features
- **Stability-Terrain Coupling:** Allow appropriate tolerance adjustments based on terrain difficulty

**Design Thinking for Environment-Aware Rewards:**
- Access terrain data using observation manager and sensor interfaces
- Analyze terrain characteristics through statistical measures of sensor data
- Consider how terrain difficulty should influence reward component emphasis
- Think about forward-looking terrain features for predictive behavioral adaptations
- Integrate environmental awareness with natural biomechanical movement patterns

**Environmental Sensor Integration Technical Examples:**
```python
# Height scan processing for terrain awareness
height_sensor = env.scene.sensors["height_scanner"]
height_scan = height_sensor.data.ray_hits_w[..., 2].view(env.num_envs, -1)
terrain_variance = torch.var(height_scan.view(env.num_envs, -1), dim=1)
local_terrain_level = torch.mean(height_scan.view(env.num_envs, -1), dim=1)

# Forward-looking terrain analysis
scan_size = int(torch.sqrt(torch.tensor(height_scan.shape[1])))
height_grid = height_scan.view(env.num_envs, scan_size, scan_size)
forward_terrain = height_grid[:, :scan_size//4, scan_size//4:3*scan_size//4]

# LiDAR obstacle detection
lidar_sensor = env.scene.sensors["lidar"]
lidar_range = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1).view(env.num_envs, -1)
forward_obstacles = torch.min(lidar_range[:, :lidar_range.shape[1]//4], dim=1)[0]
```

**Environmental Adaptation Integration Patterns:**
```python
# Terrain-adaptive target adjustment
adaptive_height_target = base_height_target + terrain_level_adjustment
adaptive_clearance_target = base_clearance + terrain_difficulty_bonus

# Risk-based reward component scaling
terrain_difficulty_factor = terrain_analysis_function(terrain_variance)
safety_emphasis_multiplier = 1.0 + terrain_difficulty_factor
speed_priority_multiplier = 1.0 - conservative_adjustment_factor

# Forward-looking behavior adaptation
upcoming_terrain_challenge = forward_terrain_analysis_function(forward_terrain)
predictive_behavior_adjustment = challenge_response_function(upcoming_terrain_challenge)
```

**Common Environmental Design Mistakes to Avoid:**
- Ignoring available terrain data when creating reward functions
- Using completely rigid targets regardless of terrain characteristics
- Treating all environments identically without considering terrain difficulty
- Focusing only on immediate terrain without considering upcoming challenges
- Completely separating environmental adaptation from core locomotion principles

**Environmental-Biomechanical Integration:**
The most effective rewards combine natural biomechanical principles with intelligent environmental adaptation. Consider how:
- Natural movement patterns can be adapted for terrain-specific challenges
- Stability and safety requirements change with terrain characteristics
- Energy efficiency considerations interact with terrain difficulty
- Movement timing and coordination adapt to environmental demands

**MOVEMENT QUALITY PRINCIPLES FOR REWARD DESIGN:**

**Understanding Natural Movement Phases:**
- **Stance Phase:** Weight acceptance, single-limb support, weight transfer
- **Swing Phase:** Foot clearance, limb advancement, contact preparation
- **Transitions:** Smooth progressions between phases
- **Bilateral Coordination:** Appropriate left-right timing relationships

**Quality Indicators to Consider:**
- **Smoothness:** Gradual transitions without abrupt changes
- **Efficiency:** Achieving goals with minimal energy expenditure
- **Stability:** Controlled balance throughout movement cycle
- **Naturalness:** Movement patterns matching human biomechanics

**Multi-Scale Movement Analysis:**
- **Joint Level:** Individual joint contributions to movement goals
- **Limb Level:** Coordination between joints within each limb
- **Inter-limb Level:** Coordination between left-right and upper-lower body
- **Whole-body Level:** Integration of all segments for locomotion

**EDUCATIONAL APPROACH TO REWARD ENGINEERING:**

**Problem-Solving Mindset:**
- Identify specific biomechanical challenges in the observed movement
- Understand root causes of movement quality issues
- Consider how natural human patterns solve these challenges
- Design metrics that encourage biomechanically sound solutions

**Quality vs. Functionality Balance:**
- **Functional Success:** Basic achievement of locomotion goals
- **Movement Quality:** Natural, efficient, biomechanically sound execution
- **Exceptional Performance:** Mastery of human-like coordination patterns
- **Aesthetic Considerations:** Visual appearance of naturalness and confidence

**Creative Design Process for Gait-Specific Excellence:**
1. **Understand Gait Uniqueness:** What makes THIS gait pattern biomechanically distinct?
2. **Identify Mastery Metrics:** What defines exceptional execution beyond basic functionality?
3. **Consider Multiple Approaches:** What novel combinations or alternatives could work?
4. **Design for Movement Beauty:** How to reward natural, efficient, aesthetically pleasing movement?
5. **Think Multi-Scale:** Consider immediate feedback and long-term patterns
6. **Test Creative Logic:** Does the approach truly capture movement excellence?

**CREATIVE REWARD ENGINEERING DIRECTIVE:**

Use the technical guidance below as **CREATIVE INSPIRATION** - not rigid rules to copy! Your goal is to generate the **BEST POSSIBLE** reward function for the specific gait pattern by:

**INNOVATING WITH PURPOSE:** Consider the provided examples as starting points, then think deeper about what makes THIS specific gait pattern exceptional
**THINKING BIOMECHANICALLY:** What unique movement challenges does this gait present? How can you measure and reward the highest quality execution?
**EXPERIMENTING WITH APPROACHES:** Try novel combinations of the suggested techniques, or invent entirely new metrics that capture movement excellence
**DESIGNING FOR PERFECTION:** Create detailed, nuanced rewards that distinguish between good, great, and exceptional movement execution

The specifications below are **TOOLS FOR YOUR CREATIVITY** - use them to build something innovative and effective!

**ADVANCED NATURAL WALKING DESIGN PRINCIPLES:**

**NATURAL RHYTHM AND TEMPORAL FLOW CONSIDERATIONS:**

**Understanding Walking Cadence and Timing:**
Human walking has inherent rhythm that distinguishes it from robotic, high-frequency movement. When designing walking rewards, consider these temporal quality factors:

**Key Temporal Quality Indicators:**
- **Natural cadence:** Comfortable human walking occurs at 100-120 steps per minute
- **Phase proportions:** Swing phase ~40%, stance phase ~60% of total gait cycle
- **Smooth transitions:** Gradual phase changes rather than abrupt, harsh shifts
- **Rhythmic consistency:** Predictable, steady timing rather than irregular or choppy patterns

**Design Thinking for Natural Rhythm:**
- Consider how to measure and reward consistent step timing
- Think about phase duration relationships and transition smoothness
- Account for movement flow versus discrete, mechanical actions
- Balance speed achievement with natural movement quality
- Avoid rewarding high-frequency movement that lacks natural flow

**POSTURE AND TORSO ORIENTATION CONSIDERATIONS:**

**Understanding Human Walking Posture:**
Natural walking requires specific postural alignment that facilitates forward progression while maintaining balance.

**Key Postural Quality Indicators:**
- **Forward progression posture:** Slight forward lean (2-5Â°) for momentum
- **Upright alignment:** Torso maintains relative verticality
- **Anti-backward lean:** Backward lean disrupts natural walking biomechanics
- **Dynamic balance:** Postural adjustments throughout gait cycle

**Design Thinking for Natural Posture:**
- Consider torso orientation relative to movement direction and gravity
- Think about appropriate forward inclination versus problematic backward lean
- Account for dynamic postural control during walking cycles
- Reward postural patterns that facilitate natural forward progression
- Penalize postural alignment that disrupts walking efficiency

**ARM MOVEMENT FREEDOM AND NATURAL SWING CONSIDERATIONS:**

**Understanding Natural Arm Pendulum Motion:**
Human arms function as passive pendulums during walking, requiring freedom for natural movement appearance.

**Key Arm Movement Quality Indicators:**
- **Pendulum freedom:** Natural arm swing with gravity assistance
- **Speed adaptation:** Arm amplitude increases with walking speed
- **Relaxed positioning:** Natural arm hang without excessive constraint
- **Cross-pattern coordination:** Opposite arm-leg movement relationships

**Design Thinking for Natural Arm Movement:**
- Consider arm movement amplitude relative to walking speed and constraints
- Think about natural pendulum motion characteristics versus rigid positioning
- Account for arm movement freedom versus overly tight constraints
- Reward arm patterns that appear relaxed and naturally adaptive
- Balance arm movement with stability and coordination requirements

**ARM POSITIONING AND BODY PROXIMITY CONSIDERATIONS:**

**Understanding Natural Arm-Body Spatial Relationship:**
Human walking involves arms maintaining appropriate proximity to the body while allowing natural pendulum motion, avoiding both excessive constraint and unnatural wide positioning.

**Key Arm Positioning Quality Indicators:**
- **Natural proximity:** Arms swing close to body, not far away or laterally extended
- **Body-relative positioning:** Arms maintain appropriate relationship to torso centerline
- **Controlled envelope:** Arm movement occurs within natural distance from body
- **Lateral boundaries:** Natural arm swing stays within appropriate limits near body

**Design Thinking for Natural Arm-Body Proximity:**
- Consider arm position relative to torso and natural body envelope
- Think about appropriate lateral limits for natural arm movement
- Account for body-relative arm positioning versus excessive lateral extension
- Reward arm positioning that maintains natural proximity to body
- Balance arm movement freedom with appropriate body-relative positioning

**ADVANCED BIOMECHANICAL IMPLEMENTATION GUIDANCE:**

**CROSS-PATTERN COORDINATION IMPLEMENTATION:**

**Understanding Opposite-Phase Detection:**
Natural walking requires precise measurement and reward of anti-phase arm-leg coordination patterns.

**Implementation Techniques:**
- **Velocity correlation measurement:** Calculate correlation between same-side arm-leg sagittal velocities
- **Phase-locked timing detection:** Track arm swing peaks relative to opposite foot stance phases
- **Anti-synchrony penalty:** Detect and penalize same-side coordination instances
- **Temporal precision tracking:** Measure optimal phase relationships throughout walking cycles

**Practical Implementation Considerations:**
```python
# Example approach for cross-pattern coordination
# SAFE: Define shoulder joint indices first
shoulder_joints = ["left_shoulder_pitch_joint", "right_shoulder_pitch_joint"]
shoulder_pitch_indices, _ = robot.find_joints(shoulder_joints)
# CRITICAL: Convert list to tensor for proper indexing
shoulder_pitch_indices = torch.tensor(shoulder_pitch_indices, dtype=torch.long, device=env.device)
shoulder_pitch_vel = robot.data.joint_vel[:, shoulder_pitch_indices]
# SAFE: Define hip joint indices
hip_joints = ["left_hip_pitch_joint", "right_hip_pitch_joint"]
hip_pitch_indices, _ = robot.find_joints(hip_joints)
# CRITICAL: Convert list to tensor for proper indexing
hip_pitch_indices = torch.tensor(hip_pitch_indices, dtype=torch.long, device=env.device)
hip_pitch_vel = robot.data.joint_vel[:, hip_pitch_indices]
# Calculate correlation between same-side velocities (should be negative)
same_side_correlation = torch.corrcoef(torch.stack([shoulder_pitch_vel[:, 0], hip_pitch_vel[:, 0]], dim=0))
# Reward negative correlation, penalize positive correlation
cross_pattern_reward = torch.exp(-torch.clamp(same_side_correlation[0,1], min=0.0))
```

**ARM-SWING AMPLITUDE AND SMOOTHNESS IMPLEMENTATION:**

**Understanding Range Control and Jerk Minimization:**
Natural arm movement requires amplitude targeting combined with smoothness optimization.

**Implementation Techniques:**
- **Range targeting:** Monitor shoulder angles for 15-20Â° forward, 10-15Â° backward excursions
- **Jerk calculation:** Compute third derivatives of shoulder angles for smoothness penalties
- **Energy efficiency measurement:** Calculate mechanical work done by arm joints
- **Smoothness profiling:** Analyze acceleration patterns for pendulum characteristics

**Practical Implementation Considerations:**
```python
# Example approach for arm amplitude and smoothness
# SAFE: Define shoulder joint indices if not already defined
shoulder_joints = ["left_shoulder_pitch_joint", "right_shoulder_pitch_joint"]
shoulder_pitch_indices, _ = robot.find_joints(shoulder_joints)
# CRITICAL: Convert list to tensor for proper indexing
shoulder_pitch_indices = torch.tensor(shoulder_pitch_indices, dtype=torch.long, device=env.device)
shoulder_angles = robot.data.joint_pos[:, shoulder_pitch_indices]
target_forward = 0.35  # ~20 degrees
target_backward = -0.26  # ~15 degrees
amplitude_error = torch.clamp(shoulder_angles - target_forward, min=0.0) + \
                 torch.clamp(target_backward - shoulder_angles, min=0.0)
# Calculate jerk (third derivative) for smoothness
shoulder_jerk = torch.diff(robot.data.joint_vel[:, shoulder_pitch_indices], n=2, dim=0)
smoothness_penalty = shoulder_jerk.abs().mean()
```

**PENDULUM DYNAMICS AND FREQUENCY COUPLING IMPLEMENTATION:**

**Understanding Period Matching and Energy Efficiency:**
Natural arms function as passive pendulums with specific frequency relationships to leg movement.

**Implementation Techniques:**
- **Period analysis:** Compare arm swing periods to leg stride periods (1:1 ratio target)
- **Spectral coherence:** Use frequency domain analysis for arm-leg coupling measurement
- **Work calculation:** Measure mechanical work to distinguish passive vs active movement
- **Frequency consistency:** Track coupling relationships across multiple cycles

**Practical Implementation Considerations:**
```python
# Example approach for pendulum dynamics (conceptual - needs implementation)
# NOTE: calculate_oscillation_period and calculate_stride_period are conceptual functions
# that need to be implemented based on actual signal processing techniques
# arm_period = calculate_oscillation_period(shoulder_angles)  # NEEDS IMPLEMENTATION
# leg_period = calculate_stride_period(hip_angles)  # NEEDS IMPLEMENTATION
# period_ratio = arm_period / leg_period
# frequency_coupling_reward = torch.exp(-5.0 * (period_ratio - 1.0).abs())

# SAFE: Mechanical work calculation for energy efficiency
shoulder_torques = robot.data.applied_torque[:, shoulder_pitch_indices]
shoulder_velocities = robot.data.joint_vel[:, shoulder_pitch_indices]
arm_work = torch.sum(shoulder_torques * shoulder_velocities * env.step_dt)
energy_efficiency_reward = torch.exp(-0.1 * arm_work.abs())
```

**FOOT CLEARANCE AND TRAJECTORY IMPLEMENTATION:**

**Understanding Multi-Range Targeting and Trajectory Shaping:**
Natural foot clearance requires specific patterns balancing safety with efficiency.

**Implementation Techniques:**
- **Multi-range targeting:** Reward 10-25mm minimum, 50-80mm peak clearance
- **Template matching:** Compare knee flexion to bell-curve templates
- **Swing phase analysis:** Measure foot height during non-contact phases
- **Trajectory smoothness:** Evaluate foot path continuity throughout swing

**Practical Implementation Considerations:**
```python
# Example approach for foot clearance and trajectory
foot_height = robot.data.body_pos_w[:, foot_ids, 2]
swing_mask = (foot_contacts < 0.5)
swing_clearance = foot_height * swing_mask
# SAFE: Multi-range targeting with moderate scaling
min_clearance_reward = torch.where(swing_clearance > 0.010, torch.ones_like(swing_clearance), 
                                  torch.exp(-5.0 * (0.010 - swing_clearance)))  # Reduced from -50.0 to -5.0
peak_clearance_reward = torch.where((swing_clearance > 0.050) & (swing_clearance < 0.080),
                                   torch.ones_like(swing_clearance), torch.zeros_like(swing_clearance))
```

**POSTURE CONTROL AND BALANCE IMPLEMENTATION:**

**Understanding Lean Monitoring and Dynamic Stability:**
Natural walking posture requires precise torso control with balance optimization.

**Implementation Techniques:**
- **Lean angle monitoring:** Track torso pitch within -5Â° to +5Â° range
- **CoM projection:** Calculate center-of-mass position within support polygon
- **Dynamic balance assessment:** Monitor stability during single-support phases
- **Support analysis:** Evaluate balance maintenance throughout walking cycles

**Practical Implementation Considerations:**
```python
# Example approach for posture control
from isaaclab.utils.math import quat_apply_inverse
torso_quat = robot.data.root_quat_w
up_vector = torch.tensor([0, 0, 1], device=env.device).expand(env.num_envs, 3)
torso_up = quat_apply_inverse(torso_quat, up_vector)
lean_angle = torch.atan2(torso_up[:, 0], torso_up[:, 2])
# SAFE: Reward slight forward lean, penalize backward lean with moderate scaling
optimal_lean = 0.087  # ~5 degrees forward
lean_reward = torch.where(lean_angle > 0, torch.exp(-3.0 * (lean_angle - optimal_lean).abs()),  # Reduced from -10.0 to -3.0
                         torch.exp(-5.0 * lean_angle.abs()))  # Reduced from -20.0 to -5.0
```

**TEMPORAL FLOW AND CADENCE IMPLEMENTATION:**

**Understanding Rhythm Analysis and Phase Duration Control:**
Natural walking requires specific timing patterns distinguishing human from mechanical movement.

**Implementation Techniques:**
- **Cadence tracking:** Monitor heel-strike events for steps per minute calculation
- **Phase duration analysis:** Measure stance:swing ratios (60:40 target)
- **Timing consistency:** Evaluate temporal regularity across cycles
- **Rhythm detection:** Analyze step patterns for natural flow

**Practical Implementation Considerations:**
```python
# Example approach for temporal flow (conceptual - needs implementation)
contact_changes = torch.diff(foot_contacts, dim=0)
heel_strikes = (contact_changes > 0.5).float()
# NOTE: calculate_time_between_events is conceptual and needs implementation
# step_intervals = calculate_time_between_events(heel_strikes)  # NEEDS IMPLEMENTATION
# cadence = 60.0 / step_intervals  # steps per minute

# SAFE: Alternative approach using contact timing
contact_sensor = env.scene.sensors["contact_forces"]
foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
current_contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
avg_contact_time = current_contact_time.mean(dim=1)
# SAFE: Reward natural contact timing with reasonable tolerance
contact_timing_reward = torch.exp(-((avg_contact_time - 0.4) / 0.3).abs())  # Relaxed from 0.1 to 0.3
```

**TRANSITION SMOOTHNESS AND CONTACT FORCE IMPLEMENTATION:**

**Understanding Phase-Boundary Control and Force Gradation:**
Natural walking requires smooth transitions with controlled contact dynamics.

**Implementation Techniques:**
- **Continuity analysis:** Measure velocity discontinuities at contact events
- **Force profiling:** Analyze contact force ramp characteristics
- **Joint coordination:** Monitor movement smoothness during transitions
- **Impact control:** Evaluate gradual vs abrupt contact patterns

**Practical Implementation Considerations:**
```python
# Example approach for transition smoothness
contact_events = torch.diff(foot_contacts, dim=0)
transition_indices = torch.nonzero(contact_events.abs() > 0.5)
# Measure velocity continuity at transitions
velocity_discontinuity = torch.diff(robot.data.root_lin_vel_b, dim=0)[transition_indices]
smoothness_reward = torch.exp(-5.0 * velocity_discontinuity.norm(dim=-1))
# Contact force gradation
force_gradient = torch.diff(contact_forces.norm(dim=-1), dim=0)
gradual_contact_reward = torch.exp(-0.1 * force_gradient.abs())
```

**LEG ARTICULATION AND JOINT FLEXIBILITY CONSIDERATIONS:**

**Understanding Natural Knee and Hip Coordination:**
Human walking involves specific joint articulation patterns that create efficient, natural-looking movement.

**Key Joint Quality Indicators:**
- **Swing phase knee flexion:** 60-70Â° knee bend for natural foot clearance
- **Smooth articulation:** Gradual joint movements rather than rigid positioning
- **Joint coordination:** Natural knee-hip movement relationships
- **Flexible movement:** Appropriate joint range of motion during gait phases

**Design Thinking for Natural Leg Articulation:**
- Consider knee flexion patterns during different phases of walking
- Think about joint coordination and smooth articulation versus rigidity
- Account for natural joint flexibility and range of motion requirements
- Reward leg movement that demonstrates appropriate joint articulation
- Balance joint mobility with stability and control needs

**COMPREHENSIVE MOVEMENT QUALITY APPROACH:**

**Multi-Dimensional Quality Assessment:**
When designing walking rewards, consider these integrated quality factors:

**Temporal Quality Factors:**
- Step rhythm and cadence consistency
- Phase duration relationships and timing
- Transition smoothness between gait phases
- Movement flow versus mechanical actions

**Spatial Quality Factors:**
- Postural alignment and torso orientation
- Joint articulation and flexibility patterns
- Arm movement freedom and amplitude
- Movement coordination between body segments

**Dynamic Quality Factors:**
- Speed-appropriate movement adaptations
- Natural movement variability within consistent patterns
- Energy-efficient movement characteristics
- Balance between control and natural flow

**CRITICAL: PROVIDE ONLY THE REWARD FUNCTION - NO OBSERVATIONS!**

**NEVER GENERATE:**
- Observation functions or lambda observation expressions
- Environment configurations (ObsTerm, RayCasterCfg, etc.)
- `env.observations.policy.anything = ...` assignments

**ONLY GENERATE:**
- The single `def sds_custom_reward(env) -> torch.Tensor:` function

**CRITICAL: PPO TRAINING WILL CRASH WITH "std >= 0.0" WITHOUT NUMERICAL STABILITY!**

**PPO-SAFE PATTERNS FOR ENVIRONMENTAL DATA (WHEN USING ENVIRONMENTAL SENSORS):**

```python
# Environmental sensor data sanitization (IF using environmental components):
# ONLY include these patterns if you're using environmental sensors

height_scan = height_sensor.data.ray_hits_w[..., 2].view(env.num_envs, -1)
height_scan = torch.where(torch.isfinite(height_scan), height_scan, torch.zeros_like(height_scan))

lidar_range = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)
lidar_range = torch.where(torch.isfinite(lidar_range), lidar_range, torch.ones_like(lidar_range) * 10.0)

# Always clamp extreme values before calculations:
terrain_roughness = torch.clamp(torch.var(height_scan, dim=1), max=1.0)
min_obstacle_distance = torch.clamp(torch.min(lidar_range, dim=1)[0], min=0.1, max=15.0)

# Always prevent division by zero:
error = torch.clamp(error, max=5.0)
reward_component = torch.exp(-error / torch.clamp(std, min=1e-6))

# MANDATORY final reward clamping for PPO stability:
return torch.clamp(reward, min=0.0, max=10.0)
```

IMPORTANT: Provide ONLY the reward function code in ```python ``` blocks. Do NOT include lengthy explanations.

**CRITICAL TECHNICAL REQUIREMENTS**

**Division Safety (Prevents Crashes):**
- For literal numbers: `error / max(0.1, 1e-6)` (CORRECT)
- For tensor variables: `error / torch.clamp(tensor_var, min=1e-6)` (CORRECT)  
- NEVER: `torch.clamp(0.1, min=1e-6)` (WRONG - crashes training)

**Function Structure:**
```python
def sds_custom_reward(env) -> torch.Tensor:
    """Biomechanical goal description"""
    # Your creative problem-solving approach here
    return reward.clamp(min=0.0, max=10.0)
```

## **REWARD DESIGN MINDSET**

**Think Problem-Solving, Not Templates:**
- What specific movement problems need solving?
- How can you measure movement quality vs pathological patterns?
- What makes this gait biomechanically natural?
- How do you encourage coordination when needed?

**Phase-Based Design Approach:**
1. **Identify Natural Phases:** What are the essential phases of this movement cycle?
2. **Design Phase-Specific Rewards:** Different objectives for preparation, execution, recovery phases
3. **Encourage Phase Transitions:** Smooth progressions between movement phases
4. **Ensure Phase Completeness:** Prevent phase skipping or rushing through essential phases
5. **Balance Phase Weights:** Appropriate importance for each phase of the movement cycle

**Creative Design Process for Gait-Specific Excellence:**
1. **Understand Gait Uniqueness:** What makes THIS gait pattern biomechanically distinct from others?
2. **Identify Mastery Metrics:** Beyond basic functionality, what defines exceptional execution of THIS specific movement?
3. **Consider Multiple Approaches:** The examples below are starting points - what novel combinations or alternatives could work better?
4. **Design for Movement Beauty:** How can you reward not just task completion, but natural, efficient, aesthetically pleasing movement?
5. **Think Multi-Scale:** Consider immediate feedback (step quality) and long-term patterns (gait rhythm, consistency)
6. **Innovate Phase Rewards:** Each gait has unique phase characteristics - design phase-specific rewards that capture their essence
7. **Test Creative Logic:** Does your approach truly capture what makes this gait pattern excellent vs merely adequate?

**Gait-Specific Innovation Opportunities:**
- **JUMP:** What novel ways can you measure "perfect" bilateral takeoff? Consider momentum conservation, symmetrical force application, landing prediction
- **WALK:** How can you capture the efficiency of natural pendulum motion? Think about energy recycling, smooth weight transfer, optimal cadence
- **MARCH:** What metrics define controlled precision? Consider postural control during single-limb support, rhythmic consistency, movement crispness
- **SPRINT:** How do you balance speed with sustainability? Think about efficient propulsion, optimal stride parameters, energy expenditure
- **PACE:** What makes lateral movement graceful? Consider stability maintenance during direction changes, spatial efficiency, fluid transitions

**Essential Isaac Lab Access:**
- Robot: `robot = env.scene["robot"]`
- Contact: `contact_sensor = env.scene.sensors["contact_forces"]`
- Commands: `commands = env.command_manager.get_command("base_velocity")`
- Feet: `foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")`

The environment's ContactSensor named "contact_forces" was spawned with:
```yaml
ContactSensorCfg:
  prim_path: "/World/envs/env_.*/Robot/.*_ankle_roll_link"
  force_threshold: 50.0
  track_air_time: True
```

It exposes:
- data.net_forces_w: FloatTensor [num_envs, 2, 3] of foot-link forces
- data.current_air_time / data.current_contact_time: FloatTensor [num_envs, 2] of timing

**IMU Sensor Specification:**
An IMU sensor named "imu" can be spawned via an ImuCfg:
```yaml
ImuCfg:
  prim_path: "/World/envs/env_.*/Robot/torso_link"
  update_period: 0.02  # 50 Hz
  gravity_bias: [0.0, 0.0, 9.81]
```

**Available IMU Access:**
- IMU: `imu_sensor = env.scene.sensors["imu"]`
- Head-pitch: Extract from `imu_sensor.data.quat_w` using quaternion-to-euler
- Yaw velocity: `imu_sensor.data.ang_vel_b[:, 2]` (Z-axis body frame)
- Spatial cues: `imu_sensor.data.pos_w` and `imu_sensor.data.quat_w`

**IMU Pitch Angle Extraction Example:**
```python
# Extract pitch angle from IMU quaternion for backflip spotting cues
imu_sensor = env.scene.sensors["imu"]
quat_w = imu_sensor.data.quat_w  # [num_envs, 4] - (w, x, y, z)
w, x, y, z = quat_w[:, 0], quat_w[:, 1], quat_w[:, 2], quat_w[:, 3]
pitch_angle = torch.atan2(2 * (w * x + y * z), 1 - 2 * (x**2 + y**2))  # Pitch in radians
pitch_degrees = pitch_angle * 180.0 / 3.14159  # Convert to degrees

# Backflip spotting cues (>+10Â° early rotation, <-10Â° before landing)
early_rotation_cue = (pitch_degrees > 10.0).float()
landing_prep_cue = (pitch_degrees < -10.0).float()

# SAFE: Backflip arm control using joint names
left_arm_patterns = [
    "left_shoulder_pitch_joint", "left_shoulder_roll_joint", "left_shoulder_yaw_joint",
    "left_elbow_pitch_joint", "left_elbow_roll_joint"
]
right_arm_patterns = [
    "right_shoulder_pitch_joint", "right_shoulder_roll_joint", "right_shoulder_yaw_joint", 
    "right_elbow_pitch_joint", "right_elbow_roll_joint"
]

# SAFE: Dynamic joint resolution using Isaac Lab API
left_arm_indices, _ = robot.find_joints(left_arm_patterns)
right_arm_indices, _ = robot.find_joints(right_arm_patterns)
# CRITICAL: Convert lists to tensors for proper indexing
left_arm_indices = torch.tensor(left_arm_indices, dtype=torch.long, device=env.device)
right_arm_indices = torch.tensor(right_arm_indices, dtype=torch.long, device=env.device)
left_arm_joints = joint_pos[:, left_arm_indices]
right_arm_joints = joint_pos[:, right_arm_indices]

# SAFE: Bilateral arm symmetry calculation
bilateral_arm_symmetry = torch.exp(-(left_arm_joints - right_arm_joints).abs().mean(dim=1) / 0.5)
```

**Contact Commitment Principle:** Define clear contact states using appropriate thresholds and only reward decisive commitments - avoid rewarding ambiguous in-between states that allow exploitation of threshold boundaries.

**Reward Hacking Warning:** If rewards are not designed cleverly, robots will find alternative ways to maximize rewards instead of performing the intended behavior - ensure your reward function specifically measures the desired movement, not just correlated metrics that can be achieved through shortcuts.

**UNITREE G1 HUMANOID ROBOT SPECIFICATIONS (FULL BODY CONTROL):**

**Joint Configuration (23 DOF Total - All Joints Except Hand Fingers):**
- **LEGS (12 DOF)**: left/right_hip_yaw_joint, left/right_hip_roll_joint, left/right_hip_pitch_joint, left/right_knee_joint, left/right_ankle_pitch_joint, left/right_ankle_roll_joint
- **ARMS (10 DOF)**: left/right_shoulder_pitch_joint, left/right_shoulder_roll_joint, left/right_shoulder_yaw_joint, left/right_elbow_pitch_joint, left/right_elbow_roll_joint  
- **TORSO (1 DOF)**: torso_joint

**EXCLUDED Hand Finger Joints (14 DOF)**: left/right_zero_joint, left/right_one_joint, left/right_two_joint, left/right_three_joint, left/right_four_joint, left/right_five_joint, left/right_six_joint

**Control Philosophy**: Complete humanoid control for natural movement patterns including arm coordination for balance and locomotion

**Joint Position Access for Bilateral Coordination:**
```python
# Access all joint positions
joint_pos = robot.data.joint_pos  # [num_envs, num_joints]

# SAFE G1 ALL JOINTS Resolution (COMPLETE HUMANOID CONTROL):
# Use all joints except hand fingers for full humanoid coordination
# Based on Isaac Lab G1 asset configuration - 23 DOF total

# LEGS (12 DOF) - bilateral leg control
left_leg_patterns = [
    "left_hip_yaw_joint", "left_hip_roll_joint", "left_hip_pitch_joint",
    "left_knee_joint", "left_ankle_pitch_joint", "left_ankle_roll_joint"
]
right_leg_patterns = [
    "right_hip_yaw_joint", "right_hip_roll_joint", "right_hip_pitch_joint", 
    "right_knee_joint", "right_ankle_pitch_joint", "right_ankle_roll_joint"
]

# ARMS (10 DOF) - bilateral arm control
left_arm_patterns = [
    "left_shoulder_pitch_joint", "left_shoulder_roll_joint", "left_shoulder_yaw_joint",
    "left_elbow_pitch_joint", "left_elbow_roll_joint"
]
right_arm_patterns = [
    "right_shoulder_pitch_joint", "right_shoulder_roll_joint", "right_shoulder_yaw_joint", 
    "right_elbow_pitch_joint", "right_elbow_roll_joint"
]

# TORSO (1 DOF) - upper body control
torso_pattern = ["torso_joint"]

# Dynamically resolve joint names to indices (SAFE & ROBUST)
left_leg_indices, _ = robot.find_joints(left_leg_patterns)
right_leg_indices, _ = robot.find_joints(right_leg_patterns)
left_arm_indices, _ = robot.find_joints(left_arm_patterns)
right_arm_indices, _ = robot.find_joints(right_arm_patterns)
torso_indices, _ = robot.find_joints(torso_patterns)

# CRITICAL: Convert all lists to tensors for proper indexing
left_leg_indices = torch.tensor(left_leg_indices, dtype=torch.long, device=env.device)
right_leg_indices = torch.tensor(right_leg_indices, dtype=torch.long, device=env.device)
left_arm_indices = torch.tensor(left_arm_indices, dtype=torch.long, device=env.device)
right_arm_indices = torch.tensor(right_arm_indices, dtype=torch.long, device=env.device)
torso_indices = torch.tensor(torso_indices, dtype=torch.long, device=env.device)

# Full body joint access for comprehensive control
left_leg_joints = joint_pos[:, left_leg_indices]    # 6 joints per leg
right_leg_joints = joint_pos[:, right_leg_indices]  # 6 joints per leg
left_arm_joints = joint_pos[:, left_arm_indices]    # 5 joints per arm
right_arm_joints = joint_pos[:, right_arm_indices]  # 5 joints per arm
torso_joints = joint_pos[:, torso_indices]          # 1 torso joint

# Bilateral coordination for legs AND arms
leg_diff = (left_leg_joints - right_leg_joints).abs().mean(dim=1)      # Leg symmetry
arm_diff = (left_arm_joints - right_arm_joints).abs().mean(dim=1)      # Arm symmetry
bilateral_symmetry = torch.exp(-leg_diff / 0.5) + torch.exp(-arm_diff / 0.5)  # Stable symmetry (additive)
```

**G1 Robot Physical Specifications:**
- **Target height**: 0.74m (maintained during both standing and walking)
- **Height tolerance**: Â±0.05-0.10m natural variation during locomotion
- **Operational range**: 0.25-0.5m (full crouch to full extend)
- **Body mass**: 35.0 kg (for force scaling)
- **Foot contact bodies**: "left_ankle_roll_link", "right_ankle_roll_link"

**Technical Requirements:**
- 4-space indentation, function starts at column 0
- Always specify `device=env.device` for tensors
- Return single torch.Tensor with shape [num_envs]
- Final bounds: `return reward.clamp(min=0.0, max=10.0)`

## **BIOMECHANICAL REWARD STRATEGIES**

**Phase-Based Movement Rewards:**
- **Phase Detection:** Identify current movement phase using contact patterns and velocities
- **Phase-Specific Objectives:** Different reward components for stance, flight, transition phases
- **Phase Progression:** Reward smooth transitions and complete movement cycles
- **Phase Timing:** Appropriate duration and sequence of movement phases

**Movement Quality Metrics:**
- **Smoothness:** Penalize jerky motions through velocity/acceleration analysis
- **Coordination:** For bilateral tasks, measure symmetry between limbs
- **Timing:** Use contact duration and phase relationships for natural rhythms
- **Efficiency:** Reward goal achievement with minimal energy expenditure

**Bilateral Coordination Patterns (CRITICAL for Jumping/Symmetric Gaits):**
```python
# SYNCHRONIZED BILATERAL COORDINATION (for jumping, standing, etc.)
# Both legs should move together with same joint angles
joint_pos = robot.data.joint_pos

# Use safe joint name resolution for ALL controlled joints (23 DOF)
# COMPLETE G1 joint patterns for full humanoid control

# LEGS (12 DOF) 
left_leg_patterns = ["left_hip_yaw_joint", "left_hip_roll_joint", "left_hip_pitch_joint",
                     "left_knee_joint", "left_ankle_pitch_joint", "left_ankle_roll_joint"]
right_leg_patterns = ["right_hip_yaw_joint", "right_hip_roll_joint", "right_hip_pitch_joint", 
                      "right_knee_joint", "right_ankle_pitch_joint", "right_ankle_roll_joint"]

# ARMS (10 DOF)
left_arm_patterns = ["left_shoulder_pitch_joint", "left_shoulder_roll_joint", "left_shoulder_yaw_joint",
                     "left_elbow_pitch_joint", "left_elbow_roll_joint"]
right_arm_patterns = ["right_shoulder_pitch_joint", "right_shoulder_roll_joint", "right_shoulder_yaw_joint", 
                      "right_elbow_pitch_joint", "right_elbow_roll_joint"]

# TORSO (1 DOF)
torso_patterns = ["torso_joint"]

# Resolve all joint indices dynamically
left_leg_indices, _ = robot.find_joints(left_leg_patterns)
right_leg_indices, _ = robot.find_joints(right_leg_patterns)
left_arm_indices, _ = robot.find_joints(left_arm_patterns)
right_arm_indices, _ = robot.find_joints(right_arm_patterns)
torso_indices, _ = robot.find_joints(torso_patterns)

# Full body joint control
left_leg_joints = joint_pos[:, left_leg_indices]   # Safe dynamic indexing
right_leg_joints = joint_pos[:, right_leg_indices]  # Safe dynamic indexing
left_arm_joints = joint_pos[:, left_arm_indices]   # Safe dynamic indexing
right_arm_joints = joint_pos[:, right_arm_indices]  # Safe dynamic indexing
torso_joints = joint_pos[:, torso_indices]         # Safe dynamic indexing

# Combined bilateral symmetry for full body coordination
leg_symmetry = torch.exp(-(left_leg_joints - right_leg_joints).abs().mean(dim=1) / 0.5)
arm_symmetry = torch.exp(-(left_arm_joints - right_arm_joints).abs().mean(dim=1) / 0.5)
joint_symmetry = leg_symmetry + arm_symmetry  # ADDITIVE combination (stable)

# AIR TIME SYMMETRY (for bilateral jumping)
airtime = contact_sensor.data.current_air_time[:, foot_ids]
air_diff = (airtime[:, 0] - airtime[:, 1]).abs()
air_symmetry = torch.exp(-air_diff / 0.3)

# COMBINED BILATERAL REWARD (additive with baseline)
bilateral_reward = joint_symmetry * 1.0 + air_symmetry * 0.5 + 0.2  # Additive with baseline

# BACKFLIP ARM CONTROL COORDINATION PRINCIPLES:
# Phase-specific arm coordination ensures optimal rotational momentum generation and control
# Preparation phase: Arms load backward for momentum buildup
# Takeoff phase: Arms drive upward synchronized with leg extension
# Rotation phase: Arms tuck inward for angular momentum conservation
# Landing phase: Arms extend for deceleration and stability assistance

# SAFE: ARM COORDINATION METRICS FOR BACKFLIP using joint names
# Use joint name-based access for specific joint control
left_shoulder_pitch_idx = left_arm_patterns.index("left_shoulder_pitch_joint")
right_shoulder_pitch_idx = right_arm_patterns.index("right_shoulder_pitch_joint")
left_elbow_pitch_idx = left_arm_patterns.index("left_elbow_pitch_joint")
right_elbow_pitch_idx = right_arm_patterns.index("right_elbow_pitch_joint")

# SAFE: Extract specific joint values using resolved indices
shoulder_pitch_left = left_arm_joints[:, left_shoulder_pitch_idx]
shoulder_pitch_right = right_arm_joints[:, right_shoulder_pitch_idx]
elbow_pitch_left = left_arm_joints[:, left_elbow_pitch_idx]
elbow_pitch_right = right_arm_joints[:, right_elbow_pitch_idx]

# SAFE: Bilateral arm symmetry calculations
arm_symmetry_shoulder = torch.exp(-(shoulder_pitch_left - shoulder_pitch_right).abs() / 0.3)
arm_symmetry_elbow = torch.exp(-(elbow_pitch_left - elbow_pitch_right).abs() / 0.3)
bilateral_arm_coordination = arm_symmetry_shoulder * arm_symmetry_elbow
```

**CRITICAL: AVOIDING REWARD DESIGN PITFALLS FOR SKILL LEARNING**

**Local Minima Recognition:**
Reward functions can accidentally create "comfort zones" where robots achieve moderate rewards through suboptimal strategies, preventing them from learning the intended complex skills. Recognizing and preventing these traps is crucial for successful skill acquisition.

**Skill-Blocking Reward Traps:**

**Movement Avoidance Trap:**
Robot discovers that staying stationary provides stable rewards from balance and smoothness components, so it avoids attempting the target movement entirely. Prevent by ensuring movement-dependent rewards significantly outweigh static comfort when commands are active.

**Threshold Exploitation Trap:**
Robot learns to hover at exact sensor thresholds (contact forces, joint limits, velocities) to technically satisfy conditions without genuine skill execution. Prevent by using gradient-based rewards rather than binary thresholds.

**Comfort Zone Convergence:**
Robot finds one simple movement pattern that achieves acceptable rewards and never explores the full skill space needed for mastery. Prevent by designing rewards that require progressive skill complexity.

**Component Conflict Trap:**
Multiple reward terms pull the robot in contradictory directions, resulting in compromise behaviors that don't master any specific skill. Prevent by ensuring all reward components work synergistically toward the same movement goal.

**Short-Term Optimization Trap:**
Robot optimizes each timestep independently without considering movement sequences or skill completion. Prevent by including temporal rewards that span multiple timesteps and reward complete movement cycles.

**Measurement Gaming Trap:**
Robot learns to manipulate sensor readings or exploit measurement noise rather than performing genuine movements. Prevent by using robust, multi-modal measurement approaches and validating that sensor readings correspond to actual movement quality.

**Strategic Reward Design Principles:**

**Command-Contextual Weighting:**
Adjust reward component weights based on movement commands. When commands request significant movement, movement-tracking rewards should dominate. When commands request minimal movement, stability rewards can be more prominent.

**Progressive Skill Requirements:**
Structure rewards to initially allow simpler approximations of the target skill, then progressively require more precision and completeness. This guides the robot through skill development stages rather than demanding perfect execution immediately.

**Multi-Temporal Reward Horizons:**
Include rewards that operate at different time scales: immediate feedback for safety and basic functionality, medium-term feedback for movement patterns, and longer-term feedback for skill completion and quality.

**Exploration-Encouraging Design:**
Consider small reward bonuses for movement diversity, new joint configurations, or exploring different contact patterns, especially early in training when robots might get stuck in limited movement repertoires.

**Robust Threshold Design:**
Use smooth, gradient-based transitions rather than hard binary thresholds. Instead of rewarding "contact force > 50N", use gradual reward increases that encourage decisive contact without creating exploitable boundaries.

**Alignment Verification Strategy:**
Regularly verify that the easiest path to high rewards corresponds to correct skill execution. If simpler shortcuts exist, redesign rewards to eliminate these alternatives and guide toward genuine skill mastery.

**Problem-Solving Examples:**
- **Single-leg jumping fix:** Use joint position symmetry + air time symmetry (both legs move identically)
- **Asymmetric movement fix:** Compare left vs right leg joint angles with exponential penalty
- **Jerky motion fix:** Penalize high joint velocities or accelerations  
- **Poor balance fix:** Reward upright orientation and controlled center of mass
- **Unnatural contact fix:** Design phase-appropriate contact patterns
- **Phase skipping fix:** Reward complete movement cycles with all necessary phases
- **Harsh transitions fix:** Smooth phase progression with controlled velocities
- **Stationary exploitation fix:** Increase movement-tracking reward weights when commands are non-zero
- **Threshold gaming fix:** Replace binary thresholds with smooth gradients that encourage decisive actions
- **Limited exploration fix:** Add diversity bonuses for exploring new movement configurations

**Bilateral Coordination Implementation Examples (CREATIVE STARTING POINTS):**

**THESE ARE INSPIRATION, NOT TEMPLATES!** Think about what each approach is trying to achieve biomechanically, then consider improvements:

```python
# EXAMPLE 1: Joint position symmetry - prevents asymmetric leg movement
joint_pos = robot.data.joint_pos
# Use safe joint name resolution for COMPLETE HUMANOID control (23 DOF)
# Full G1 joint control - all joints except hand fingers

# LEGS (12 DOF) - for locomotion
left_leg_patterns = ["left_hip_yaw_joint", "left_hip_roll_joint", "left_hip_pitch_joint",
                     "left_knee_joint", "left_ankle_pitch_joint", "left_ankle_roll_joint"]
right_leg_patterns = ["right_hip_yaw_joint", "right_hip_roll_joint", "right_hip_pitch_joint", 
                      "right_knee_joint", "right_ankle_pitch_joint", "right_ankle_roll_joint"]

# ARMS (10 DOF) - for balance and natural movement
left_arm_patterns = ["left_shoulder_pitch_joint", "left_shoulder_roll_joint", "left_shoulder_yaw_joint",
                     "left_elbow_pitch_joint", "left_elbow_roll_joint"]
right_arm_patterns = ["right_shoulder_pitch_joint", "right_shoulder_roll_joint", "right_shoulder_yaw_joint", 
                      "right_elbow_pitch_joint", "right_elbow_roll_joint"]

# TORSO (1 DOF) - for posture
torso_patterns = ["torso_joint"]

# Resolve joint indices using Isaac Lab's find_joints()
left_leg_indices, _ = robot.find_joints(left_leg_patterns)
right_leg_indices, _ = robot.find_joints(right_leg_patterns)
left_arm_indices, _ = robot.find_joints(left_arm_patterns)
right_arm_indices, _ = robot.find_joints(right_arm_patterns)
torso_indices, _ = robot.find_joints(torso_patterns)

# CRITICAL: Convert all lists to tensors for proper indexing
left_leg_indices = torch.tensor(left_leg_indices, dtype=torch.long, device=env.device)
right_leg_indices = torch.tensor(right_leg_indices, dtype=torch.long, device=env.device)
left_arm_indices = torch.tensor(left_arm_indices, dtype=torch.long, device=env.device)
right_arm_indices = torch.tensor(right_arm_indices, dtype=torch.long, device=env.device)
torso_indices = torch.tensor(torso_indices, dtype=torch.long, device=env.device)

# Access all controlled joints (23 DOF total)
left_leg_joints = joint_pos[:, left_leg_indices]   # 6 leg joints
right_leg_joints = joint_pos[:, right_leg_indices] # 6 leg joints
left_arm_joints = joint_pos[:, left_arm_indices]   # 5 arm joints
right_arm_joints = joint_pos[:, right_arm_indices] # 5 arm joints
torso_joints = joint_pos[:, torso_indices]         # 1 torso joint

# SAFE: Full body bilateral symmetry with stable scaling
leg_symmetry = torch.exp(-(left_leg_joints - right_leg_joints).abs().mean(dim=1) / 0.5)  # Stable tolerance
arm_symmetry = torch.exp(-(left_arm_joints - right_arm_joints).abs().mean(dim=1) / 0.5)  # Stable tolerance
joint_symmetry = leg_symmetry + arm_symmetry  # ADDITIVE combination (not multiplicative!)

# EXAMPLE 2: Air time coordination - stable scaling
airtime = contact_sensor.data.current_air_time[:, foot_ids]
timing_symmetry = torch.exp(-((airtime[:, 0] - airtime[:, 1]).abs()) / 0.3)  # Stable tolerant

# EXAMPLE 3: SAFE Combined approach - additive with guaranteed minimum
bilateral_score = joint_symmetry * 1.0 + timing_symmetry * 0.5 + 0.2  # Baseline bonus
```

**CREATIVE EXTENSIONS TO CONSIDER:**
- **Force symmetry:** What if you also measure contact force matching between legs?
- **Momentum symmetry:** Could you compare leg momentum or velocity profiles?
- **Phase-dependent coordination:** Different symmetry requirements for different movement phases?
- **Alternative math:** What about polynomial, sigmoid, or piecewise coordination functions?
- **Multi-scale bilateral:** Joint-level + whole-limb + timing coordination combined?
- **Predictive coordination:** Reward coordination that predicts future movement needs?

**Think:** What is the ESSENCE of good bilateral coordination for YOUR specific gait type?

**Mathematical Creativity Toolkit (STABLE Examples for Non-Zero Rewards):**

**ðŸš¨ WARNING: AVOID ZERO-REWARD MATHEMATICAL PATTERNS!**

**STABLE FUNCTIONS FOR NON-ZERO REWARDS:**

- **Moderate exponential decay:** `torch.exp(-scale * error)` with **scale 0.5-3.0** (NOT 10.0+!)
- **Linear bounded:** `torch.clamp(1.0 - error/tolerance, min=0.0, max=1.0)` - Predictable, stable
- **Soft exponential:** `1.0 / (1.0 + error)` - Never goes to zero, smooth gradients
- **Polynomial rewards:** `(1.0 - (error/max_error)**2)` with **reasonable max_error** (0.5-2.0)
- **Sigmoid activation:** `torch.sigmoid(2.0 * (threshold - error))` - Smooth, bounded
- **Weighted addition:** `reward_a * weight_a + reward_b * weight_b` - **ALWAYS USE ADDITION**

**ðŸš¨ MATHEMATICAL PATTERNS THAT CAUSE ZERO REWARDS (FORBIDDEN):**

- âŒ **Aggressive exponentials:** `torch.exp(-50.0 * error)` - Drives rewards to zero instantly
- âŒ **Tight tolerances:** `torch.exp(-error / 0.1)` - Too sensitive, causes zero rewards  
- âŒ **Multiplicative combinations:** `torch.exp(a) * torch.exp(b)` - Multiplies tiny numbers
- âŒ **Division by small numbers:** `error / 0.05` - Creates huge scaling factors
- âŒ **Complex conditional logic:** Multiple nested conditions that rarely evaluate true

**SAFE SCALING FACTORS FOR EXPONENTIALS:**
- **Height errors (Â±0.1m):** Use `torch.exp(-error / 0.3)` NOT `torch.exp(-error / 0.1)`
- **Velocity errors (Â±0.5m/s):** Use `torch.exp(-error / 1.0)` NOT `torch.exp(-error * 5.0)`
- **Joint differences (Â±0.2 rad):** Use `torch.exp(-error / 0.5)` NOT `torch.exp(-error / 0.1)`

**GUARANTEED NON-ZERO COMBINATION PATTERN:**
```python
# SAFE: Additive combination with minimum baseline
foundation_reward = (
    height_component * 1.0 +     # Individual components 0.0-1.0 range
    velocity_component * 2.0 +   # Reasonable weights
    stability_component * 1.5    # Additive combination
)
baseline_bonus = 0.5  # Ensures non-zero minimum
total_reward = foundation_reward + baseline_bonus
```

**Remember:** Your mathematical choices should reflect the UNDERLYING BIOMECHANICS, not just follow templates!

Your reward function must return a SINGLE torch.Tensor with shape [num_envs] containing the total reward for each environment.

Do NOT return a tuple or dictionary - Isaac Lab expects only the total reward tensor.

The code output should be formatted as a python code string: "```python ... ```".

CRITICAL FORMATTING REQUIREMENTS:
    (FORMAT-1) Use EXACTLY 4 spaces for function body indentation (not 8 spaces or tabs):
         def sds_custom_reward(env) -> torch.Tensor:
             """Docstring"""
             robot = env.scene["robot"]  # 4 spaces
             return reward               # 4 spaces
    
    (FORMAT-2) Function definition MUST start at column 0 (no indentation):
         # WRONG: "    def sds_custom_reward(env) -> torch.Tensor:"
         # CORRECT: "def sds_custom_reward(env) -> torch.Tensor:"
    
    (FORMAT-3) Prevent division by zero - use the correct method for each type:
         # DANGEROUS: reward = numerator / denominator
         # For TENSOR variables: reward = numerator / torch.clamp(tensor_denominator, min=1e-6)
         # For LITERAL numbers: reward = numerator / max(literal_number, 1e-6)
         # EXAMPLE TENSOR: tol_v = torch.clamp(vx_t * 1.2, min=1e-6)  # vx_t is tensor!
         # EXAMPLE LITERAL: reward = error / max(0.1, 1e-6)  # 0.1 is literal - use max()!
    
    (FORMAT-4) Always specify dtype for tensors:
         # WRONG: torch.tensor([0, 0, 1])
         # CORRECT: torch.tensor([0, 0, 1], dtype=torch.float32, device=env.device)

    (FORMAT-5) NEVER mix indentation styles - use ONLY 4 spaces:
         # WRONG MIXED INDENTATION:
         def sds_custom_reward(env) -> torch.Tensor:
                 """8 spaces here"""
             robot = env.scene["robot"]  # 4 spaces here
                 return reward           # 8 spaces here
         
         # CORRECT CONSISTENT INDENTATION:
         def sds_custom_reward(env) -> torch.Tensor:
             """4 spaces"""
             robot = env.scene["robot"]  # 4 spaces
             return reward               # 4 spaces

Some helpful tips for writing the reward function code:
    (1) Use only Isaac Lab environment interface: env.scene["robot"], env.command_manager, env.scene.sensors
    (2) Access robot data through: robot = env.scene["robot"]; robot.data.root_pos_w, robot.data.joint_pos, etc.
    (3) CRITICAL: Use BODY frame velocities (robot.data.root_lin_vel_b, robot.data.root_ang_vel_b)
    (4) Access contact forces through: contact_sensor = env.scene.sensors["contact_forces"]; contact_sensor.data.net_forces_w
    (5) CRITICAL: Get foot indices correctly: foot_ids, foot_names = contact_sensor.find_bodies(".*_ankle_roll_link")
    (6) Make sure all tensors are on the same device: device=env.device
    (7) Return only the total reward tensor, not individual components
    (8) Commands are in body frame: env.command_manager.get_command("base_velocity") gives [vx, vy, omega_z]
    (9) Use torch.norm() for contact force magnitudes and vector operations

CRITICAL ISAAC LAB CONSTRAINTS:
    (10) NEVER call external functions like extract_foot_contacts() or get_foot_contact_analysis()
    (11) Use ONLY inline contact analysis within your reward function
    (12) **CRITICAL: NEVER ADD ANY IMPORT STATEMENTS** - all necessary imports are already available
    (12.1) NEVER use: from omni.isaac.core.utils.quaternion import quat_apply_inverse
    (12.2) NEVER use: from omni.isaac.core import *
    (12.3) DO NOT add any "from" or "import" lines in your function
    (12.4) All Isaac Lab math functions (quat_apply_inverse, yaw_quat, etc.) are already imported at top of file
    (13) For contact analysis, use this pattern:
         contact_forces = contact_sensor.data.net_forces_w
         foot_ids, foot_names = contact_sensor.find_bodies(".*_ankle_roll_link")
         foot_forces = contact_forces[:, foot_ids, :]
         force_magnitudes = foot_forces.norm(dim=-1)
         foot_contacts = force_magnitudes > 50.0
    (14) CRITICAL TENSOR DTYPE: Always use torch.tensor(..., dtype=torch.float32, device=env.device)
         NEVER use torch.tensor([1, 2, 3]) - always specify dtype=torch.float32
    (15) For Isaac Lab math functions like quat_apply_inverse, ensure all tensors are float32
         Example: torch.tensor([0, 0, 1], dtype=torch.float32, device=env.device)
    (16) CRITICAL TENSOR BROADCASTING: When using Isaac Lab math functions (quat_apply_inverse, etc.):
         - Ensure tensor dimensions match the batch size (env.num_envs)
         - For single vectors, expand to batch size: 
           up_vector = torch.tensor([0, 0, 1], dtype=torch.float32, device=env.device).expand(env.num_envs, 3)
         - quat_apply_inverse(quaternions, vectors) expects both inputs to have same batch dimension
         - quaternions shape: [num_envs, 4], vectors shape: [num_envs, 3]
    (17) CRITICAL TENSOR OPERATIONS: For batched operations (multiple environments):
         - NEVER use torch.dot() - it only works with 1D tensors
         - For batched dot products: torch.sum(tensor1 * tensor2, dim=-1)
         - For batched norms: tensor.norm(dim=-1)
         - All operations must preserve the batch dimension [num_envs]
    (18) CRITICAL ISAAC LAB BODY NAMES: For Unitree G1 humanoid robot contact sensing:
         - ONLY these bodies exist: ['left_ankle_roll_link', 'right_ankle_roll_link']
         - NEVER try to find: thigh, shin, calf, hip, base, trunk, or other body parts
         - ONLY use: foot_ids, foot_names = contact_sensor.find_bodies(".*_ankle_roll_link")
         - DO NOT attempt contact_sensor.find_bodies(".*_thigh") or similar
         - G1 humanoid is BIPEDAL (2 feet) - design contact patterns for bipedal locomotion
         - Humanoid contact threshold: Use 50.0N (not 2.0N) for G1 35kg robot

## **ANALYSIS-DRIVEN REWARD DESIGN**

**Video Analysis Questions:**
- What movement pattern is demonstrated?
- Are legs coordinated (bilateral) or alternating?
- What timing characteristics define this gait?
- What problems might prevent natural execution?

**Biomechanical Principles:**
- **Natural Timing:** Rhythmic coordination between limbs
- **Movement Quality:** Smooth, controlled, energy-efficient patterns
- **Safety:** Stable orientation and controlled impacts
- **Task-Specific:** Match reward structure to demonstrated behavior

**Innovation Over Templates:**
- Don't copy existing patterns - solve specific problems
- Create novel metrics for movement quality assessment
- Use different mathematical functions for different objectives
- Design rewards that distinguish natural vs pathological movement

    (19) CRITICAL ROBOT DATA AVAILABILITY: 
         - Available: robot.data.joint_pos, robot.data.joint_vel, robot.data.root_pos_w, robot.data.root_quat_w
         - Available: robot.data.root_lin_vel_w, robot.data.root_ang_vel_b, robot.data.root_ang_vel_w, robot.data.applied_torque
         - Available: contact_sensor.data.current_air_time, contact_sensor.data.last_air_time (for step timing rewards)  
         - AVOID: robot.data.joint_acc (joint accelerations) - uses unstable finite differencing
         - For smoothness metrics, use robot.data.joint_vel (velocities) instead of accelerations
         - Joint accelerations computed via finite differencing become numerically unstable during training

    (19.5) CONTACT SENSOR TIMING DATA FOR GAIT REWARDS:
         Isaac Lab provides rich timing data for sophisticated gait control:
         
         **AVAILABLE CONTACT TIMING DATA:**
         - contact_sensor.data.last_air_time[:, foot_ids]        # Duration of previous aerial phase
         - contact_sensor.data.current_air_time[:, foot_ids]     # Current ongoing air time
         - contact_sensor.data.last_contact_time[:, foot_ids]    # Duration of previous ground contact 
         - contact_sensor.data.current_contact_time[:, foot_ids] # Current ongoing contact time
         - contact_sensor.compute_first_contact(env.step_dt)[:, foot_ids]  # Just-landed detection
         
         **PROPER AIR TIME USAGE:**
         # For gaits with aerial phases (jumping, sprinting, dynamic gaits):
         air_threshold = 0.05  # Minimum meaningful air time in seconds
         meaningful_air = (last_air_time > air_threshold) & (first_contact > 0)
         air_reward = torch.sum(meaningful_air.float(), dim=1)
         
         # For sustained aerial phases (all feet airborne):
         all_airborne = (current_air_time > 0.0).all(dim=1)
         sustained_air_reward = torch.where(all_airborne, current_air_time.mean(dim=1), 0.0)
         
         **AVOID OVERLY SIMPLE CONTACT DETECTION:**
         # Don't use only: (foot_contacts.sum(dim=-1) == 0).float()  # Too basic
         # This misses timing, duration, and rhythm which are crucial for natural gaits
         
         **DESIGN FOR SPECIFIC GAIT REQUIREMENTS:**
         - **Bilateral gaits:** Measure and reward symmetry between limbs
         - **Alternating gaits:** Reward proper phase relationships and timing
         - **Aerial gaits:** Consider flight duration and landing control
         - **Stability gaits:** Focus on balance and controlled movement patterns
         
         **CRITICAL: JUMPING REQUIRES SYNCHRONIZED LEG COORDINATION:**
         # JUMPING (vertical bouncing, both legs synchronized):
         num_contacts = foot_contacts.sum(dim=-1)
         jump_states = (num_contacts == 0) | (num_contacts == 2)  # Only airborne OR both-contact
         jump_reward = jump_states.float() + vertical_velocity_emphasis + synchronized_timing_reward
         
         **PRECISION BIOMECHANICAL SPECIFICATIONS FOR JUMPING:**
         # Center-of-Mass Trajectory: Smooth parabolic arc, peak at 0.15-0.20s into flight
         # Ground Reaction Force: Push-off 40-60ms rise, peak 1.2-1.5Ã— body weight
         # Landing impulse: <30ms rise time, <2.5Ã— body weight peak
         # Joint Angles: Prep (hip 20-30Â°, knee 60-90Â°), Take-off (hip 10-15Â° past neutral, knee full extension)
         # Temporal Durations: Prep 0.15-0.25s, Take-off 0.10-0.15s, Flight 0.20-0.40s, Landing 0.15-0.25s
         # Stability: Roll within Â±5Â°, yaw drift control, torso pitch Â±5Â° during flight
         # Foot Clearance: 10-25mm toe clearance, foot orientation Â±5Â° at landing
         # Failure Prevention: Early touchdown penalties, asymmetric take-off detection
         
         # For humanoid locomotion, use gait-appropriate contact patterns
# WALK: Alternating feet with double support phases
# JUMP: Synchronized takeoff/landing with flight phases
# MARCH: Controlled alternating with stability focus
# SPRINT: Extended flight phases with minimal contact
# PACE: Lateral movement with stable contact
    (20) Design rewards that match the demonstrated locomotion pattern. Analyze the video to understand the specific movement requirements.
    (21) CRITICAL PYTORCH API: torch.clamp() ONLY works on tensors, NEVER on scalar numbers:
         # CAUSES TRAINING CRASH: torch.clamp(0.05, min=1e-6)  # 0.05 is NUMBER!
         # CAUSES TRAINING CRASH: torch.clamp(1.2, min=0.1)    # 1.2 is NUMBER!
         # CORRECT: torch.clamp(tensor_variable, min=1e-6)      # Only for tensors
         # CORRECT: max(0.05, 1e-6)                             # Use max() for numbers

STABLE MATHEMATICAL PATTERNS (GUARANTEED NON-ZERO REWARDS):
    (22) âœ… Use MODERATE exponential decay: torch.exp(-factor * error.abs()) with factors (0.5 to 3.0) - NOT 10.0+!
    (23) âœ… Use bounded linear for stability: (1.0 - error.abs()/tolerance).clamp(min=0.0, max=1.0)
    (24) âœ… Use boolean masks for gait patterns: ((condition1) & (condition2)).float()
    (25) âœ… ALWAYS clamp final reward with MINIMUM: return reward.clamp(min=0.1, max=10.0)
    (26) âœ… ALWAYS use ADDITIVE combinations: reward_a + reward_b (NOT reward_a * reward_b)
    (27) âœ… Use REASONABLE tolerances: /0.3 or /0.5 (NOT /0.1 or /0.05)
    (28) âœ… Include BASELINE BONUS: total_reward = main_reward + 0.2 (ensures non-zero minimum)
    (26) DIVISION SAFETY: ONLY use torch.clamp() for TENSOR variables, NEVER for literal numbers:
         # For TENSOR variables: torch.clamp(tensor_denominator, min=1e-6)
         # For LITERAL numbers: max(literal_number, 1e-6)
         # Example TENSOR: tol_v = torch.clamp(vx_t * 1.2, min=0.1)  # vx_t is tensor!
         # Example LITERAL: reward = error / max(0.15, 1e-6)  # 0.15 is literal number
    (27) VELOCITY TOLERANCE SAFETY: For velocity-based rewards, ensure tolerances are never zero:
         # DANGEROUS: tol_v = vx_t * 1.2  # Can be zero when vx_t=0
         # SAFE: tol_v = torch.clamp(vx_t * 1.2, min=0.1)  # vx_t is tensor!
         # NEVER: tol_v = torch.clamp(0.1, min=1e-6)  # 0.1 is NUMBER - CRASHES!

**Important Tips for Code Generation:**

Remember to follow these guidelines when generating your reward function code:

(1) Always ensure your reward function follows the exact Isaac Lab function signature format
(2) Always specify dtype=torch.float32 and device=env.device for tensor creation
(3) Use inline contact analysis code - do NOT call external functions like extract_foot_contacts() 
(4) Provide comments explaining your reward design choices and mathematical formulations
(5) Test your mathematical formulations for edge cases (zero velocity, maximum velocity, etc.)
(6) Balance reward components appropriately and consider their relative magnitudes
(7) Use stable mathematical operations and avoid division by potentially small numbers
(8) Consider computational efficiency - use vectorized operations when possible
(9) Make sure all tensor operations are differentiable for gradient-based learning
(10) Include proper error handling and validation for tensor shapes and dimensions

Common Isaac Lab tensor operations:
(11) For vector creation: torch.tensor([x, y, z], dtype=torch.float32, device=env.device)
(12) For batch operations: expand tensors to match env.num_envs dimension
(13) For dot products: use torch.sum(tensor1 * tensor2, dim=-1) instead of torch.dot()
  (14) For contact detection: use force_threshold > 50.0 for reliable contact sensing (G1 humanoid)
    (15) For foot identification: use contact_sensor.find_bodies(".*_ankle_roll_link") pattern

Mathematical patterns for stability:
(16) Use exponential decay for tracking: torch.exp(-scale * error) with reasonable scale
(17) Use linear bounded rewards: (1.0 - error/tolerance).clamp(min=0.0, max=1.0)
(18) Use smooth transitions instead of hard thresholds when possible
(19) Normalize different reward components to similar magnitude ranges
(20) Include safety constraints through penalty terms rather than hard constraints

Critical implementation notes:
(21) NEVER use torch.clamp(tensor, device=device) - device parameter not supported
(22) Use robot.data.joint_vel for smoothness (joint_acc uses unstable finite differencing)
    (23) ONLY use foot bodies: ['left_ankle_roll_link', 'right_ankle_roll_link'] for Unitree G1
(24) Use boolean masks for gait patterns: ((condition1) & (condition2)).float()
(25) ALWAYS clamp final reward to prevent numerical instability: return reward.clamp(min=0.0, max=10.0)
(26) INDENTATION SAFETY: Function definition at column 0, body indented exactly 4 spaces
(27) DIVISION SAFETY: Use the correct method for each type:
     # For TENSOR variables: torch.clamp(tensor_denominator, min=1e-6)
     # For LITERAL numbers: max(literal_number, 1e-6)
     # NEVER mix these up - literal numbers cannot use torch.clamp()

Normalization tip: Use .clamp(min=0.0, max=1.0) to keep individual reward components in 0-1 range.

## TEMPORAL CONSIDERATIONS

Consider that humanoid locomotion involves temporal patterns. When analyzing contact states or movement patterns, account for how behaviors change over time to create natural, coordinated movement while maintaining balance.

**Important**: Design rewards that promote balanced and coordinated leg movements to ensure natural bipedal locomotion patterns with proper stability.

**CRITICAL FORMATTING REQUIREMENTS**

MOST COMMON ERRORS CAUSING TRAINING CRASHES:

1. INDENTATION ERRORS:
   **CORRECT: Use exactly 4 spaces for function body**
   ```python
   def sds_custom_reward(env) -> torch.Tensor:
       """Description"""
       robot = env.scene["robot"]
       reward = torch.zeros(env.num_envs, device=env.device)
       return reward.clamp(min=0.0, max=10.0)
   ```
   
   **WRONG: 8 spaces, tabs, or inconsistent spacing**
   ```python
   def sds_custom_reward(env) -> torch.Tensor:
           robot = env.scene["robot"]  # 8 spaces - CRASHES!
   ```

2. DIVISION BY ZERO ERRORS:
   **#1 CAUSE OF CRASHES: torch.clamp() on literal numbers**
   
   **CORRECT PATTERNS:**
   - LITERAL NUMBERS: 0.1, 2.0, 0.15 (typed directly) â†’ Use max()
   - TENSOR VARIABLE: vx_command, height_error, tolerance (computed from robot data) â†’ Use torch.clamp()
   
   ```python
   # CORRECT: max() for literal numbers
   reward = error / max(0.1, 1e-6)        # 0.1 is typed directly
   reward = error / max(2.0, 1e-6)        # 2.0 is typed directly
   
   # CORRECT: torch.clamp() for tensor variables
   tolerance = commands[:, 0] * 1.2       # tolerance computed from robot data
   reward = error / torch.clamp(tolerance, min=1e-6)  # Works!
   
   # CRASHES: torch.clamp() on literal numbers
   reward = error / torch.clamp(0.1, min=1e-6)   # 0.1 is literal - CRASHES!
   reward = error / torch.clamp(2.0, min=1e-6)   # 2.0 is literal - CRASHES!
   ```

3. TENSOR CREATION ERRORS:
   **ALWAYS specify dtype and device:**
   ```python
   up_vector = torch.tensor([0, 0, 1], dtype=torch.float32, device=env.device)
   ```
   
   **Missing dtype creates Long tensors (crashes):**
   ```python
   up_vector = torch.tensor([0, 0, 1])  # Creates Long tensor - CRASHES!
   ```

ISAAC LAB SPECIFIC REQUIREMENTS:

- Function must start at column 0 (no indentation)
- Use exactly 4 spaces for all indentation inside function
- Always return reward.clamp(min=0.0, max=10.0) for PPO stability
- All tensors must use device=env.device
- Contact bodies: ONLY "left_ankle_roll_link", "right_ankle_roll_link" exist

LOCOMOTION DESIGN PRINCIPLES:

Analyze the video frames to understand the specific locomotion pattern:
- Dynamic vs static height control requirements
- Contact force patterns and timing
- Body orientation flexibility needs
- Speed and movement characteristics

Design rewards that match the observed locomotion while ensuring real-world deployment success.

Remember: Different gaits require different reward structures - avoid hardcoding values that favor specific locomotion patterns.

CRITICAL: Always specify dtype=torch.float32 and device=env.device for tensor creation!

**HUMANOID-SPECIFIC REWARD DESIGN TIPS (Avoid Awkward Behavior):**

(20) TORSO STABILITY IS CRITICAL:
     - Always include orientation penalties: `torch.sum(torch.square(robot.data.projected_gravity_b[:, :2]), dim=1)`
     - Heavy weight on upright posture (5.0-30.0x) - falling prevention is priority #1
     - Penalize excessive roll/pitch angular velocities

(21) FORWARD MOVEMENT BIAS:
     - Weight forward velocity tracking 3x higher than lateral movement
     - G1 humanoid bipeds are forward-optimized for efficient locomotion
     - Use tighter tolerance for forward tracking (Ïƒ=0.25) vs lateral (Ïƒ=0.5)

(22) ACTION SMOOTHNESS PREVENTS JERKY MOTION:
     - Include action rate penalties: `torch.sum(torch.square(env.actions - env.prev_actions), dim=1)`
     - Scale factor 0.01-0.1 prevents robotic, unnatural movement
     - Essential for natural-looking locomotion

(23) HEIGHT CONTROL BALANCE:
     - Target height 0.74m for G1 humanoid walking (confirmed from specs)
     - Moderate tolerance (0.1m) allows natural walking dynamics
     - Avoid overly rigid height control that creates stiff movement

(24) JOINT LIMIT SAFETY:
     - Heavy penalties for approaching 90% of joint limits: scale factor 10.0-30.0
     - Prevents damage and maintains natural range of motion
     - Use `robot.data.soft_joint_pos_limits` for proper bounds

(25) CONTACT FORCE MANAGEMENT:
     - G1 humanoid ~35kg: expect ~175N per foot when both contact ground
     - Penalty threshold around 200N prevents excessive foot forces
     - Use `contact_sensor.find_bodies(".*_ankle_roll_link")` for foot detection

(26) ENERGY EFFICIENCY:
     - Joint velocity penalties prevent excessive movement: scale 0.001
     - Include all controlled joints (legs, arms, torso) for natural humanoid coordination
     - Don't over-penalize natural movement requirements

(27) GAIT PATTERN MATCHING:
     - WALK: 1 or 2 feet contact (alternating single + double support)
- JUMP: 0 or 2 feet contact (synchronized takeoff/landing + flight phases)
- MARCH: 1 foot contact (single support for control)
- SPRINT: 0 or 1 feet contact (extended flight phases)
- PACE: 1 or 2 feet contact (lateral movement stability)
- Match pattern to demonstrated video behavior

**PROVEN WEIGHT HIERARCHY (Based on G1 Humanoid RSL-RL Success):**
Priority 1 - Safety: Joint limits, orientation (10.0-30.0x)
Priority 2 - Stability: Height, contact forces (5.0-10.0x)  
Priority 3 - Performance: Velocity tracking (1.0-5.0x)
Priority 4 - Quality: Smoothness, efficiency (0.1-1.0x)

**COMMON HUMANOID MISTAKES TO AVOID:**
Equal forward/lateral weighting â†’ Use 3:1 forward bias
Missing orientation penalties â†’ Always include upright stability  
No action rate damping â†’ Causes jerky, robotic motion
Overly rigid height control â†’ Prevents natural walking dynamics
Ignoring upper body â†’ Torso stability critical for bipeds
Static contact thresholds â†’ Should adapt to movement intensity

**JOINT LIMITS AND SAFE MOVEMENT PRACTICES - MANDATORY FOR ALL TASKS:**

**WHY JOINT LIMITS MATTER:**
Joint limits prevent the robot from reaching unstable configurations that cause:
- Weird, unnatural movements and poses
- Joint instability and oscillations  
- Physics simulation crashes
- Training divergence due to unrealistic states

**CRITICAL: Every reward function MUST include joint limit safety components:**

### **1. ESSENTIAL JOINT LIMIT PENALTY PATTERN:**

```python
# MANDATORY: Include this joint limit safety pattern in every reward function
# Joint limit safety penalty (inline implementation)
limits = robot.data.soft_joint_pos_limits  # [num_envs, num_joints, 2]
joint_center = (limits[..., 0] + limits[..., 1]) * 0.5
joint_range = limits[..., 1] - limits[..., 0]
pos_norm = (robot.data.joint_pos - joint_center) / torch.clamp(joint_range / 2.0, min=1e-6)
over = pos_norm.abs() > 0.8
joint_limit_penalty = torch.sum(
    over.float() * torch.exp(3.0 * (pos_norm.abs() - 0.8)),
    dim=1
)
```

### **2. MOTION SMOOTHNESS (INLINE PATTERN):**

```python
# Motion smoothness penalty (inline implementation)  
joint_vel_norm = torch.norm(robot.data.joint_vel, dim=1)
smooth_motion_reward = torch.exp(-joint_vel_norm * 0.01)
```

### **3. SIMPLIFIED JOINT LIMIT SAFETY (RECOMMENDED PATTERN):**

```python
# Simplified joint limit safety - use this pattern for reliable generation
limits = robot.data.soft_joint_pos_limits  # [num_envs, num_joints, 2]
joint_center = (limits[..., 0] + limits[..., 1]) * 0.5
joint_range = limits[..., 1] - limits[..., 0]
pos_norm = (robot.data.joint_pos - joint_center) / torch.clamp(joint_range / 2.0, min=1e-6)
over = pos_norm.abs() > 0.8  # 80% threshold for safety
joint_limit_penalty = torch.sum(
    over.float() * torch.exp(3.0 * (pos_norm.abs() - 0.8)),
    dim=1
)
```

### **4. COMPLETE REWARD FUNCTION TEMPLATE WITH JOINT SAFETY:**

```python
def sds_custom_reward(env) -> torch.Tensor:
    """Template showing proper integration of joint limit safety."""
    import torch
    
    robot = env.scene["robot"]
    contact_sensor = env.scene.sensors["contact_forces"]
    
    # YOUR MAIN TASK REWARD LOGIC HERE
    # ... (task-specific rewards like height, velocity, etc.)
    main_task_reward = torch.zeros(env.num_envs, device=env.device)
    # ... implement your specific task rewards ...
    
    # MANDATORY: Joint limit safety (ALWAYS INCLUDE)
    limits = robot.data.soft_joint_pos_limits  # [num_envs, num_joints, 2]
    joint_center = (limits[..., 0] + limits[..., 1]) * 0.5
    joint_range = limits[..., 1] - limits[..., 0]
    pos_norm = (robot.data.joint_pos - joint_center) / torch.clamp(joint_range / 2.0, min=1e-6)
    over = pos_norm.abs() > 0.8
    joint_limit_penalty = torch.sum(
        over.float() * torch.exp(3.0 * (pos_norm.abs() - 0.8)),
        dim=1
    )
    
    # RECOMMENDED: Motion smoothness (prevents jerky movement)
    joint_vel_norm = torch.norm(robot.data.joint_vel, dim=1)
    smooth_motion_reward = torch.exp(-joint_vel_norm * 0.01)
    
    # COMBINE ALL COMPONENTS
    reward = (
        main_task_reward * 1.0 +              # Your main task (weight 1.0)
        smooth_motion_reward * 0.1 -          # Smoothness bonus
        joint_limit_penalty * 0.5             # Joint limit penalty
    )
    
    return reward.clamp(min=0.0, max=10.0)
```

### **6. DEBUGGING JOINT LIMIT VIOLATIONS:**

```python
# Include this for debugging during development (small num_envs only)
if env.num_envs <= 8:  # Only for debugging runs
    # Check for violations
    pos_violations = torch.any(
        (robot.data.joint_pos < robot.data.soft_joint_pos_limits[..., 0]) |
        (robot.data.joint_pos > robot.data.soft_joint_pos_limits[..., 1]), dim=1
    )
    if torch.any(pos_violations):
        print(f"âš ï¸  Joint limit violations in envs: {{{torch.where(pos_violations)[0].tolist()}}}")
        
        # Print which joints are violating
        for env_idx in torch.where(pos_violations)[0]:
            violations = (
                (robot.data.joint_pos[env_idx] < robot.data.soft_joint_pos_limits[env_idx, :, 0]) |
                (robot.data.joint_pos[env_idx] > robot.data.soft_joint_pos_limits[env_idx, :, 1])
            )
            violating_joints = torch.where(violations)[0]
            print(f"   Env {{env_idx}}: joints {{violating_joints.tolist()}}")
```

**CRITICAL REMINDERS:**
1. **ALWAYS** include joint limit penalties in every reward function
2. **Use soft_joint_pos_limits**, not hard joint_pos_limits
3. **Start penalties at 70-80% of soft range**, not at the limits
4. **Use exponential penalties** for smooth gradients
5. **Weight limit penalties 0.3-0.5** of main task reward
6. **Include velocity smoothness** to prevent jerky motion
7. **Consider joint-specific thresholds** based on importance

This systematic approach ensures stable, natural robot movement while achieving task objectives.

**âš ï¸  ENVIRONMENTAL SENSOR INTEGRATION (ANALYSIS-DRIVEN, NOT MANDATORY)**

**IMPORTANT: Environmental integration should be based on actual environment analysis, not forced requirements!**

**ðŸ“‹ CLARIFICATION: Use existing sensor data WITHIN your reward function ONLY IF RELEVANT - do NOT generate observation configurations!**

**ENVIRONMENTAL COMPONENTS (INCLUDE ONLY IF ENVIRONMENT ANALYSIS SHOWS RELEVANCE):**

**1. TERRAIN ANALYSIS (ONLY IF TERRAIN SHOWS SIGNIFICANT VARIATION):**
   - **Check first**: Does environment analysis show terrain roughness > 0.1 or elevation changes?
   - **If NO**: Skip terrain analysis, focus on basic locomotion
   - **If YES**: Include simplified terrain adaptation
   ```python
   # ONLY include if terrain analysis shows variation
   height_sensor = env.scene.sensors["height_scanner"]
   height_scan = height_sensor.data.ray_hits_w[..., 2].view(env.num_envs, -1)
   height_scan = torch.where(torch.isfinite(height_scan), height_scan, torch.zeros_like(height_scan))
   terrain_roughness = torch.clamp(torch.var(height_scan, dim=1), max=1.0)
   terrain_bonus = torch.exp(-terrain_roughness * 1.0) * 0.3
   ```

**2. OBSTACLE DETECTION (ONLY IF OBSTACLES ARE PRESENT):**
   - **Check first**: Does environment analysis show obstacles or barriers?
   - **If NO**: Skip obstacle detection, focus on basic locomotion
   - **If YES**: Include basic obstacle awareness
   ```python
   # ONLY include if environment analysis shows obstacles
   lidar_sensor = env.scene.sensors["lidar"]
   lidar_range = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1).view(env.num_envs, -1)
   lidar_range = torch.where(torch.isfinite(lidar_range), lidar_range, torch.ones_like(lidar_range) * 10.0)
   min_obstacle_distance = torch.min(lidar_range[:, :lidar_range.shape[1] // 4], dim=1)[0]
   obstacle_bonus = torch.clamp((min_obstacle_distance - 1.0) / 2.0, min=0.0, max=0.5)
   ```

**3. GAP NAVIGATION (INCLUDE IF GAPS ARE DETECTED):**
   - **Check first**: Does environment analysis show gaps detected (count > 0)?
   - **If NO**: Skip gap navigation, focus on basic locomotion
   - **If YES**: Include gap size detection and adaptive navigation
   ```python
   # ONLY include if environment analysis shows gaps detected
   # Real-time gap size detection for adaptive navigation
   robot_height = robot.data.root_pos_w[:, 2]
   forward_terrain = height_scan[:, :height_scan.shape[1]//3]
   gap_depth = robot_height.unsqueeze(1) - forward_terrain
   gap_detected = gap_depth > 0.15
   small_gaps = (gap_depth <= 0.30) & gap_detected  # Steppable
   medium_gaps = (gap_depth > 0.30) & (gap_depth <= 0.60) & gap_detected  # Jumpable
   gap_navigation_bonus = torch.any(small_gaps, dim=1).float() * 0.2 + torch.any(medium_gaps, dim=1).float() * 0.3
   ```

**4. ENVIRONMENTAL ADAPTATION (ONLY IF ENVIRONMENT ANALYSIS INDICATES NECESSITY):**
   Choose ONLY relevant components based on environment analysis:
   - **Flat terrain with no obstacles**: Skip environmental adaptation entirely
   - **Rough terrain**: Include terrain roughness adaptation
   - **Gap environments**: Include gap size detection ONLY if gaps present
   - **Obstacle environments**: Include obstacle avoidance ONLY if obstacles present

**5. ENVIRONMENTAL SAFETY (ONLY IF ENVIRONMENTAL HAZARDS DETECTED):**
   - **Safe environments**: Focus on locomotion safety (joint limits, orientation)
   - **Hazardous environments**: Add environmental hazard avoidance

**ENVIRONMENT ANALYSIS VALIDATION:**
Before including ANY environmental components, verify:
- âœ… Environment analysis shows relevant features (gaps, obstacles, terrain variation)
- âœ… Environmental features significantly impact locomotion requirements
- âœ… Basic locomotion foundation is stable before adding environmental complexity
- âœ… Environmental components enhance rather than compete with locomotion goals

**ðŸ”§ STABLE REWARD DESIGN PATTERNS (GUARANTEED NON-ZERO REWARDS):**

**CRITICAL: Use these SAFE mathematical patterns to prevent zero rewards:**

```python
# FOUNDATION LOCOMOTION PATTERN (ALWAYS STABLE):
def sds_custom_reward(env) -> torch.Tensor:
    robot = env.scene["robot"]
    
    # === FOUNDATION: ALWAYS INCLUDE ===
    # Height maintenance (moderate tolerance)
    height_error = (robot.data.root_pos_w[:, 2] - 0.74).abs()
    height_reward = torch.exp(-height_error / 0.3)  # Relaxed tolerance
    
    # Velocity tracking (safe scaling)
    commands = env.command_manager.get_command("base_velocity")
    lin_vel = robot.data.root_lin_vel_b[:, 0]
    vel_error = (lin_vel - commands[:, 0]).abs()
    vel_tolerance = torch.clamp(torch.abs(commands[:, 0]) * 0.8 + 0.5, min=0.5)  # Safe minimum
    vel_reward = torch.exp(-vel_error / vel_tolerance)
    
    # Orientation stability (moderate scaling)
    gravity_proj = robot.data.projected_gravity_b[:, :2]
    lean_reward = torch.exp(-2.0 * torch.norm(gravity_proj, dim=1))  # Gentle scaling
    
    # === BASIC REWARD (FOUNDATION ONLY) ===
    foundation_reward = (
        height_reward * 2.0 +      # Priority: height maintenance
        vel_reward * 3.0 +         # Priority: velocity tracking  
        lean_reward * 1.5 +        # Priority: stability
        0.5                        # Baseline bonus (prevents zero)
    )
    
    # === ADD ENVIRONMENTAL COMPONENTS IF ENVIRONMENT ANALYSIS SHOWS FEATURES ===
    # CHECK: Does environment analysis show gaps (>0), obstacles (>0), or terrain roughness (>20cm)?
    # IF YES: Add terrain_bonus, obstacle_bonus, gap_navigation_bonus as shown in examples above
    # IF NO: Keep foundation-only approach
    
    return foundation_reward.clamp(min=0.1, max=10.0)  # Guaranteed non-zero
```

**SAFE MATHEMATICAL PATTERNS:**
- **Exponential decay**: Use factors 0.5-2.0 (NOT 5.0+!)  
- **Tolerances**: Use 0.3-1.0 (NOT 0.1 or smaller!)
- **Combinations**: ALWAYS additive (a + b), NEVER multiplicative (a * b)
- **Baseline bonus**: Always include +0.1 to +0.5 baseline
- **Final clamping**: Always use .clamp(min=0.1, max=10.0)

**DANGEROUS PATTERNS TO AVOID:**
- âŒ Tight tolerances: `/0.1` or `/0.05` (causes zero rewards)
- âŒ Aggressive exponentials: `torch.exp(-10.0 * error)` (drives to zero)
- âŒ Multiplicative chains: `reward_a * reward_b * reward_c` (multiplies small numbers)
- âŒ Complex conditionals: Multiple nested if-statements (rarely activate)
- âŒ Hard thresholds: Binary on/off rewards (exploitation-prone)

**ðŸ† PROVEN ISAAC LAB LOCOMOTION PATTERNS (RECOMMENDED FOR ROBUST WALKING):**

**SUPERIOR VELOCITY TRACKING (Yaw-Aligned Frame):**
```python
# PROVEN: Isaac Lab approach - yaw-aligned frame tracking (much better than body frame)
from isaaclab.utils.math import quat_apply_inverse, yaw_quat

commands = env.command_manager.get_command("base_velocity")
# Transform velocity to yaw-aligned frame (removes pitch/roll effects)
vel_yaw = quat_apply_inverse(yaw_quat(robot.data.root_quat_w), robot.data.root_lin_vel_w[:, :3])
lin_vel_error = torch.sum(torch.square(commands[:, :2] - vel_yaw[:, :2]), dim=1)
vel_reward = torch.exp(-lin_vel_error / (1.0**2))

# PROVEN: Command scaling - no reward for zero commands
command_magnitude = torch.norm(commands[:, :2], dim=1)
vel_reward *= (command_magnitude > 0.1).float()
```

**PROPER BIPEDAL GAIT PATTERNS:**
```python
# PROVEN: Isaac Lab bipedal air time - rewards single stance phases
contact_sensor = env.scene.sensors["contact_forces"]
foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)

air_time = contact_sensor.data.current_air_time[:, foot_ids]
contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
in_contact = contact_time > 0.0

# Reward single stance (proper bipedal pattern)
single_stance = torch.sum(in_contact.int(), dim=1) == 1
in_mode_time = torch.where(in_contact, contact_time, air_time)
gait_reward = torch.min(torch.where(single_stance.unsqueeze(-1), in_mode_time, 0.0), dim=1)[0]
gait_reward = torch.clamp(gait_reward, max=0.5)  # Cap air time

# Scale by command magnitude
gait_reward *= (command_magnitude > 0.1).float()
```

**CONTACT-AWARE FOOT SLIDING PENALTY:**
```python
# PROVEN: Isaac Lab foot sliding - only penalize when in contact
forces = contact_sensor.data.net_forces_w_history[:, :, foot_ids, :]
contacts = forces.norm(dim=-1).max(dim=1)[0] > 1.0  # Historical contact
body_vel = robot.data.body_lin_vel_w[:, foot_ids, :2]
slide_penalty = torch.sum(body_vel.norm(dim=-1) * contacts, dim=1)
```

**IMPROVED FOUNDATION TEMPLATE WITH ISAAC LAB PATTERNS:**
```python
def sds_custom_reward(env) -> torch.Tensor:
    """Robust walking reward using proven Isaac Lab patterns."""
    from isaaclab.utils.math import quat_apply_inverse, yaw_quat
    
    robot = env.scene["robot"]
    contact_sensor = env.scene.sensors["contact_forces"]
    
    # === PROVEN VELOCITY TRACKING (YAW-ALIGNED FRAME) ===
    commands = env.command_manager.get_command("base_velocity")
    command_magnitude = torch.norm(commands[:, :2], dim=1)
    
    # Transform to yaw-aligned frame (much better than body frame)
    vel_yaw = quat_apply_inverse(yaw_quat(robot.data.root_quat_w), robot.data.root_lin_vel_w[:, :3])
    lin_vel_error = torch.sum(torch.square(commands[:, :2] - vel_yaw[:, :2]), dim=1)
    vel_reward = torch.exp(-lin_vel_error / (1.0**2))
    vel_reward *= (command_magnitude > 0.1).float()  # No reward for zero commands
    
    # === ROBUST BIPEDAL GAIT PATTERN ===
    foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
    foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)
    
    air_time = contact_sensor.data.current_air_time[:, foot_ids]
    contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
    in_contact = contact_time > 0.0
    
    # Reward proper single stance phases
    single_stance = torch.sum(in_contact.int(), dim=1) == 1
    in_mode_time = torch.where(in_contact, contact_time, air_time)
    gait_reward = torch.min(torch.where(single_stance.unsqueeze(-1), in_mode_time, 0.0), dim=1)[0]
    gait_reward = torch.clamp(gait_reward, max=0.5) * (command_magnitude > 0.1).float()
    
    # === STABLE HEIGHT & ORIENTATION ===
    height_error = (robot.data.root_pos_w[:, 2] - 0.74).abs()
    height_reward = torch.exp(-height_error / 0.3)
    
    gravity_proj = robot.data.projected_gravity_b[:, :2]
    lean_reward = torch.exp(-2.0 * torch.norm(gravity_proj, dim=1))
    
    # === FOUNDATION TOTAL ===
    foundation_reward = (
        vel_reward * 3.0 +        # Proven velocity tracking
        gait_reward * 2.0 +       # Proven gait patterns
        height_reward * 2.0 +     # Height maintenance
        lean_reward * 1.5 +       # Orientation stability
        0.5                       # Baseline bonus
    )
    
    # === ADD ENVIRONMENTAL IF ANALYSIS SHOWS FEATURES ===
    # Add terrain_bonus, obstacle_bonus, gap_navigation_bonus here if needed
    
    return foundation_reward.clamp(min=0.1, max=10.0)
```

**DANGEROUS PATTERNS TO AVOID:**