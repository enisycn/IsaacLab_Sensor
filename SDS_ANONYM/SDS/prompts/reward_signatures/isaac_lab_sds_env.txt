üåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåü
üåüüåüüåü ENVIRONMENT-AWARE LOCOMOTION REWARD DESIGN GUIDANCE üåüüåüüåü
üåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåü

**CRITICAL PROJECT UNDERSTANDING:**

These guidance explanations are designed for creating **environment-aware locomotion rewards** that enable **intelligent sensor-driven behavior adaptation** for humanoid robots.

**üéØ PRIMARY OBJECTIVE:** 
Create reward functions that make **measurable positive impact when sensors are used** compared to **without sensor usage**.

**üî¨ RESEARCH PURPOSE:**
Enable comparison studies demonstrating:
- **WITHOUT SENSORS:** Basic locomotion behavior 
- **WITH SENSORS:** Adaptive, context-aware behavior that responds to environmental challenges

**üß† DESIGN METHODOLOGY:**
1. **EXTRACT:** Get the pre-analyzed environment data from input (gaps, obstacles, terrain complexity)
2. **THINK:** Determine which sensors are needed and how they should influence behavior
3. **DECIDE:** Implement context-aware behavioral switching (NOT simultaneous conflicting rewards)
4. **IMPACT:** Ensure sensors create observable behavioral differences for scientific comparison

**üìä SUCCESS CRITERIA:**
‚úÖ Robot behaves measurably different with sensors vs. without sensors
‚úÖ Sensor-enabled robot adapts to environmental challenges more effectively
‚úÖ Clear behavioral switching based on environmental context
‚úÖ No conflicting simultaneous behaviors (e.g., walking + jumping simultaneously)

**‚ö†Ô∏è FAILURE INDICATORS:**
‚ùå Robot behaves identically with/without sensors
‚ùå Sensors provide only minor bonuses without changing core behavior
‚ùå Conflicting reward objectives that confuse the policy

---

üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®ÔøΩÔøΩüö®üö®üö®üö®üö®üö®üö®üö®üö®
üö®üö®üö® ISAAC LAB STANDARD: RAW SENSOR ACCESS FOR REWARD FUNCTIONS üö®üö®üö®
üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®

**üö® CRITICAL: NEVER IMPORT ISAAC LAB MDP FUNCTIONS IN CUSTOM REWARDS!**
```python
# ‚ùå THESE WILL CAUSE ImportError:
from __main__ import feet_air_time_positive_biped
from isaaclab.mdp import any_function
import isaaclab_tasks.manager_based.locomotion.velocity.mdp as mdp

# ‚úÖ SAFE IMPORTS ONLY:
# NOTE: torch, quat_apply_inverse, yaw_quat, SceneEntityCfg already imported in rewards.py
```

**‚úÖ IMPLEMENT ALL PATTERNS INLINE - DON'T IMPORT MDP FUNCTIONS**

üéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØ
üéØüéØüéØ SENSOR-DRIVEN BEHAVIORAL ADAPTATION üéØüéØüéØ
üéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØ

**üö® CRITICAL: SENSORS MUST CREATE MEASURABLE BEHAVIORAL DIFFERENCES! üö®**

**PROJECT PURPOSE: Enable with/without sensor comparison studies**

**‚ùå FORBIDDEN: Conflicting simultaneous behaviors:**
```python
# BAD - Robot confused between walking and jumping at same time
walking_air_time = 0.3  # Want short air time for walking
jumping_air_time = 0.8  # Want long air time for jumping
total = walking_reward + jumping_reward  # CONFLICTING OBJECTIVES!
```

**Context-aware behavioral switching:**
```python
# GOOD - Robot adapts behavior based on sensor input
gap_ahead = height_measurements[:, front_indices] > 0.2   # Look ahead for gaps (CORRECTED: positive = gaps)
obstacle_ahead = lidar_distances[:, front_rays] < 2.0     # Look ahead for obstacles

# Behavioral switching based on sensor input
if torch.any(gap_ahead, dim=1).any():
    # DIFFERENT BEHAVIOR: Prepare for gap crossing
    behavior = foundation + gap_preparation_behavior()
elif torch.any(obstacle_ahead, dim=1).any():
    # DIFFERENT BEHAVIOR: Navigate around obstacles  
    behavior = foundation + obstacle_avoidance_behavior()
else:
    # EFFICIENT BEHAVIOR: Normal walking without sensors
    behavior = foundation_locomotion_only()
```

**üéØ SENSOR IMPACT REQUIREMENTS:**

**1. HEIGHT SCANNER BEHAVIORAL IMPACT:**
- **Flat terrain**: Robot walks normally (sensor unused)
- **Small gap terrain (10-20cm)**: Robot modifies gait 1-2 steps BEFORE reaching gaps (predictive stepping adaptation)
- **Large gap terrain (25cm+)**: Robot prepares for jumping 2-3 steps BEFORE reaching gaps (predictive jump preparation)
- **Stair terrain**: Robot adjusts height expectations (terrain following)

**2. LIDAR BEHAVIORAL IMPACT:**
- **Open terrain**: Robot walks efficiently (sensor unused)
- **Obstacle terrain**: Robot maintains safe distances, careful navigation
- **Dense obstacles**: Robot moves conservatively, plans paths

**3. COMPARISON STUDY VALIDATION:**
- **Without sensors**: Generic foundation walking ‚Üí fails on challenging terrain
- **With sensors**: Adaptive behavior ‚Üí succeeds on challenging terrain
- **Measurable improvement**: Success rate, stability, efficiency gains


```

**CHOOSE BASED ON ENVIRONMENT ANALYSIS:**
- Small gaps requiring stepping (15-25cm) ‚Üí Absolute height maintains clearance
- Large gaps requiring jumping (25cm+) ‚Üí Absolute height with increased clearance targets
- Variable surface heights ‚Üí Terrain-relative adapts to surface
- Consistent platform levels ‚Üí Absolute height for smooth transitions
- Climbing/descending behavior needed ‚Üí Terrain-relative for adaptation
- Jump tasks ‚Üí Absolute height with dynamic targets (crouch: 0.64m, jump: 0.94m, normal: 0.74m)

These patterns are battle-tested and known to work for humanoid locomotion. Use them as a foundation:

**üèÜ ISAAC LAB PROVEN REWARD FUNCTIONS (USE THESE FIRST!)**

**These are production-ready Isaac Lab functions that create excellent human-like walking:**

**1. BIPEDAL AIR TIME REWARD (SUPERIOR FOR HUMANOIDS) - SINGLE STANCE IS CRITICAL:**
```python
# üéØ PROVEN: Built-in Isaac Lab function for bipedal gait patterns
# CRITICAL INSIGHT: Natural human walking = alternating SINGLE SUPPORT phases!
contact_sensor = env.scene.sensors["contact_forces"]
foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)

# Access air time and contact time data (VERIFIED WORKING)
air_time = contact_sensor.data.current_air_time[:, foot_ids]
contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
in_contact = contact_time > 0.0

# üöÄ KEY FOR NATURAL WALKING: Single stance reward (one foot at a time)
# This is what makes human walking look natural vs robotic shuffling!
single_stance = torch.sum(in_contact.int(), dim=1) == 1  # ONLY ONE FOOT DOWN!
in_mode_time = torch.where(in_contact, contact_time, air_time)
gait_reward = torch.min(torch.where(single_stance.unsqueeze(-1), in_mode_time, 0.0), dim=1)[0]

# ‚ö° PROPER AIR TIME MANAGEMENT: 
# - Threshold prevents excessive foot lifting for normal walking (unnatural high knees)
# - Rewards natural step timing for smooth walking rhythm
# - EXCEPTION: Jump tasks require extended air time (0.8s+) - do not clamp for jumping behaviors
commands = env.command_manager.get_command("base_velocity")
is_moving = torch.norm(commands[:, :2], dim=1) > 0.1
gait_reward = torch.where(is_moving, torch.clamp(gait_reward, max=0.5), gait_reward)  # Cap only for walking, not jumping

# üéØ COMMAND DEPENDENCY: No reward for zero command (prevents stationary exploitation)
commands = env.command_manager.get_command("base_velocity")
command_magnitude = torch.norm(commands[:, :2], dim=1)
gait_reward *= (command_magnitude > 0.1).float()  # Only reward when actually moving
```

**WHY THIS PATTERN IS SUPERIOR TO GENERIC AIR TIME:**
- **Single stance detection**: Natural walking requires alternating support (not double support shuffling)
- **Contact-aware timing**: Uses actual contact sensor data, not estimated patterns
- **Anti-exploitation**: No reward for standing still or micro-movements
- **Natural rhythm**: 0.5s threshold matches human walking cadence
- **Isaac Lab optimized**: Uses proven sensor patterns that work reliably

**2. YAW-ALIGNED VELOCITY TRACKING (PROVEN SUPERIOR TO BODY FRAME):**

**üö® JUMP TASK REQUIREMENT: Always include forward velocity tracking for jump tasks, regardless of terrain type! Gap jumping requires maintained forward momentum for successful crossing - velocity tracking ensures robot doesn't stop before gaps.**
```python
# üéØ PROVEN: Isaac Lab's yaw-frame tracking - VASTLY superior to basic body frame
# CRITICAL: Decouples velocity control from robot tilt/lean (prevents control coupling)
# NOTE: quat_apply_inverse, yaw_quat already available in rewards.py

robot = env.scene["robot"]
commands = env.command_manager.get_command("base_velocity")

# üöÄ YAW-ALIGNED TRANSFORMATION: Removes pitch/roll interference
# This is why Isaac Lab velocity tracking works so much better!
vel_yaw = quat_apply_inverse(yaw_quat(robot.data.root_quat_w), robot.data.root_lin_vel_w[:, :3])
lin_vel_error = torch.sum(torch.square(commands[:, :2] - vel_yaw[:, :2]), dim=1)
vel_reward = torch.exp(-lin_vel_error / (1.0**2))  # Exponential kernel for smooth decay

# üéØ CRITICAL: No reward for zero commands (anti-exploitation)
vel_reward *= (torch.norm(commands[:, :2], dim=1) > 0.1).float()
```

**WHY YAW-ALIGNED IS SUPERIOR:**
- **Decoupled control**: Velocity tracking unaffected by robot lean/tilt
- **Stable tracking**: Works even when robot pitches forward/backward
- **Natural movement**: Allows body dynamics while maintaining velocity goals
- **Isaac Lab proven**: This is the actual pattern used in successful Isaac Lab locomotion

**3. ANGULAR VELOCITY TRACKING (WORLD FRAME FOR STABILITY):**
```python
# üéØ PROVEN: Isaac Lab's world frame angular tracking (superior to body frame)
commands = env.command_manager.get_command("base_velocity")
robot = env.scene["robot"]

# World frame angular velocity for consistent turning control
ang_vel_error = torch.square(commands[:, 2] - robot.data.root_ang_vel_w[:, 2])
ang_reward = torch.exp(-ang_vel_error / (1.0**2))
```

**4. CONTACT-AWARE FOOT SLIDING PENALTY (INTELLIGENT SLIDING DETECTION):**
```python
# üéØ PROVEN: Isaac Lab's sliding penalty - ONLY when foot is actually in contact
# CRITICAL: This prevents penalizing swing leg motion (which should move freely!)
contact_sensor = env.scene.sensors["contact_forces"]
foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)

# üöÄ SMART CONTACT DETECTION: Use force history for reliable contact state
forces = contact_sensor.data.net_forces_w_history[:, :, foot_ids, :]
contacts = forces.norm(dim=-1).max(dim=1)[0] > 1.0  # Actual contact detection

# Get foot velocities and apply sliding penalty ONLY when in contact
robot = env.scene["robot"]
body_vel = robot.data.body_lin_vel_w[:, foot_ids, :2]
slide_penalty = torch.sum(body_vel.norm(dim=-1) * contacts, dim=1)  # Contact-aware!
```

**WHY CONTACT-AWARE SLIDING IS CRITICAL:**
- **Swing phase freedom**: Doesn't penalize moving feet during swing phase
- **Realistic physics**: Only applies when feet should be stationary (in contact)
- **Force-based detection**: Uses actual physics data, not position estimates
- **Natural walking**: Allows proper foot lifting and placement

**üîë CRITICAL SUCCESS FACTORS FOR BIPEDAL WALKING:**

1. **SINGLE STANCE DOMINANCE**: Most natural walking happens in single support!
2. **YAW-ALIGNED CONTROL**: Decouples velocity from body orientation
3. **CONTACT-AWARE PENALTIES**: Only apply constraints when physically relevant
4. **COMMAND SCALING**: No rewards for micro-movements or standing still
5. **NATURAL TIMING**: Air time thresholds that match human walking rhythm (0.3-0.5s)
6. **FORCE-BASED DETECTION**: Use contact sensors, not position approximations

**üö® COMMON BIPEDAL WALKING FAILURES TO AVOID:**
- **Double support shuffling**: Not rewarding single stance phases
- **Robotic high stepping**: No upper threshold on air time rewards
- **Control coupling**: Using body frame instead of yaw-aligned frame
- **Swing leg penalties**: Penalizing foot motion during swing phase
- **Zero command exploitation**: Rewarding stationary micro-movements

**‚úÖ USE THESE ISAAC LAB PATTERNS AS YOUR FOUNDATION - THEY ARE BATTLE-TESTED!**

**2. PROPER BIPEDAL GAIT PATTERNS (UPDATED WITH ISAAC LAB INSIGHTS):**
```python
# üéØ PROVEN: Rewards single stance phases (proper walking pattern)
# CRITICAL FOR WALKING: Human walking = 85% single support, 15% double support!
foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)

air_time = contact_sensor.data.current_air_time[:, foot_ids]
contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
in_contact = contact_time > 0.0

# üöÄ SINGLE STANCE DETECTION: The secret to natural walking!
single_stance = torch.sum(in_contact.int(), dim=1) == 1  # One foot at a time
in_mode_time = torch.where(in_contact, contact_time, air_time)
gait_reward = torch.min(torch.where(single_stance.unsqueeze(-1), in_mode_time, 0.0), dim=1)[0]

# üéØ NATURAL STEP TIMING: Prevent robotic high-stepping
gait_reward = torch.clamp(gait_reward, max=0.5) * (command_magnitude > 0.1).float()

# üî• ALTERNATIVE PATTERN - Double Support Bonus (for stability when needed):
double_stance = torch.sum(in_contact.int(), dim=1) == 2  # Both feet down
stability_bonus = double_stance.float() * 0.1  # Small bonus for stable phases
```

**ADVANCED BIPEDAL PATTERNS FOR SPECIFIC GAITS:**

**WALKING GAIT (Primary Pattern):**
```python
# Natural walking = alternating single support + brief double support
walking_pattern = single_stance.float() * 0.8 + double_stance.float() * 0.2
```

**RUNNING GAIT (Flight Phase):**
```python
# Running = single support + flight phase (no double support)
flight_phase = torch.sum(in_contact.int(), dim=1) == 0  # Both feet up
running_pattern = single_stance.float() * 0.6 + flight_phase.float() * 0.4
```

**MARCHING GAIT (Controlled Single Support):**
```python
# Marching = extended single support phases for precision
extended_single = single_stance & (in_mode_time.max(dim=1)[0] > 0.3)
marching_pattern = extended_single.float()
```

**üöÄ JUMPING GAIT PATTERNS (For Gap Crossing):**

**JUMP PREPARATION (Crouch Phase):**
```python
# Pre-jump crouch: both feet down, lowered center of mass
prep_height = robot.data.root_pos_w[:, 2] < (baseline_height - 0.1)  # 10cm crouch
jump_prep = double_stance & prep_height
prep_reward = jump_prep.float() * 2.0  # Reward preparation phase
```

**SYNCHRONIZED TAKEOFF (Launch Phase):**
```python
# Jump takeoff: both feet leave ground simultaneously for gaps
both_airborne = torch.sum(in_contact.int(), dim=1) == 0  # Flight phase
sufficient_height = robot.data.root_pos_w[:, 2] > (baseline_height + 0.2)  # 20cm clearance
jump_takeoff = both_airborne & sufficient_height
takeoff_reward = jump_takeoff.float() * 3.0  # High reward for successful takeoff
```

**COORDINATED LANDING (Absorption Phase):**
```python
# Jump landing: controlled impact absorption, both feet contact
simultaneous_landing = double_stance & (air_time.min(dim=1)[0] > 0.6)  # Both feet landed after flight
controlled_impact = torch.norm(robot.data.root_lin_vel_w[:, 2], dim=0) < 2.0  # Controlled vertical velocity
jump_landing = simultaneous_landing & controlled_impact
landing_reward = jump_landing.float() * 2.0  # Reward safe landing
```

**üö® PRIORITY: Use these proven Isaac Lab functions as your foundation, then add task-specific enhancements!**

**üî• ADDITIONAL ISAAC LAB BUILT-IN FUNCTION USAGE:**

When environment analysis shows you need specific locomotion patterns, you can use Isaac Lab's built-in reward functions directly in your reward configuration. However, for SDS custom rewards, you should implement the patterns inline as shown above.

**BUILT-IN FUNCTION REFERENCE (for understanding, not direct use in custom rewards):**
```python
# These are the actual Isaac Lab functions - understand their patterns:
# mdp.feet_air_time_positive_biped(env, command_name="base_velocity", threshold=0.5, sensor_cfg=SceneEntityCfg("contact_forces"))
# mdp.track_lin_vel_xy_yaw_frame_exp(env, std=1.0, command_name="base_velocity")
# mdp.track_ang_vel_z_world_exp(env, command_name="base_velocity", std=1.0)
# mdp.feet_slide(env, sensor_cfg=SceneEntityCfg("contact_forces"))
```

**üéØ CRITICAL ISAAC LAB SUCCESS INSIGHTS:**
- **Command scaling**: NEVER reward when commands are near zero - prevents exploitation
- **Yaw alignment**: Removes pitch/roll interference from velocity tracking - critical for stability
- **Single stance**: Encourages proper alternating foot pattern - key for natural walking
- **Contact awareness**: Only apply penalties when actually relevant (foot in contact) - prevents swing phase penalties
- **Capped rewards**: Air time and other metrics should have reasonable upper bounds - prevents over-optimization
- **Force-based detection**: Use contact sensor forces, not position estimates - more reliable
- **Exponential kernels**: Provide smooth reward gradients for stable learning - better than linear penalties

## ISAAC LAB REWARD COMPUTATION PATTERNS

**Focus: Technical implementation for reward functions, not biomechanical theory**

** ENHANCED ENVIRONMENT REWARD PATTERNS (NEW - FOR ADVANCED LOCOMOTION):**

**4. VELOCITY-OBSTACLE CONFLICT RESOLUTION MATH:**
```python
# Enhanced sensor-based dynamic target modification for reward computation
height_sensor = env.scene.sensors["height_scanner"]
lidar_sensor = env.scene.sensors["lidar"]
contact_sensor = env.scene.sensors["contact_forces"]

# Process sensor data with appropriate sanitization using Isaac Lab standard
height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5
height_measurements = torch.where(torch.isfinite(height_measurements), height_measurements, torch.zeros_like(height_measurements))

# STEP 2: ADAPTIVE CORRELATION ARCHITECTURE
# Create flexible prediction-validation system
environmental_predictions = extract_environmental_context(height_measurements, lidar_sensor)
contact_classification = categorize_contact_events(contact_sensor, environmental_predictions)

# STEP 3: CONTEXT-SENSITIVE SCALING
# Scale rewards based on environmental assessment, not fixed values
environmental_complexity = assess_terrain_complexity(height_measurements)
correlation_confidence = measure_sensor_agreement(height_sensor, lidar_sensor)
reward_scaling = compute_adaptive_scaling(environmental_complexity, correlation_confidence)

# STEP 4: INTELLIGENT CORRELATION IMPLEMENTATION
# Reward/penalize based on prediction-reality correlation
contact_intelligence = correlate_contact_with_predictions(contact_classification, environmental_predictions)
terrain_aware_reward = contact_intelligence * reward_scaling  # Adaptive, not fixed values

# TECHNICAL GUIDELINES FOR FLEXIBLE IMPLEMENTATION:
# - Extract environmental context from available sensors
# - Classify contact events by their environmental appropriateness  
# - Scale correlation strength based on sensor confidence and environmental complexity
# - Adapt reward magnitudes to situational requirements, not fixed penalty/reward values
```

**CRITICAL: TRAINING WILL FAIL WITHOUT THESE FIXES!**

**GUARANTEED TRAINING FAILURE - COMMON BUGS THAT CRASH THE SYSTEM:**

1. **TENSOR CONVERSION BUG (CAUSES TypeError):**
   ```python
   # TRAINING KILLER - NEVER DO THIS:
   indices, _ = robot.find_joints(["joint_name"])
   data = robot.data.joint_pos[:, indices]  # INSTANT CRASH!
   
   # MANDATORY FIX - ALWAYS DO THIS:
   indices, _ = robot.find_joints(["joint_name"])
   indices = torch.tensor(indices, dtype=torch.long, device=env.device)
   data = robot.data.joint_pos[:, indices]  # WORKS!
   ```

2. **NUMERICAL INSTABILITY BUG (CAUSES "std >= 0.0" ERROR):**
   ```python
   # TRAINING KILLER:
   reward = torch.exp(-huge_value)  # Creates NaN/inf!
   
   # MANDATORY FIX:
   reward = torch.exp(-torch.clamp(value, max=10.0))
   reward = torch.where(torch.isfinite(reward), reward, torch.zeros_like(reward))
   ```

**EVERY SINGLE find_joints() CALL MUST BE FOLLOWED BY torch.tensor() CONVERSION!**

**ALL NUMERICAL EXAMPLES, CODE SNIPPETS, AND REWARD PATTERNS IN THIS PROMPT ARE FOR TECHNICAL DEMONSTRATION ONLY.**
**DO NOT COPY EXAMPLES DIRECTLY! UNDERSTAND THE PRINCIPLES AND ADAPT TO YOUR ENVIRONMENT.**

** CRITICAL: NO HELPER FUNCTIONS ALLOWED! **

** ABSOLUTELY FORBIDDEN:**
- `def get_velocity_tracking_error(...)` - NO HELPER FUNCTIONS!
- `def calculate_foot_contacts(...)` - NO HELPER FUNCTIONS!
- `def any_helper_function(...)` - NO HELPER FUNCTIONS!

**REQUIRED PATTERN: ALL LOGIC INLINE**
```python
def sds_custom_reward(env) -> torch.Tensor:
    # NOTE: torch already imported in rewards.py
    # ALL your calculation logic goes here directly - no function calls!
    lin_vel_error = torch.norm(robot.data.root_lin_vel_b[:, :2] - commands[:, :2], dim=1)  #  INLINE
    # NOT: lin_err, ang_err = get_velocity_tracking_error(...)  #  FORBIDDEN
    return reward.clamp(min=0.0, max=10.0)
```

**CRITICAL: ONLY GENERATE REWARD FUNCTIONS - NO OBSERVATION CONFIGURATIONS!**

**NEVER GENERATE THESE:**
- `def get_height_scan(env):` or any observation functions
- `lambda env: env.scene.sensors["height_scanner"].data.ray_hits_w[...]` 
- `ObsTerm(func=lambda env: ...)` observation configurations
- Environment sensor configurations
- Any code outside the reward function

**ONLY GENERATE THIS:**
- `def sds_custom_reward(env) -> torch.Tensor:` function only
- Access existing sensors WITHIN the reward function

**The environment already provides all sensor data - just use it in your reward!**

**CRITICAL: PPO CRASHES WITH "std >= 0.0" ERROR WITHOUT THESE!**

**GUARANTEED PPO FAILURE PATTERNS:**

```python
# DEADLY: Environmental sensor data contains NaN/Inf values
height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5  # Can contain NaN!
reward += torch.mean(height_measurements)  # NaN propagates, crashes PPO

# DEADLY: Unbounded reward values crash PPO standard deviation  
reward = some_large_calculation  # Can be >100, causes std <= 0 error

# DEADLY: Division by zero in environmental calculations
reward = 1.0 / distance_to_obstacle  # Zero distance = Inf reward = PPO crash
```

**PPO-SAFE ENVIRONMENTAL PATTERNS (WHEN USING ENVIRONMENTAL SENSORS):**

```python
# CRITICAL: ALWAYS sanitize sensor data FIRST - sensors can return NaN/Inf when rays miss!
height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5
height_measurements = torch.where(torch.isfinite(height_measurements), height_measurements, torch.zeros_like(height_measurements))

lidar_distances = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)
lidar_distances = torch.where(torch.isfinite(lidar_distances), lidar_distances, torch.ones_like(lidar_distances) * 5.0)

# DEADLY BUG PREVENTION: Explosive exponentials crash PPO!
terrain_variance = torch.var(height_measurements, dim=1)
terrain_variance = torch.clamp(terrain_variance, max=0.01)  # CRITICAL: Prevent explosion
safe_terrain_reward = torch.exp(-terrain_variance * 10.0)  # NOT 100.0! Causes NaN!

# DIVISION SAFETY: Prevent near-zero denominators
min_distance = torch.min(lidar_distances, dim=1)[0]
min_distance = torch.clamp(min_distance, min=0.05, max=10.0)  # Prevent division by ~0
safety_reward = (min_distance - 0.2) / 0.3  # Now safe from division issues

# FINAL SAFETY NET: Last chance to prevent PPO crash
total_reward = foundation_reward + environmental_reward
total_reward = torch.where(torch.isfinite(total_reward), total_reward, torch.ones_like(total_reward) * 0.5)
return torch.clamp(total_reward, min=0.1, max=8.0)
```

**üö® VERIFIED PPO CRASH PATTERNS - NEVER DO THESE:**

```python
# ‚ùå GUARANTEED CRASH: Unprotected sensor data
height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5
reward += torch.mean(height_measurements)  # NaN propagates ‚Üí PPO crash

# ‚ùå GUARANTEED CRASH: Explosive exponentials  
terrain_var = torch.var(height_measurements, dim=1)
reward = torch.exp(-terrain_var * 100.0)  # Creates -‚àû ‚Üí exp(-‚àû) = 0, but if terrain_var is negative...

# ‚ùå GUARANTEED CRASH: Division by near-zero
min_dist = torch.min(lidar_distances, dim=1)[0]  # Can be 0.001
reward = 1.0 / min_dist  # = 1000 ‚Üí explodes gradient ‚Üí PPO crash

# ‚ùå GUARANTEED CRASH: No final bounds checking
return reward  # If reward contains NaN/Inf ‚Üí immediate PPO crash
```

Isaac Lab Reward Function Format:

**CRITICAL: JOINT INDEXING REQUIREMENT**

**COMMON BUG - WILL CAUSE TRAINING FAILURE:**
```python
# WRONG - robot.find_joints() returns LISTS, not tensors!
joint_indices, _ = robot.find_joints(["joint_name"])
joint_data = robot.data.joint_pos[:, joint_indices]  # TypeError!
```

**CORRECT PATTERN - ALWAYS CONVERT TO TENSOR:**
```python
# RIGHT - Convert list to tensor for proper indexing
joint_indices, _ = robot.find_joints(["joint_name"])
joint_indices = torch.tensor(joint_indices, dtype=torch.long, device=env.device)
joint_data = robot.data.joint_pos[:, joint_indices]  # Works!
```

**MANDATORY: Every time you use robot.find_joints(), immediately convert the result to a tensor!**

Isaac Lab SDS Environment - G1 Humanoid Locomotion

## Robot Configuration (VERIFIED & UPDATED FOR FULL BODY CONTROL)
- **Robot**: Unitree G1 EDU U4 Humanoid (37 DOF total)
- **Action Space**: 23 DOF controlled for complete humanoid locomotion (all joints except hand fingers)
- **Height**: Relative to terrain baseline using height sensor (adaptive)
- **Mass**: ~35kg humanoid

## Action Configuration (FULL BODY HUMANOID CONTROL)
**Controlled Joints (23 DOF for complete humanoid locomotion):**
- Legs: 12 DOF (6 per leg: hip_yaw, hip_roll, hip_pitch, knee, ankle_pitch, ankle_roll)

- Torso: 1 DOF (torso_joint)

**Fixed Joints (24 DOF):**
- Hand Fingers: 14 DOF maintain default poses (zero, one, two, three, four, five, six_joint per hand)
- Arms: 10 DOF maintain default poses

## Contact Detection (VERIFIED WORKING)
**Foot Bodies**: `left_ankle_roll_link`, `right_ankle_roll_link`
**Detection Pattern**: `contact_sensor.find_bodies(".*_ankle_roll_link")`
**Contact Threshold**: 50.0N (corrected for humanoid mass)

## Key Functions Examples for Reward Generation. Don't Copy Paste Directly!
```python
# Foot contact detection (VERIFIED WORKING)
contact_forces = env.scene.sensors["contact_forces"].data.net_forces_w
foot_ids, _ = env.scene.sensors["contact_forces"].find_bodies(".*_ankle_roll_link")
foot_forces = contact_forces[:, foot_ids, :]
foot_contacts = (foot_forces.norm(dim=-1) > 50.0).float()

# Velocity tracking
robot = env.scene["robot"]
commands = env.command_manager.get_command("base_velocity")
vel_error = robot.data.root_lin_vel_b[:, :2] - commands[:, :2]
ang_error = robot.data.root_ang_vel_b[:, 2] - commands[:, 2]

# Height maintenance  
height = robot.data.root_pos_w[:, 2]
height_error = (height - (terrain_baseline + 0.74)).abs()  # Relative to terrain

# Bipedal gait patterns
left_contact = foot_contacts[:, 0]
right_contact = foot_contacts[:, 1] 
single_support = ((left_contact > 0.5) & (right_contact < 0.5)) | ((left_contact < 0.5) & (right_contact > 0.5))
double_support = (left_contact > 0.5) & (right_contact > 0.5)
```

## Successful Training Metrics
- **Episode Rewards**: 0.01-0.02 range (working)
- **Action Scale**: 1.0 (allows proper joint movement)
- **Training Progress**: Mean rewards ~0.2-0.23, episode length 28-32 steps

# TECHNICAL REFERENCE - ISAAC LAB API DOCUMENTATION

## Robot Data Access Patterns
    
    # ROBOT DATA FIRST APPROACH:
    # Available robot data:
    # robot.data.root_pos_w[:, 2] - height (z-coordinate, nominal 0.74m for G1)
    # robot.data.root_lin_vel_b[:, 0] - forward velocity (x-axis in body frame)
    # robot.data.root_lin_vel_w - linear velocity in world frame [num_envs, 3]
    # robot.data.root_ang_vel_b - angular velocity in body frame [num_envs, 3]
    # robot.data.root_ang_vel_w - angular velocity in world frame [num_envs, 3]
    # robot.data.root_quat_w - orientation quaternion [w,x,y,z]
    # robot.data.joint_pos - joint positions [num_envs, 37] for G1 EDU U4 with dexterous hands (VERIFIED)
    # robot.data.joint_vel - joint velocities [num_envs, 37] (VERIFIED)
    # CONTROLLED JOINTS (23 DOF): Use robot.find_joints() to get indices for legs + arms + torso (all except hand fingers)
    # HAND FINGER JOINTS (14 DOF): Excluded from control but can be accessed if needed
    # robot.data.root_pos_w[:, 2] - robot height [num_envs] - should be around 0.74m for G1 in Isaac Lab (VERIFIED)
    # robot.data.root_lin_vel_b - linear velocity in body frame [num_envs, 3]
    # robot.data.root_quat_w - quaternion orientation [num_envs, 4]
    # commands[:, :3] - [forward_vel, lateral_vel, yaw_rate] commands
    
    # Available command data:
    # commands[:, 0] - desired forward velocity (vx)
    # commands[:, 1] - desired lateral velocity (vy) 
    # commands[:, 2] - desired angular velocity (omega_z)
    # ADAPTIVE ROBOT CONFIGURATION:
    # DO NOT hardcode joint counts or specific robot parameters
    # Use dynamic robot configuration detection:
    num_joints = robot.data.joint_pos.shape[1]
    robot_height_baseline = robot.data.root_pos_w[:, 2].mean()  # Adaptive baseline
    
    # OPTIONAL IMU SENSOR (use as fallback only):
    # An IMU sensor "imu" can be spawned via ImuCfg if needed:
    # ImuCfg:
    #   prim_path: "/World/envs/env_.*/Robot/torso_link"
    #   update_period: 0.02  # 50 Hz
    #   gravity_bias: [0.0, 0.0, 9.81]
    # It exposes in env.scene.sensors["imu"].data:
    # - pos_w: FloatTensor [num_envs, 3] - World position
    # - quat_w: FloatTensor [num_envs, 4] - World orientation (w,x,y,z)
    # - lin_vel_b: FloatTensor [num_envs, 3] - Body-frame linear velocity
    # - ang_vel_b: FloatTensor [num_envs, 3] - Body-frame angular velocity
    # - lin_acc_b: FloatTensor [num_envs, 3] - Body-frame linear acceleration 
    # - ang_acc_b: FloatTensor [num_envs, 3] - Body-frame angular acceleration
    
    # Initialize reward (4-space indent)
    reward = torch.zeros(env.num_envs, dtype=torch.float32, device=env.device)
    
    # IMPORTANT: For contact analysis, use this inline approach:
    # Get foot contact forces for G1 humanoid
    contact_forces = contact_sensor.data.net_forces_w  # [num_envs, num_bodies, 3]
    foot_ids, foot_names = contact_sensor.find_bodies(".*_ankle_roll_link")
    foot_forces = contact_forces[:, foot_ids, :]  # [num_envs, 2, 3] - 2 feet for humanoid
    force_magnitudes = foot_forces.norm(dim=-1)  # [num_envs, 2]
    
    # Contact detection - analyze video to determine appropriate threshold
    # G1 humanoid requires higher thresholds: gentle gaits (20-50N), dynamic gaits (50-100N)
    # Evidence: G1 standing forces ~150-250N, much higher than quadrupeds
    contact_threshold = 50.0  # Default for G1 humanoid - adjust based on observed contact forces in video
    foot_contacts = (force_magnitudes > contact_threshold).float()  # Convert to float for partial credit
    
    # Note: Design contact rewards based on the observed gait pattern in the video
    # G1 humanoid bipedal locomotion: left_ankle_roll_link, right_ankle_roll_link
    
    
    # HUMANOID-SPECIFIC CONSIDERATIONS:
    # - Bipedal stability is critical: balance and contact pattern rewards
    # - Height maintenance: G1 initial height is 0.74m in Isaac Lab (VERIFIED)
    # - Upper body stability: maintain upright torso
    # - Gait patterns: Walk (alternating with double support), Jump (synchronized takeoff/landing), March (controlled single support), Sprint (extended flight), Pace (lateral movement)
    # - Isaac Lab G1 joint structure: 37 DOF total (23 controlled + 14 fixed)
    # - CONTROLLED joint naming: left/right_hip_[yaw/roll/pitch]_joint, left/right_knee_joint, left/right_ankle_[pitch/roll]_joint, torso_joint
    # - FIXED joint naming: left/right_[zero/one/two/three/four/five/six]_joint (hand fingers only)
    
    # HUMAN-LIKE LOCOMOTION DESIGN PRINCIPLES:
    # 1. Dynamic Balance: Humanoids require continuous balance management (not static stability)
    #    Consider torso orientation, roll/pitch control, and center of mass dynamics
    # 2. Movement Efficiency: Natural locomotion minimizes energy expenditure
    #    Consider smooth joint motion, appropriate muscle activation patterns
    # 3. Directional Preference: Forward movement often has higher priority than lateral/backward
    #    Consider command-dependent weighting based on intended movement direction
    # 4. Temporal Coordination: Natural gaits involve timing and rhythm
    #    Consider phase relationships between limbs, contact duration, and step timing
    # 5. Upper Body Integration: Torso contributes to locomotion stability
    #    Consider how upper body motion supports or disrupts locomotion goals
    # 6. Adaptive Contact Patterns: Different gaits require different contact strategies
    #    Analyze video to determine appropriate contact timing and patterns for the demonstrated behavior
    
    # Contact sensor access pattern for Isaac Lab
    foot_ids, foot_names = contact_sensor.find_bodies(".*_ankle_roll_link")  # G1 uses ankle_roll_link for contact
    contact_forces = contact_sensor.data.net_forces_w[:, foot_ids, :]  # [num_envs, 2, 3] for G1 bipedal
    contact_magnitudes = torch.norm(contact_forces, dim=-1)  # [num_envs, 2]
    foot_contacts = contact_magnitudes > contact_threshold  # Binary contact detection
    
    # HUMANOID GAIT PATTERNS (not quadruped!)
    # G1 is BIPEDAL - only 2 feet: left_foot (index 0), right_foot (index 1)
    left_contact = foot_contacts[:, 0]   # Left foot contact
    right_contact = foot_contacts[:, 1]  # Right foot contact
    
    # Bipedal locomotion phases (CORRECTED - not quadruped patterns)
    double_support = left_contact & right_contact        # Both feet down (Walk/Jump)
    single_support_left = left_contact & ~right_contact  # Only left foot down (Walk/March/Sprint)
    single_support_right = ~left_contact & right_contact # Only right foot down (Walk/March/Sprint)
    flight_phase = ~left_contact & ~right_contact        # Both feet up (Jump/Sprint)
    
    # Key differences from quadruped robots:
    # - G1 height: Relative to terrain baseline (0.74m offset) - CRITICAL for adaptive height rewards
    # - Only 2 contact points (not 4)
    # - Bipedal gait patterns (alternating support, not complex quadruped gaits)
    # - Higher contact forces due to full body weight on fewer feet
    # - Dynamic balance required (not static stability like quadrupeds)
    # - Dexterous hands: 37 total DOF with advanced manipulation capabilities


**FORMATTING REQUIREMENTS:**
- Use EXACTLY 4 spaces for each indentation level
- NEVER use 8 spaces, tabs, or inconsistent spacing
- Always add safety checks before any division operations
- Specify dtype=torch.float32 for all tensor creations

# Height Sensor Guide for Isaac Lab RL Rewards

> **‚ö†Ô∏è IMPORTANT**: These are technical explanations. for reward generation you shoul come up correct reward terms suitable for analyzed environment.

## üìê **Isaac Lab Formula**
```python
# Official height scan observation
height_reading = sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - offset
# Default offset = 0.5m (NOT 0.05!)
```

## üéØ **Core Rules**
- **OBSTACLES** = Lower readings (< baseline - threshold) = Terrain HIGHER than expected
- **GAPS** = Higher readings (> baseline + threshold) = Terrain LOWER than expected  
- **BASELINE** = Dynamic terrain calculation (0.209m fallback) = torch.median(valid_heights)
- **THRESHOLDS** = ¬±0.07m (7cm) for balanced detection

## ‚ö° **Correct Implementation**
```python
def terrain_classification(height_readings, baseline=0.209):
    obstacle_threshold = 0.07  # 7cm
    gap_threshold = 0.07       # 7cm
    
    obstacles = height_readings < (baseline - obstacle_threshold)    # < 0.139m
    gaps = height_readings > (baseline + gap_threshold)              # > 0.279m  
    normal = ~obstacles & ~gaps                                      # 0.139-0.279m
    extreme_gaps = height_readings == float('inf')                   # No terrain detected
    
    return obstacles, gaps, normal, extreme_gaps
```

## üìä **Optimized Thresholds**
- **Standard**: 0.07m (7cm) - balanced for most robots
- **Sensitive**: 0.05m (5cm) - careful navigation
- **Relaxed**: 0.10m (10cm) - rough terrain
- **Range**: 0.05-0.15m acceptable, 0.07m optimal

## üî¨ **Height Scanner Specifications**

### **G1 Robot Configuration (Enhanced)**
```python
# Enhanced sensor configuration from flat_with_box_env_cfg.py
height_scanner = RayCasterCfg(
    prim_path="/World/envs/env_0/Robot/torso_link",
    offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 0.6)),  # 60cm above torso
    attach_yaw_only=True,  # Only yaw rotation (not pitch/roll)
    pattern_cfg=patterns.GridPatternCfg(
        resolution=0.075,  # 7.5cm spacing between rays
        size=[2.0, 1.5],   # 2m forward √ó 1.5m lateral coverage
    ),
    max_distance=3.0,      # 3m maximum ray distance
    update_period=0.02,    # 50Hz update rate
    mesh_prim_paths=["/World/ground"],
)
```

### **Ray Pattern Analysis**
```python
# Enhanced configuration calculations:
forward_coverage = 2.0m  # Total forward scan distance
lateral_coverage = 1.5m  # Total lateral scan distance  
ray_resolution = 0.075m  # 7.5cm between rays
rays_forward = int(2.0 / 0.075) + 1 = 27 rays  # Forward direction
rays_lateral = int(1.5 / 0.075) + 1 = 21 rays  # Lateral direction
total_rays = 27 √ó 21 = 567 rays  # Total ray count

# Standard configuration calculations:
standard_forward = 1.6m
standard_lateral = 1.0m
standard_resolution = 0.1m
standard_rays_forward = int(1.6 / 0.1) + 1 = 17 rays
standard_rays_lateral = int(1.0 / 0.1) + 1 = 11 rays
standard_total = 17 √ó 11 = 187 rays
```

## üéØ **Relative Height Tracking Rewards**

### **1. ONLY Relative Terrain Navigation**
```python
def relative_terrain_reward(env, sensor_cfg=SceneEntityCfg("height_scanner")):
    """Pure relative height tracking - NO absolute positioning."""
    
    sensor = env.scene.sensors[sensor_cfg.name]
    
    # Isaac Lab formula (RELATIVE measurements only)
    height_readings = sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - 0.5
    baseline = 0.209  # G1 robot baseline
    
    # Classification using ONLY sensor readings
    obstacles = height_readings < (baseline - 0.07)
    gaps = height_readings > (baseline + 0.07)
    normal_terrain = ~obstacles & ~gaps & (height_readings != float('inf'))
    
    # Count features
    total_rays = height_readings.shape[-1]
    obstacle_count = obstacles.sum(dim=-1)
    gap_count = gaps.sum(dim=-1)
    normal_count = normal_terrain.sum(dim=-1)
    
    # Reward ONLY based on terrain sensing (not absolute height)
    terrain_safety = (normal_count / total_rays) * 0.5
    obstacle_penalty = -(obstacle_count / total_rays) * 2.0
    gap_penalty = -(gap_count / total_rays) * 1.5
    
    return terrain_safety + obstacle_penalty + gap_penalty
```

### **2. Look-Ahead Terrain Preview**
```python
def lookahead_terrain_reward(env, sensor_cfg=SceneEntityCfg("height_scanner")):
    """Forward-looking terrain analysis for proactive navigation."""
    
    sensor = env.scene.sensors[sensor_cfg.name]
    height_readings = sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - 0.5
    baseline = 0.209
    
    # Reshape to grid pattern for spatial analysis
    # For 567 rays (27√ó21 grid): rays_forward=27, rays_lateral=21
    rays_forward, rays_lateral = 27, 21
    height_grid = height_readings.view(-1, rays_forward, rays_lateral)
    
    # Split into zones: near (0-0.7m), mid (0.7-1.3m), far (1.3-2.0m)
    near_zone = height_grid[:, :9, :]    # First 9 rays = 0-0.675m
    mid_zone = height_grid[:, 9:18, :]   # Next 9 rays = 0.675-1.35m  
    far_zone = height_grid[:, 18:, :]    # Last 9 rays = 1.35-2.0m
    
    # Analyze each zone for upcoming terrain
    def analyze_zone(zone_data, zone_weight):
        obstacles = (zone_data < (baseline - 0.07)).float().mean(dim=(-1, -2))
        gaps = (zone_data > (baseline + 0.07)).float().mean(dim=(-1, -2))
        return -(obstacles * 2.0 + gaps * 1.5) * zone_weight
    
    # Weight zones: near=highest, far=planning
    near_reward = analyze_zone(near_zone, 1.0)    # Immediate danger
    mid_reward = analyze_zone(mid_zone, 0.5)      # Tactical planning
    far_reward = analyze_zone(far_zone, 0.2)     # Strategic planning
    
    return near_reward + mid_reward + far_reward
```

### **3. Adaptive Baseline Calculation**
```python
def adaptive_baseline_terrain_reward(env, sensor_cfg=SceneEntityCfg("height_scanner")):
    """Dynamic baseline adaptation for varying terrain conditions."""
    
    sensor = env.scene.sensors[sensor_cfg.name]
    height_readings = sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - 0.5
    
    # Filter out infinite readings for baseline calculation
    finite_mask = height_readings != float('inf')
    finite_readings = height_readings[finite_mask]
    
    if finite_readings.numel() > 0:
        # Dynamic baseline: use median of current readings
        # More robust than mean for terrain with obstacles/gaps
        dynamic_baseline = torch.median(finite_readings.view(-1, -1), dim=-1)[0]
        
        # Adaptive thresholds based on terrain variation
        terrain_std = torch.std(finite_readings.view(-1, -1), dim=-1)
        adaptive_threshold = torch.clamp(terrain_std * 2.0, 0.05, 0.15)  # 2œÉ rule
        
        # Classification using adaptive parameters
        obstacles = height_readings < (dynamic_baseline.unsqueeze(-1) - adaptive_threshold.unsqueeze(-1))
        gaps = height_readings > (dynamic_baseline.unsqueeze(-1) + adaptive_threshold.unsqueeze(-1))
        
        # Reward based on terrain complexity
        total_rays = height_readings.shape[-1]
        obstacle_ratio = obstacles.sum(dim=-1).float() / total_rays
        gap_ratio = gaps.sum(dim=-1).float() / total_rays
        
        # Penalty scales with terrain difficulty
        complexity_factor = torch.clamp(terrain_std, 0.5, 2.0)
        obstacle_penalty = -obstacle_ratio * 2.0 * complexity_factor
        gap_penalty = -gap_ratio * 1.5 * complexity_factor
        
        return obstacle_penalty + gap_penalty
    else:
        # Fallback for all-infinite readings
        return torch.zeros(env.num_envs, device=env.device)
```

## üéØ **Complete Reward Example**
```python
def comprehensive_terrain_reward(env, sensor_cfg=SceneEntityCfg("height_scanner")):
    sensor = env.scene.sensors[sensor_cfg.name]
    robot = env.scene["robot"]
    
    # Isaac Lab formula
    height_readings = sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - 0.5
    baseline = 0.209  # G1 robot baseline
    
    # Classification
    obstacles = height_readings < (baseline - 0.07)
    gaps = height_readings > (baseline + 0.07)
    normal_terrain = ~obstacles & ~gaps & (height_readings != float('inf'))
    infinite_gaps = height_readings == float('inf')
    
    # Count features
    total_rays = height_readings.shape[-1]
    obstacle_count = obstacles.sum(dim=-1)
    gap_count = gaps.sum(dim=-1)
    normal_count = normal_terrain.sum(dim=-1)
    infinite_count = infinite_gaps.sum(dim=-1)
    
    # Percentage-based rewards
    obstacle_penalty = -(obstacle_count / total_rays) * 2.0
    gap_penalty = -(gap_count / total_rays) * 1.5
    stability_reward = (normal_count / total_rays) * 0.5
    cliff_penalty = -(infinite_count / total_rays) * 10.0
    
    return obstacle_penalty + gap_penalty + stability_reward + cliff_penalty
```

### 3. Dynamic Gap Crossing Reward
```python
def dynamic_gap_crossing(env, sensor_cfg=SceneEntityCfg("height_scanner")):
    """Reward function that encourages crossing appropriate gaps."""
    
    sensor = env.scene.sensors[sensor_cfg.name]
    robot = env.scene["robot"]
    
    height_readings = sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - 0.5
    baseline = 0.209
    
    # Gap classification
    small_gaps = (height_readings > (baseline + 0.05)) & (height_readings < (baseline + 0.15))
    medium_gaps = (height_readings > (baseline + 0.15)) & (height_readings < (baseline + 0.25))
    large_gaps = height_readings > (baseline + 0.25)
    extreme_gaps = height_readings == float('inf')
    
    # Robot capabilities (based on leg length and body size)
    robot_velocity = torch.norm(robot.data.root_lin_vel_w[:, :2], dim=-1)
    can_jump = robot_velocity > 0.3  # Moving fast enough to jump
    
    # Dynamic gap rewards based on robot state
    small_gap_reward = torch.where(
        can_jump & torch.any(small_gaps, dim=-1),
        0.2,  # Small reward for crossing small gaps when able
        torch.where(torch.any(small_gaps, dim=-1), -0.5, 0.0)  # Penalty if not able
    )
    
    medium_gap_penalty = torch.sum(medium_gaps, dim=-1) * -1.0
    large_gap_penalty = torch.sum(large_gaps, dim=-1) * -3.0
    extreme_gap_penalty = torch.sum(extreme_gaps, dim=-1) * -10.0
    
    return small_gap_reward + medium_gap_penalty + large_gap_penalty + extreme_gap_penalty
```

---

## ‚úÖ **Validation Checklist**

### 1. **FORMULA COMPLIANCE**
‚úÖ **CORRECT**: `sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - 0.5`
‚ùå **REJECT**: Direct use of `ray_hits_w[..., 2]` without sensor position/offset

### 2. **BASELINE UNDERSTANDING** 
‚úÖ **FLAT TERRAIN BASELINE:** ~0.209m
- Calculation: sensor_height(0.709) - terrain_z(0.000) - offset(0.5) = 0.209m
- All thresholds should be RELATIVE to this baseline, not absolute

‚ùå **REJECT these approaches:**
- Using 0.209m as absolute threshold
- Hardcoded terrain Z values
- Ignoring sensor mounting height variations

### 3. **OBSTACLE vs GAP INTERPRETATION**
‚úÖ **CORRECT INTERPRETATION:**
```python
# OBSTACLES: Negative height readings (terrain higher than expected)
obstacles = height_readings < (baseline - 0.07)  # < 0.139m for 0.209 baseline
obstacle_penalty = torch.where(obstacles, -penalty_value, 0.0)

# GAPS: Positive height readings (terrain lower than expected)  
gaps = height_readings > (baseline + 0.07)  # > 0.279m for 0.209 baseline
gap_penalty = torch.where(gaps, -penalty_value, 0.0)
```

‚ùå **REJECT these patterns:**
- Treating positive values as obstacles
- Using absolute thresholds without baseline consideration
- Confusing height readings with terrain coordinates

### 4. **THRESHOLD VALIDATION**
‚úÖ **OPTIMIZED THRESHOLDS:**
- **Standard obstacles:** 0.07m above baseline (baseline - 0.07)
- **Standard gaps:** 0.07m below baseline (baseline + 0.07)
- **Acceptable range:** 0.05-0.15m for balanced sensitivity
- **Research validation:** 5-25cm proven successful in academic studies

‚ùå **REJECT these ranges:**
- Thresholds > 0.30m (too large for most robots)
- Thresholds < 0.03m (too sensitive to noise)
- Same threshold for obstacles and gaps (should be different)

### 5. **INFINITE READING HANDLING**
‚úÖ **PROPER INFINITE HANDLING:**
```python
# Handle max range exceeded
valid_readings = height_readings[height_readings != float('inf')]
infinite_penalty = torch.sum(height_readings == float('inf')) * extreme_gap_penalty
```

‚ùå **REJECT these approaches:**
- Ignoring infinite readings completely
- Treating infinite as zero
- Not penalizing extreme gaps (cliffs)

### 6. **SENSOR CONFIGURATION VALIDATION**
‚úÖ **CORRECT SENSOR ACCESS:**
```python
sensor: RayCaster = env.scene.sensors["height_scanner"]
sensor_cfg = SceneEntityCfg("height_scanner")
```

‚ùå **REJECT these patterns:**
- Hardcoded sensor names not matching environment
- Missing sensor existence checks
- Wrong sensor type assumptions

### 7. **CLIPPING & NORMALIZATION VALIDATION**

#### **Observation Clipping**
‚úÖ **CHECK CLIPPING COMPATIBILITY:**
```python

clip=(-0.5, 3.0)      # Custom extended range

### 4. **THRESHOLD VALUES**
‚úÖ **CORRECT**: 0.05-0.15m range, 0.07m optimal
‚ùå **REJECT**: >0.30m (too large), <0.03m (too sensitive)

### 5. **INFINITE HANDLING**
‚úÖ **CORRECT**: `cliff_penalty = torch.sum(height_readings == float('inf')) * penalty`
‚ùå **REJECT**: Ignoring infinite readings

### 6. **CLIPPING COMPATIBILITY**
‚úÖ **CHECK**: Thresholds work with clip ranges:
- `clip=(-1.0, 1.0)` or `clip=(-0.5, 3.0)`
- 0.07m thresholds ‚Üí 0.139m, 0.279m ‚úÖ Within range

### 7. **RELATIVE TRACKING VALIDATION**
‚úÖ **CORRECT RELATIVE TRACKING:**
- Use ONLY height sensor readings for terrain navigation
- NO absolute robot height tracking in rewards
- Dynamic baseline adaptation for varying terrain
- Look-ahead zones for proactive navigation

‚ùå **REJECT ABSOLUTE TRACKING:**
- Direct use of `robot.data.root_pos_w[:, 2]` in terrain rewards
- Fixed absolute height targets
- Mixing absolute positioning with relative terrain sensing

## ‚ùå **Common Mistakes**
```python
# ‚ùå WRONG - Missing sensor position/offset
height = sensor.data.ray_hits_w[..., 2] - 0.5

# ‚ùå WRONG - Backwards logic
obstacles = height_readings > 0.2  # Positive is gaps!

# ‚ùå WRONG - Absolute thresholds
obstacles = height_readings < 0.1  # Ignores baseline

# ‚ùå WRONG - Absolute height tracking in terrain rewards
target_height = 0.7  # Fixed absolute height
height_reward = -torch.square(robot.data.root_pos_w[:, 2] - target_height)

# ‚úÖ CORRECT - Isaac Lab formula + relative thresholds
height = sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - 0.5
obstacles = height_readings < (baseline - 0.07)
gaps = height_readings > (baseline + 0.07)
# ‚úÖ CORRECT - Only relative terrain navigation
terrain_reward = analyze_terrain_features(height_readings, baseline)
```

## üîß **Quick Troubleshooting**
- **All negative readings**: Check sensor height, adjust baseline
- **All classified as obstacles**: Baseline too high, use dynamic calculation
- **Robot avoiding terrain**: Thresholds too sensitive, use 0.07m
- **Infinite crashes**: Always filter with `height_readings != float('inf')`
- **Poor look-ahead**: Check ray pattern grid dimensions (27√ó21 for enhanced config)
- **Baseline drift**: Use median instead of mean for adaptive baseline

## üìä **Visual Scale**
```
0.139m ‚Üê OBSTACLE THRESHOLD (baseline - 0.07)
0.209m ‚Üê BASELINE (G1 flat terrain) 
0.279m ‚Üê GAP THRESHOLD (baseline + 0.07)

Examples:
0.120m = 7cm obstacle (terrain higher)
0.209m = flat terrain (normal)
0.300m = 9cm gap (terrain lower)
inf    = extreme gap (cliff/void)
```

## üéØ **Key Points**
- **Default offset = 0.5m** (half meter, NOT 5cm!)
- **Lower readings = obstacles** (terrain closer to sensor)
- **Higher readings = gaps** (terrain farther from sensor)
- **0.209m baseline** for G1 robot specifically
- **¬±0.07m thresholds** for balanced detection
- **Always handle infinite** readings as dangerous cliffs
- **Use relative thresholds**, never absolute values
- **567 rays total** (27√ó21 grid) for enhanced configuration
- **3m sensor range** with 7.5cm resolution for detailed scanning
- **Look-ahead zones** enable proactive navigation planning
- **NO absolute height** tracking in terrain-based rewards

**The key insight: smaller height readings mean obstacles, larger height readings mean gaps!** üéØ 

**üö® GAP NAVIGATION DEBUGGING GUIDE**

**TURNING IN PLACE ISSUE:**
- **Cause:** Environmental penalties overwhelm velocity tracking
- **Fix:** Foundation-dominant weight structure (60/40 split)
- **Pattern:** `total = foundation * 0.6 + environmental * 0.4`

**GAP AVOIDANCE ISSUE:**
- **Cause:** Rewarding `(1.0 - gap_ratio)` encourages gap-free areas
- **Fix:** Gap traversal bonuses for forward movement through gaps
- **Pattern:** `forward_vel * gap_ratio * weight`

**BASELINE MISCLASSIFICATION:**
- **Cause:** Fixed 0.209 baseline on varying terrain
- **Fix:** Dynamic baseline per environment
- **Pattern:** `baseline[i] = torch.median(valid_heights[i])`

**PROVEN SUCCESSFUL WEIGHT STRUCTURE:**
```python
# Foundation components (must dominate)
vel_reward * 3.0        # Strongest component
gait_reward * 2.0       # Secondary
height_reward * 2.0     # Secondary  
lean_reward * 1.5       # Supporting
baseline + 0.3          # Stability

# Environmental components (supporting role)
gap_traversal_bonus     # Positive reinforcement
safety_penalty         # Minimal progressive
terrain_bonus          # Small positive

# Final composition (critical for stability)
total = foundation * 0.6 + environmental * 0.4
``` 