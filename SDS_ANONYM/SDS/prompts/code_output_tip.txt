**CRITICAL DISCLAIMER: EXAMPLES ARE TECHNICAL REFERENCE ONLY**

**ALL NUMERICAL EXAMPLES, CODE SNIPPETS, AND REWARD PATTERNS IN THIS PROMPT ARE FOR TECHNICAL DEMONSTRATION ONLY.**
**DO NOT COPY EXAMPLES DIRECTLY! UNDERSTAND THE PRINCIPLES AND ADAPT TO YOUR ENVIRONMENT.**

**PROVEN ISAAC LAB LOCOMOTION PATTERNS**

These patterns are proven effective for humanoid locomotion. Use them as technical foundation:

**SUPERIOR VELOCITY TRACKING (Yaw-Aligned Frame):**
```python
# PROVEN: Isaac Lab approach - yaw-aligned frame tracking (much better than body frame)
from isaaclab.utils.math import quat_apply_inverse, yaw_quat

commands = env.command_manager.get_command("base_velocity")
# Transform velocity to yaw-aligned frame (removes pitch/roll effects)
vel_yaw = quat_apply_inverse(yaw_quat(robot.data.root_quat_w), robot.data.root_lin_vel_w[:, :3])
lin_vel_error = torch.sum(torch.square(commands[:, :2] - vel_yaw[:, :2]), dim=1)
vel_reward = torch.exp(-lin_vel_error / (1.0**2))

# PROVEN: Command scaling - no reward for zero commands
command_magnitude = torch.norm(commands[:, :2], dim=1)
vel_reward *= (command_magnitude > 0.1).float()
```

**PROPER BIPEDAL GAIT PATTERNS:**
   ```python
# PROVEN: Isaac Lab bipedal air time - rewards single stance phases
contact_sensor = env.scene.sensors["contact_forces"]
foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)

air_time = contact_sensor.data.current_air_time[:, foot_ids]
contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
in_contact = contact_time > 0.0

# Reward single stance (proper bipedal pattern)
single_stance = torch.sum(in_contact.int(), dim=1) == 1
in_mode_time = torch.where(in_contact, contact_time, air_time)
gait_reward = torch.min(torch.where(single_stance.unsqueeze(-1), in_mode_time, 0.0), dim=1)[0]
gait_reward = torch.clamp(gait_reward, max=0.5)  # Cap air time

# Scale by command magnitude
gait_reward *= (command_magnitude > 0.1).float()
```

**CONTACT-AWARE FOOT SLIDING PENALTY:**
```python
# PROVEN: Isaac Lab foot sliding - only penalize when in contact
forces = contact_sensor.data.net_forces_w_history[:, :, foot_ids, :]
contacts = forces.norm(dim=-1).max(dim=1)[0] > 1.0  # Historical contact
body_vel = robot.data.body_lin_vel_w[:, foot_ids, :2]
slide_penalty = torch.sum(body_vel.norm(dim=-1) * contacts, dim=1)
```

**IMPROVED FOUNDATION TEMPLATE WITH ISAAC LAB PATTERNS:**
```python
def sds_custom_reward(env) -> torch.Tensor:
    """Robust walking reward using proven Isaac Lab patterns."""
    from isaaclab.utils.math import quat_apply_inverse, yaw_quat
    
    robot = env.scene["robot"]
    contact_sensor = env.scene.sensors["contact_forces"]
    
    # === PROVEN VELOCITY TRACKING (YAW-ALIGNED FRAME) ===
    commands = env.command_manager.get_command("base_velocity")
    command_magnitude = torch.norm(commands[:, :2], dim=1)
    
    # Transform to yaw-aligned frame (much better than body frame)
    vel_yaw = quat_apply_inverse(yaw_quat(robot.data.root_quat_w), robot.data.root_lin_vel_w[:, :3])
    lin_vel_error = torch.sum(torch.square(commands[:, :2] - vel_yaw[:, :2]), dim=1)
    vel_reward = torch.exp(-lin_vel_error / (1.0**2))
    vel_reward *= (command_magnitude > 0.1).float()  # No reward for zero commands
    
    # === ROBUST BIPEDAL GAIT PATTERN ===
    foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
    foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)
    
    air_time = contact_sensor.data.current_air_time[:, foot_ids]
    contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
    in_contact = contact_time > 0.0
    
    # Reward proper single stance phases
    single_stance = torch.sum(in_contact.int(), dim=1) == 1
    in_mode_time = torch.where(in_contact, contact_time, air_time)
    gait_reward = torch.min(torch.where(single_stance.unsqueeze(-1), in_mode_time, 0.0), dim=1)[0]
    gait_reward = torch.clamp(gait_reward, max=0.5) * (command_magnitude > 0.1).float()
    
    # === STABLE HEIGHT & ORIENTATION ===
    height_error = (robot.data.root_pos_w[:, 2] - 0.74).abs()
    height_reward = torch.exp(-height_error / 0.3)
    
    gravity_proj = robot.data.projected_gravity_b[:, :2]
    lean_reward = torch.exp(-2.0 * torch.norm(gravity_proj, dim=1))
    
    # === FOUNDATION TOTAL ===
foundation_reward = (
        vel_reward * 3.0 +        # Proven velocity tracking
        gait_reward * 2.0 +       # Proven gait patterns
        height_reward * 2.0 +     # Height maintenance
        lean_reward * 1.5 +       # Orientation stability
        0.5                       # Baseline bonus
    )
    
    # === ADD ENVIRONMENTAL IF ANALYSIS SHOWS FEATURES ===
    # Add terrain_bonus, obstacle_bonus, gap_navigation_bonus here if needed
    
    return foundation_reward.clamp(min=0.1, max=10.0)
```

**MANDATORY: INTELLIGENT DESIGN, NOT TEMPLATE COPYING**

**FORBIDDEN: GENERIC TEMPLATE COPYING**
**AVOID THESE APPROACHES:**
- Copy code patterns from examples without thinking
- Use generic thresholds for all environments  
- Apply same weights to all scenarios
- Include components because they exist in examples

**REQUIRED: ANALYSIS-DRIVEN INTELLIGENT DESIGN**
- Read environment analysis data systematically
- Extract specific parameters and use them to inform design decisions
- Adapt thresholds, weights, and strategies to the actual environment
- Consider what makes this environment unique

**INTELLIGENT DESIGN PROCESS PRINCIPLES:**

**STEP 1: ANALYZE ENVIRONMENT DATA**
- Extract exact numbers: gap counts, obstacle counts, terrain measurements
- Identify dominant challenges: What's the biggest issue for the robot?
- Assess complexity level: Simple stepping vs complex navigation?

**STEP 2: DESIGN CONTEXTUAL STRATEGY**  
- **If 90% steppable gaps**: Focus on stepping precision, minimal jumping
- **If 70% jumpable gaps**: Focus on jumping mechanics, secondary stepping
- **If dense obstacles**: Conservative navigation with large safety margins
- **If low terrain roughness**: Efficiency focus, minimal terrain adaptation

**STEP 3: IMPLEMENT INTELLIGENT ADAPTATIONS**
- Use analysis data to set appropriate thresholds
- Weight components based on challenge priorities  
- Design for the ACTUAL environment, not generic scenarios

**ENVIRONMENT-DRIVEN DESIGN PRINCIPLES:**

**ANALYSIS-BASED THRESHOLD DESIGN:**
1. **Extract quantitative data** from environment analysis
2. **Identify challenge distributions** (what percentage of each type)
3. **Prioritize by frequency** (address dominant challenges first)
4. **Design adaptive boundaries** that match actual observed values

**INTELLIGENT WEIGHTING STRATEGY:**
```python
# Principle: Weight components based on actual environment challenges
challenge_frequency_analysis = analyze_environment_distribution()
primary_challenge_weight = calculate_dominant_weight(challenge_frequency_analysis)
secondary_challenge_weight = calculate_supporting_weight(challenge_frequency_analysis)
safety_constraint_weight = calculate_safety_weight(challenge_frequency_analysis)
```

**INTELLIGENT THINKING PRINCIPLES:**

1. **READ THE DATA**: Don't assume - extract actual numbers from environment analysis
2. **IDENTIFY PATTERNS**: What type of challenges dominate? 
3. **PRIORITIZE INTELLIGENTLY**: Focus effort on the biggest challenges
4. **ADAPT PARAMETERS**: Don't use generic thresholds - adapt to the environment
5. **THINK LIKE AN EXPERT**: What would a roboticist do for THIS specific terrain?

**TRAINING STABILITY WARNING: FOUNDATION-FIRST APPROACH REQUIRED**

**CRITICAL: Basic locomotion must be stable BEFORE adding environmental components!**

**FOUNDATION-FIRST DEVELOPMENT SEQUENCE:**
1. **START**: Basic walking (velocity tracking, height maintenance, orientation stability)
2. **ADD**: Contact control and smoothness  
3. **THEN**: Simple environmental components IF needed
4. **FINALLY**: Complex environmental integration IF environment analysis shows it's necessary

**ENVIRONMENTAL COMPONENTS ARE OPTIONAL, NOT MANDATORY**
- **ONLY include environmental sensing IF your analysis shows terrain variations, gaps, or obstacles**
- **FLAT walking environments**: Focus on natural locomotion, skip complex environmental sensing
- **SIMPLE terrains**: Basic terrain adaptation only
- **COMPLEX environments**: Gradual environmental integration after basic locomotion works

**STABLE TRAINING PRIORITIES:**
1. **Safety First**: Joint limits, orientation stability (prevent crashes)
2. **Basic Function**: Height, velocity tracking (achieve task goals)
3. **Movement Quality**: Smoothness, coordination (natural appearance)
4. **Environmental Adaptation**: ONLY if environment analysis indicates necessity

**CRITICAL CODE STRUCTURE REQUIREMENT:**

**FORBIDDEN: DO NOT CREATE HELPER FUNCTIONS**
- **NO** separate function definitions (def helper_function...)
- **NO** function calls outside of available Isaac Lab functions
- **ALL LOGIC MUST BE INLINE** within the sds_custom_reward function
- **USE ONLY** torch operations and provided Isaac Lab data access

**REQUIRED PATTERN: ALL LOGIC INLINE**
```python
def sds_custom_reward(env) -> torch.Tensor:
    import torch
    # ALL your calculation logic goes here directly - no function calls!
    lin_vel_error = torch.norm(robot.data.root_lin_vel_b[:, :2] - commands[:, :2], dim=1)  # INLINE
# NOT: lin_err, ang_err = get_velocity_tracking_error(...)  # FORBIDDEN
    return reward.clamp(min=0.0, max=10.0)
```

**ðŸ§  ENVIRONMENT-DRIVEN REWARD DESIGN:**
- **Foundation first**: Establish stable, human-like locomotion before adding complexity
- **Quality over complexity**: Simple, well-designed rewards beat complex environmental integration
- **Biomechanical principles**: Base design on how humans actually walk
- **Progressive learning**: Guide robot from simple to complex behaviors as needed

**MANDATORY: GENTLE MOVEMENT ENFORCEMENT**

ALL generated reward functions MUST prioritize GENTLE, CONTROLLED, SUSTAINABLE movements:

Do not hardcode anything from examples, think and decide for task requirements.

**ðŸ“‹ ENVIRONMENTAL SENSOR INTEGRATION (Reference Only - Use Sparingly)**

**IMPORTANT: Environmental integration should be based on environment analysis, not forced requirements!**

**ENVIRONMENTAL COMPONENTS (INCLUDE ONLY IF ENVIRONMENT ANALYSIS SHOWS RELEVANCE):**

**1. TERRAIN ANALYSIS (ONLY IF TERRAIN SHOWS SIGNIFICANT VARIATION):**
   - **Check first**: Does environment analysis show terrain roughness > 0.1 or elevation changes?
   - **If NO**: Skip terrain analysis, focus on basic locomotion
   - **If YES**: Include simplified terrain adaptation
   ```python
   # ONLY include if terrain analysis shows variation
   height_sensor = env.scene.sensors["height_scanner"]
   height_scan = height_sensor.data.ray_hits_w[..., 2].view(env.num_envs, -1)
   height_scan = torch.where(torch.isfinite(height_scan), height_scan, torch.zeros_like(height_scan))
   terrain_roughness = torch.clamp(torch.var(height_scan, dim=1), max=1.0)
   terrain_bonus = torch.exp(-terrain_roughness * 1.0) * 0.3
   ```

**2. OBSTACLE DETECTION (ONLY IF OBSTACLES ARE PRESENT):**
   - **Check first**: Does environment analysis show obstacles detected (count > 0)?
   - **If NO**: Skip obstacle avoidance, focus on basic locomotion
   - **If YES**: Include simplified obstacle awareness
   ```python
   # ONLY include if environment analysis shows obstacles
   lidar_sensor = env.scene.sensors["lidar"]
   lidar_range = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1).view(env.num_envs, -1)
   lidar_range = torch.where(torch.isfinite(lidar_range), lidar_range, torch.ones_like(lidar_range) * 10.0)
   min_obstacle_distance = torch.min(lidar_range[:, :lidar_range.shape[1] // 4], dim=1)[0]
   obstacle_bonus = torch.clamp((min_obstacle_distance - 1.0) / 2.0, min=0.0, max=0.5)
   ```

**3. GAP NAVIGATION (ONLY IF GAPS ARE DETECTED):**
   - **Check first**: Does environment analysis show gaps detected (count > 0)?
   - **If NO**: Skip gap navigation, focus on basic locomotion
   - **If YES**: Include adaptive navigation based on gap size distribution
   ```python
   # ONLY include if environment analysis shows gaps detected
   # Real-time gap size detection for adaptive navigation
   robot_height = robot.data.root_pos_w[:, 2]
   forward_terrain = height_scan[:, :height_scan.shape[1]//3]
   gap_depth = robot_height.unsqueeze(1) - forward_terrain
   gap_detected = gap_depth > 0.15
   small_gaps = (gap_depth <= 0.30) & gap_detected  # Steppable
   medium_gaps = (gap_depth > 0.30) & (gap_depth <= 0.60) & gap_detected  # Jumpable
   gap_navigation_bonus = torch.any(small_gaps, dim=1).float() * 0.2 + torch.any(medium_gaps, dim=1).float() * 0.3
   ```

**ADVANCED: ADAPTIVE BEHAVIOR IMPLEMENTATION PATTERNS**

Technical implementation patterns for environment-adaptive behaviors:

**CONDITIONAL BEHAVIOR IMPLEMENTATION PATTERN:**
```python
# Technical pattern: Environment-conditional reward computation
environmental_condition = detect_environmental_feature(sensor_data)
if torch.any(environmental_condition):
    # Adapt reward computation based on detected conditions
    adapted_metrics = calculate_context_specific_metrics(robot_state, environmental_condition)
    conditional_reward = apply_adaptive_reward_function(adapted_metrics)
    
    # Combine with base locomotion using appropriate weighting
    total_reward = base_locomotion_reward + conditional_reward * environment_weight
    else:
    conditional_reward = torch.zeros(env.num_envs, device=env.device)
```

**SENSOR DATA PROCESSING PATTERNS:**
```python
# Technical pattern: Safe sensor data access and processing
sensor_data = env.scene.sensors["sensor_name"].data.raw_data
processed_data = apply_sensor_processing(sensor_data)
environment_features = extract_relevant_features(processed_data)

# Combine environmental information with robot state
robot_state_vector = extract_robot_state_vector(robot.data)
context_aware_reward = compute_context_reward(robot_state_vector, environment_features)
```

**ADAPTIVE REWARD INTEGRATION PATTERN:**
```python
# Technical pattern: Combining multiple reward components
base_reward_components = [velocity_reward, gait_reward, height_reward]
environmental_reward_components = [context_aware_reward]

# Weight environmental components appropriately
base_total = sum(base_reward_components)
environmental_total = sum(environmental_reward_components) * environment_weight

# Ensure environmental components don't overwhelm base locomotion
final_reward = base_total + torch.clamp(environmental_total, max=base_total * 0.3)
```

**KEY IMPLEMENTATION DIFFERENCES:**

**STEPPING (Small Gaps):**
- **Focus**: Stride extension + precision + controlled speed
- **Mechanics**: Single-foot clearance, extended step length, accurate placement
- **Speed**: Controlled forward velocity (0.7-1.3 m/s)

**JUMPING (Medium Gaps):**
- **Focus**: Bilateral coordination + vertical motion + aerial phase
- **Mechanics**: Both feet takeoff, sustained air time, arm coordination
- **Speed**: Burst vertical velocity (1.5-2.5 m/s upward)

**AVOIDANCE (Large Gaps):**
- **Focus**: Path planning + turning + lateral movement
- **Mechanics**: Direction changes, lateral stepping, conservative approach
- **Speed**: Reduced forward velocity (<0.5 m/s near impossible gaps)

**ðŸ§  INTELLIGENT IMPLEMENTATION TIPS:**

1. **Real-time Gap Classification**: Detect gap types dynamically using height scanner
2. **Behavior Switching**: Only activate relevant behavior based on detected gap type
3. **Progressive Learning**: Start with stepping, advance to jumping, then avoidance
4. **Safety Priority**: Always prioritize stability over gap crossing efficiency
5. **Combine with Foundation**: These behaviors enhance, don't replace, basic locomotion

This approach gives the robot **three distinct locomotion strategies** that activate automatically based on gap size detection!

**TERRAIN CLASSIFICATION TECHNICAL PATTERNS**

**COMMON IMPLEMENTATION ISSUE: Environment misclassification**
1. **Static thresholds** fail to adapt to different terrain types
2. **Simple classification** may not distinguish between navigable and dangerous features
3. **Conflicting objectives** when reward components have opposing goals

**TECHNICAL SOLUTION: ADAPTIVE CLASSIFICATION PATTERNS**

**PATTERN 1: ENVIRONMENT FEATURE CLASSIFICATION**
```python
# Technical pattern: Multi-criteria classification with adaptive thresholds
sensor_data = process_sensor_input(env.scene.sensors["sensor_name"].data)
feature_metrics = calculate_classification_metrics(sensor_data)
classification_result = apply_multi_threshold_classification(feature_metrics)

# Use classification results to modify reward computation
adaptive_reward_weights = compute_adaptive_weights(classification_result)
context_modified_reward = base_reward * adaptive_reward_weights
```

**PATTERN 2: ADAPTIVE TARGET COMPUTATION**
```python
# Technical pattern: Environment-based target adaptation
environment_context = extract_environment_context(sensor_data)
base_target = get_default_target_value()
adapted_target = compute_context_adaptive_target(base_target, environment_context)

# Apply adaptive tolerance based on environmental conditions
tolerance_factor = calculate_adaptive_tolerance(environment_context)
target_error = torch.abs(current_value - adapted_target)
adaptive_reward = torch.exp(-target_error / tolerance_factor)

# Bonus rewards for context-appropriate behaviors
context_appropriate_behavior = detect_appropriate_behavior(robot_state, environment_context)
bonus_reward = context_appropriate_behavior * context_strength * bonus_weight
```

**STEP 3: STAIR-SPECIFIC LOCOMOTION BEHAVIOR**
```python
# DOWNWARD STAIR NAVIGATION STRATEGY
if torch.any(is_stair_down) or torch.any(is_steep_stair):
    # 1. CONTROLLED FORWARD SPEED (slower on stairs)
    forward_velocity = robot.data.root_lin_vel_b[:, 0]
    stair_speed = torch.exp(-torch.abs(forward_velocity - 0.6) / 0.2)  # Target 0.6 m/s
    
    # 2. ENHANCED FOOT PLACEMENT (precise stepping)
    foot_positions = robot.data.body_pos_w[:, foot_ids, :]
    step_length = torch.norm(foot_positions[:, 0, :2] - foot_positions[:, 1, :2], dim=1)
    controlled_steps = torch.exp(-torch.abs(step_length - 0.4) / 0.1)  # Shorter steps: 40cm
    
    # 3. INCREASED GROUND CONTACT (stability priority)
    contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
    foot_contacts = (contact_time > 0.05).float()
    stability_contact = foot_contacts.mean(dim=1)  # Prefer more ground contact
    
    # 4. CONSERVATIVE LEAN (prevent forward tumbling)
    gravity_proj = robot.data.projected_gravity_b[:, :2]
    conservative_lean = torch.exp(-3.0 * torch.norm(gravity_proj, dim=1))  # Tighter lean control
    
    # 5. ARM BALANCE (extended for stability)
    shoulder_pitch_indices, _ = robot.find_joints(["left_shoulder_pitch_joint", "right_shoulder_pitch_joint"])
shoulder_pitch_indices = torch.tensor(shoulder_pitch_indices, dtype=torch.long, device=env.device)
shoulder_angles = robot.data.joint_pos[:, shoulder_pitch_indices]
    balance_arms = torch.exp(-torch.abs(shoulder_angles.mean(dim=1) + 0.1) / 0.3)  # Slight forward arm extension
    
    stair_navigation_reward = (
        stair_speed * 0.3 +           # Controlled speed
        controlled_steps * 0.25 +     # Precise stepping  
        stability_contact * 0.2 +     # Ground contact priority
        conservative_lean * 0.15 +    # Prevent tumbling
        balance_arms * 0.1            # Arm balance
    )
else:
    stair_navigation_reward = torch.zeros(env.num_envs, device=env.device)
```

**STEP 4: INTEGRATE WITH EXISTING REWARD STRUCTURE**
```python
# REPLACE RIGID HEIGHT CONSTRAINT
# OLD: height_reward = torch.exp(-height_err / 0.05)  # RIGID - CAUSES FREEZING
# NEW: Use adaptive height + stair bonuses

improved_height_system = (
    adaptive_height_reward * 1.0 +      # Terrain-adaptive height
    stair_descent_bonus * 0.8 +         # Controlled descent bonus
    stair_navigation_reward * 1.2       # Stair-specific locomotion
)

# REPLACE GAP DETECTION WITH INTELLIGENT CLASSIFICATION  
# OLD: Simple gap_depth calculation misclassifies stairs
# NEW: Use stair detection + appropriate navigation strategies

intelligent_terrain_navigation = (
    stair_navigation_reward * 1.2 +     # HIGHEST: Stair navigation when stairs detected
    stepping_reward * 1.0 +             # Standard stepping for small gaps
    jumping_reward * 1.2 +              # Jumping for medium gaps
    avoidance_reward * 0.8              # Avoidance for true gaps
)
```

**ðŸŽ¯ KEY BEHAVIORAL CHANGES FOR STAIR SUCCESS:**

**STAIRS DETECTION:**
- **Regular step pattern**: Low variance in descent heights
- **Gradual descent**: 10-80cm drops, not >1.5m gaps
- **Terrain classification**: Distinguish navigable stairs from dangerous gaps

**STAIR DESCENT BEHAVIOR:**
- **Controlled speed**: 0.6 m/s (slower than flat terrain)
- **Shorter steps**: 40cm step length for precision
- **Enhanced contact**: More ground contact time for stability  
- **Conservative lean**: Tighter posture control to prevent tumbling
- **Adaptive height**: Target height adjusts to terrain level

**CONFLICT RESOLUTION:**
- **Terrain-adaptive height constraint** (not rigid 0.75m)
- **Descent-encouraging velocity rewards** for stair terrain
- **Coordinated locomotion** that balances movement with stability

**IMPLEMENTATION NOTE:**
This approach resolves conflicting objectives by using adaptive targets and context-aware reward computation rather than rigid constraints.

**PATTERN 3: CONFLICTING OBJECTIVE RESOLUTION**

**COMMON ISSUE: Multiple reward components with opposing goals**

When different reward components encourage conflicting behaviors:
- Component A encourages behavior X
- Component B discourages behavior X

**TECHNICAL RESOLUTION FRAMEWORK:**

**APPROACH 1: DYNAMIC WEIGHT COMPUTATION**
   ```python
# Technical pattern: Context-dependent component weighting
environmental_context = analyze_environmental_state(sensor_data)
priority_weights = compute_dynamic_weights(environmental_context)

# Apply contextual weighting to reward components
weighted_component_a = component_a_value * priority_weights.component_a
weighted_component_b = component_b_value * priority_weights.component_b
final_reward = weighted_component_a + weighted_component_b
```

**APPROACH 2: ADAPTIVE TARGET MODIFICATION**
   ```python
# Technical pattern: Context-sensitive target adjustment
base_targets = get_default_targets()
environmental_constraints = evaluate_environmental_constraints(sensor_data)
modified_targets = apply_constraint_based_modification(base_targets, environmental_constraints)

# Compute rewards using modified targets
target_tracking_reward = compute_tracking_reward(current_state, modified_targets)
```

**APPROACH 3: MULTI-OBJECTIVE REWARD DESIGN**
   ```python
# Technical pattern: Balanced multi-objective optimization
objective_balance = compute_balanced_objectives(safety_metrics, goal_metrics)

Reward Components:
- Base velocity tracking (when safe)
- Evasive maneuvers (when obstacles detected)
- Path efficiency (maintaining general direction)
- Safety margins (maintaining distance from obstacles)
```

**PRINCIPLE 4: SITUATIONAL AWARENESS**
```
Instead of: Reactive collision avoidance
predictive_planning = implement_forward_looking_behavior(sensor_data, prediction_horizon)

Awareness Levels:
- Immediate proximity (emergency response)
- Forward path prediction (planning ahead)
- Environmental density assessment (context switching)
- Historical collision patterns (learning from experience)
```

**ðŸŽ¯ IMPLEMENTATION DECISION TREE:**

```
1. ASSESS SITUATION:
   â†’ Is forward path clear? â†’ Use standard velocity tracking
   â†’ Are obstacles nearby? â†’ Apply contextual strategy
   â†’ Is collision imminent? â†’ Emergency avoidance priority

2. CHOOSE STRATEGY BASED ON ANALYSIS:
   â†’ Dense obstacles (>50% sensors triggered) â†’ Safety-first mode
   â†’ Sparse obstacles (<20% sensors triggered) â†’ Velocity-priority mode
   â†’ Moderate obstacles (20-50%) â†’ Balanced adaptive mode

3. IMPLEMENT ADAPTIVE BEHAVIOR:
   â†’ Modify velocity targets (don't abandon them)
   â†’ Reward intelligent evasion (lateral movement, turning)
   â†’ Maintain directional intent while ensuring safety
   â†’ Scale rewards based on situational requirements
```

**ðŸ’¡ DESIGN THINKING PROMPTS:**

- **"How would a human handle this obstacle while trying to reach their destination?"**
- **"What's more important: reaching the exact commanded velocity or avoiding collision?"**
- **"Can I modify the velocity target to be both safe and goal-directed?"**
- **"How can I reward the robot for being smart about navigation?"**

**FLEXIBLE IMPLEMENTATION GUIDELINES:**

1. **Use environmental sensor data** to assess obstacle density and proximity
2. **Scale velocity targets** based on safety requirements (don't make them binary)
3. **Reward intelligent alternatives** when direct paths are blocked
4. **Consider prediction horizons** appropriate for robot speed and environment
5. **Balance task completion with safety** based on situational assessment

**AVOID THESE ANTI-PATTERNS:**
- Fixed priority hierarchies that ignore context
- Binary switches (either velocity OR safety)
- Reactive-only responses without prediction
- Hardcoded distance thresholds for all environments
- Ignoring the overall task goal when avoiding obstacles

**EMBRACE THESE PRINCIPLES:**
- Context-aware behavior switching
- Intelligent target modification 
- Predictive planning with appropriate horizons
- Multi-objective optimization that considers both goals and constraints
- Adaptive strategies that match human-like intelligence

The goal is to create robots that navigate like intelligent agents, not simple reactive systems!

The output of the reward function should be only your reward value.
The code output should be formatted as a python code string: "```python ... ```".

Some helpful tips for writing the reward function code:
    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like torch.exp to the overall reward or its components
    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable
    (3) Most importantly, the reward code's must only use attributes of the provided environment object (namely, variables that have prefix env.). Under no circumstance can you introduce new input variables.
    (4) You may use functions from torch and numpy as well as Isaac Lab math functions (already imported): quat_apply_inverse, yaw_quat, matrix_from_quat, etc. All Isaac Lab math functions are available from isaaclab.utils.math (already imported). Do NOT import from isaacgym - use Isaac Lab APIs only.
    (5) Make sure any new tensor or variable you introduce is on the same device as the environment tensors.
    (6) The reward terms for desired behavior should be positive. Do not write a term that negatively penalizes the policy for a lack of desired behavior. We do not want the total reward to be negative.
    (7) The reward terms that penalize undesirable behavior should be negative. You should not provide a positive reward for avoiding undesirable behavior, but instead you should penalize the policy when it does execute undesirable behavior.
    (8) When using the exponential function, make sure that your value is bounded. If it's not, please clamp it to provide an upper or lower bound.