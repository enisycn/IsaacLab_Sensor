üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®
üö®üö®üö® CRITICAL: .distances AND .height_measurements DO NOT EXIST - CAUSE INSTANT CRASH üö®üö®üö®
üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®

‚ùå THESE LINES CAUSE ATTRIBUTEERROR CRASH - NEVER USE:
```python
lidar_sensor.data.distances          # AttributeError: 'RayCasterData' object has no attribute 'distances'
height_sensor.data.height_measurements  # AttributeError: 'RayCasterData' object has no attribute 'height_measurements'
lidar_dist = lidar_sensor.data.distances                 # INSTANT CRASH!
height_data = height_sensor.data.height_measurements     # INSTANT CRASH!
```

‚úÖ THESE ARE THE ONLY WORKING PATTERNS - COPY EXACTLY:
```python
# LiDAR distances - MANDATORY PATTERN:
lidar_sensor = env.scene.sensors["lidar"]
lidar_range = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)

# Height scanner - MANDATORY PATTERN:
height_sensor = env.scene.sensors["height_scanner"]
height_scan = height_sensor.data.ray_hits_w[..., 2]
```

üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®

**PROVEN ISAAC LAB LOCOMOTION PATTERNS - PRIORITIZE THESE!**

**üö®üö®üö® CRITICAL ATTRIBUTEERROR WARNING üö®üö®üö®**

**THE #1 TRAINING CRASH ERROR:**
```python
# ‚ùå DEADLY ERROR - WILL CRASH TRAINING 100% OF THE TIME:
lidar_sensor.data.distances  # AttributeError: 'RayCasterData' object has no attribute 'distances'
height_sensor.data.height_measurements  # AttributeError: 'RayCasterData' object has no attribute 'height_measurements'
```

**‚úÖ MANDATORY CORRECT PATTERNS - COPY EXACTLY:**
```python
# LiDAR distances - ALWAYS use this pattern:
lidar_sensor = env.scene.sensors["lidar"]
lidar_range = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)

# Height scanner - ALWAYS use this pattern:
height_sensor = env.scene.sensors["height_scanner"]
height_scan = height_sensor.data.ray_hits_w[..., 2]
```

**üö® Isaac Lab RayCaster sensors ONLY have these attributes:**
- `data.pos_w` - Sensor position [num_envs, 3]
- `data.quat_w` - Sensor orientation [num_envs, 4]  
- `data.ray_hits_w` - Ray hit positions [num_envs, num_rays, 3]

**NO OTHER ATTRIBUTES EXIST! Using .distances or .height_measurements = INSTANT CRASH!**

**üö®üö®üö® CRITICAL BEHAVIORAL ERRORS WARNING üö®üö®üö®**

**‚ùå HEIGHT HARDCODING ERROR - CAUSES ROBOTS TO STAND STILL ON STAIRS:**
```python
# WRONG - Absolute world height (robots stand still on elevated terrain):
height_err = torch.abs(robot.data.root_pos_w[:, 2] - 0.74)  # Forces 0.74m absolute height!
```

**‚úÖ TERRAIN-RELATIVE HEIGHT - ALLOWS NATURAL TERRAIN NAVIGATION:**
```python
# CORRECT - Height relative to terrain underneath:
height_sensor = env.scene.sensors["height_scanner"]
terrain_height = height_sensor.data.ray_hits_w[..., 2].mean(dim=-1)  # Average terrain under robot
relative_height = robot.data.root_pos_w[:, 2] - terrain_height
height_err = torch.abs(relative_height - 0.74)  # 0.74m ABOVE terrain
```

**‚ùå ARM BILATERAL SYNCHRONY ERROR - CAUSES UNNATURAL ARM CROSSING:**
```python
# WRONG - Forces both arms to same angle (bilateral synchrony):
mean_sh = torch.mean(torch.abs(sh_angles), dim=1)  # Both arms same angle!
arm_err = (mean_sh - 0.3)**2  # Forces both to 17¬∞ - UNNATURAL!
```

**‚úÖ NATURAL ARM MOVEMENT - PREFER GRAVITY-DRIVEN OR RECIPROCAL:**
```python
# OPTION 1: NO ARM REWARDS (let physics/gravity handle naturally)
# - Often produces better results than forced constraints
# - Arms naturally swing as passive pendulums

# OPTION 2: Reciprocal arm-leg coordination (if needed):
left_arm = sh_angles[:, 0]   # Left shoulder angle
right_arm = sh_angles[:, 1]  # Right shoulder angle
reciprocal_reward = -torch.abs(left_arm + right_arm)  # Reward opposite directions
```

These patterns are battle-tested and known to work for humanoid locomotion. Use them as a foundation:

**üèÜ ISAAC LAB PROVEN REWARD FUNCTIONS (USE THESE FIRST!)**

**These are production-ready Isaac Lab functions that create excellent human-like walking:**

**1. BIPEDAL AIR TIME REWARD (SUPERIOR FOR HUMANOIDS) - SINGLE STANCE IS CRITICAL:**
```python
# üéØ PROVEN: Built-in Isaac Lab function for bipedal gait patterns
# CRITICAL INSIGHT: Natural human walking = alternating SINGLE SUPPORT phases!
contact_sensor = env.scene.sensors["contact_forces"]
foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)

# Access air time and contact time data (VERIFIED WORKING)
air_time = contact_sensor.data.current_air_time[:, foot_ids]
contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
in_contact = contact_time > 0.0

# üöÄ KEY FOR NATURAL WALKING: Single stance reward (one foot at a time)
# This is what makes human walking look natural vs robotic shuffling!
single_stance = torch.sum(in_contact.int(), dim=1) == 1  # ONLY ONE FOOT DOWN!
in_mode_time = torch.where(in_contact, contact_time, air_time)
gait_reward = torch.min(torch.where(single_stance.unsqueeze(-1), in_mode_time, 0.0), dim=1)[0]

# ‚ö° PROPER AIR TIME MANAGEMENT: 
# - Threshold prevents excessive foot lifting (unnatural high knees)
# - Rewards natural step timing for smooth walking rhythm
gait_reward = torch.clamp(gait_reward, max=0.5)  # Cap at 0.5s for natural step timing

# üéØ COMMAND DEPENDENCY: No reward for zero command (prevents stationary exploitation)
commands = env.command_manager.get_command("base_velocity")
command_magnitude = torch.norm(commands[:, :2], dim=1)
gait_reward *= (command_magnitude > 0.1).float()  # Only reward when actually moving
```

**WHY THIS PATTERN IS SUPERIOR TO GENERIC AIR TIME:**
- **Single stance detection**: Natural walking requires alternating support (not double support shuffling)
- **Contact-aware timing**: Uses actual contact sensor data, not estimated patterns
- **Anti-exploitation**: No reward for standing still or micro-movements
- **Natural rhythm**: 0.5s threshold matches human walking cadence
- **Isaac Lab optimized**: Uses proven sensor patterns that work reliably

**2. YAW-ALIGNED VELOCITY TRACKING (PROVEN SUPERIOR TO BODY FRAME):**
```python
# üéØ PROVEN: Isaac Lab's yaw-frame tracking - VASTLY superior to basic body frame
# CRITICAL: Decouples velocity control from robot tilt/lean (prevents control coupling)
from isaaclab.utils.math import quat_apply_inverse, yaw_quat

robot = env.scene["robot"]
commands = env.command_manager.get_command("base_velocity")

# üöÄ YAW-ALIGNED TRANSFORMATION: Removes pitch/roll interference
# This is why Isaac Lab velocity tracking works so much better!
vel_yaw = quat_apply_inverse(yaw_quat(robot.data.root_quat_w), robot.data.root_lin_vel_w[:, :3])
lin_vel_error = torch.sum(torch.square(commands[:, :2] - vel_yaw[:, :2]), dim=1)
vel_reward = torch.exp(-lin_vel_error / (1.0**2))  # Exponential kernel for smooth decay

# üéØ CRITICAL: No reward for zero commands (anti-exploitation)
vel_reward *= (torch.norm(commands[:, :2], dim=1) > 0.1).float()
```

**WHY YAW-ALIGNED IS SUPERIOR:**
- **Decoupled control**: Velocity tracking unaffected by robot lean/tilt
- **Stable tracking**: Works even when robot pitches forward/backward
- **Natural movement**: Allows body dynamics while maintaining velocity goals
- **Isaac Lab proven**: This is the actual pattern used in successful Isaac Lab locomotion

**3. ANGULAR VELOCITY TRACKING (WORLD FRAME FOR STABILITY):**
```python
# üéØ PROVEN: Isaac Lab's world frame angular tracking (superior to body frame)
commands = env.command_manager.get_command("base_velocity")
robot = env.scene["robot"]

# World frame angular velocity for consistent turning control
ang_vel_error = torch.square(commands[:, 2] - robot.data.root_ang_vel_w[:, 2])
ang_reward = torch.exp(-ang_vel_error / (1.0**2))
```

**4. CONTACT-AWARE FOOT SLIDING PENALTY (INTELLIGENT SLIDING DETECTION):**
```python
# üéØ PROVEN: Isaac Lab's sliding penalty - ONLY when foot is actually in contact
# CRITICAL: This prevents penalizing swing leg motion (which should move freely!)
contact_sensor = env.scene.sensors["contact_forces"]
foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)

# üöÄ SMART CONTACT DETECTION: Use force history for reliable contact state
forces = contact_sensor.data.net_forces_w_history[:, :, foot_ids, :]
contacts = forces.norm(dim=-1).max(dim=1)[0] > 1.0  # Actual contact detection

# Get foot velocities and apply sliding penalty ONLY when in contact
robot = env.scene["robot"]
body_vel = robot.data.body_lin_vel_w[:, foot_ids, :2]
slide_penalty = torch.sum(body_vel.norm(dim=-1) * contacts, dim=1)  # Contact-aware!
```

**WHY CONTACT-AWARE SLIDING IS CRITICAL:**
- **Swing phase freedom**: Doesn't penalize moving feet during swing phase
- **Realistic physics**: Only applies when feet should be stationary (in contact)
- **Force-based detection**: Uses actual physics data, not position estimates
- **Natural walking**: Allows proper foot lifting and placement

**üîë CRITICAL SUCCESS FACTORS FOR BIPEDAL WALKING:**

1. **SINGLE STANCE DOMINANCE**: Most natural walking happens in single support!
2. **YAW-ALIGNED CONTROL**: Decouples velocity from body orientation
3. **CONTACT-AWARE PENALTIES**: Only apply constraints when physically relevant
4. **COMMAND SCALING**: No rewards for micro-movements or standing still
5. **NATURAL TIMING**: Air time thresholds that match human walking rhythm (0.3-0.5s)
6. **FORCE-BASED DETECTION**: Use contact sensors, not position approximations

**üö® COMMON BIPEDAL WALKING FAILURES TO AVOID:**
- **Double support shuffling**: Not rewarding single stance phases
- **Robotic high stepping**: No upper threshold on air time rewards
- **Control coupling**: Using body frame instead of yaw-aligned frame
- **Swing leg penalties**: Penalizing foot motion during swing phase
- **Zero command exploitation**: Rewarding stationary micro-movements

**‚úÖ USE THESE ISAAC LAB PATTERNS AS YOUR FOUNDATION - THEY ARE BATTLE-TESTED!**

**2. PROPER BIPEDAL GAIT PATTERNS (UPDATED WITH ISAAC LAB INSIGHTS):**
```python
# üéØ PROVEN: Rewards single stance phases (proper walking pattern)
# CRITICAL FOR WALKING: Human walking = 85% single support, 15% double support!
foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)

air_time = contact_sensor.data.current_air_time[:, foot_ids]
contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
in_contact = contact_time > 0.0

# üöÄ SINGLE STANCE DETECTION: The secret to natural walking!
single_stance = torch.sum(in_contact.int(), dim=1) == 1  # One foot at a time
in_mode_time = torch.where(in_contact, contact_time, air_time)
gait_reward = torch.min(torch.where(single_stance.unsqueeze(-1), in_mode_time, 0.0), dim=1)[0]

# üéØ NATURAL STEP TIMING: Prevent robotic high-stepping
gait_reward = torch.clamp(gait_reward, max=0.5) * (command_magnitude > 0.1).float()

# üî• ALTERNATIVE PATTERN - Double Support Bonus (for stability when needed):
double_stance = torch.sum(in_contact.int(), dim=1) == 2  # Both feet down
stability_bonus = double_stance.float() * 0.1  # Small bonus for stable phases
```

**ADVANCED BIPEDAL PATTERNS FOR SPECIFIC GAITS:**

**WALKING GAIT (Primary Pattern):**
```python
# Natural walking = alternating single support + brief double support
walking_pattern = single_stance.float() * 0.8 + double_stance.float() * 0.2
```

**RUNNING GAIT (Flight Phase):**
```python
# Running = single support + flight phase (no double support)
flight_phase = torch.sum(in_contact.int(), dim=1) == 0  # Both feet up
running_pattern = single_stance.float() * 0.6 + flight_phase.float() * 0.4
```

**MARCHING GAIT (Controlled Single Support):**
```python
# Marching = extended single support phases for precision
extended_single = single_stance & (in_mode_time.max(dim=1)[0] > 0.3)
marching_pattern = extended_single.float()
```

**üö® PRIORITY: Use these proven Isaac Lab functions as your foundation, then add task-specific enhancements!**

**üî• ADDITIONAL ISAAC LAB BUILT-IN FUNCTION USAGE:**

When environment analysis shows you need specific locomotion patterns, you can use Isaac Lab's built-in reward functions directly in your reward configuration. However, for SDS custom rewards, you should implement the patterns inline as shown above.

**BUILT-IN FUNCTION REFERENCE (for understanding, not direct use in custom rewards):**
```python
# These are the actual Isaac Lab functions - understand their patterns:
# mdp.feet_air_time_positive_biped(env, command_name="base_velocity", threshold=0.5, sensor_cfg=SceneEntityCfg("contact_forces"))
# mdp.track_lin_vel_xy_yaw_frame_exp(env, std=1.0, command_name="base_velocity")
# mdp.track_ang_vel_z_world_exp(env, command_name="base_velocity", std=1.0)
# mdp.feet_slide(env, sensor_cfg=SceneEntityCfg("contact_forces"))
```

**üéØ CRITICAL ISAAC LAB SUCCESS INSIGHTS:**
- **Command scaling**: NEVER reward when commands are near zero - prevents exploitation
- **Yaw alignment**: Removes pitch/roll interference from velocity tracking - critical for stability
- **Single stance**: Encourages proper alternating foot pattern - key for natural walking
- **Contact awareness**: Only apply penalties when actually relevant (foot in contact) - prevents swing phase penalties
- **Capped rewards**: Air time and other metrics should have reasonable upper bounds - prevents over-optimization
- **Force-based detection**: Use contact sensor forces, not position estimates - more reliable
- **Exponential kernels**: Provide smooth reward gradients for stable learning - better than linear penalties

## ISAAC LAB REWARD COMPUTATION PATTERNS

**Focus: Technical implementation for reward functions, not biomechanical theory**

** ENHANCED ENVIRONMENT REWARD PATTERNS (NEW - FOR ADVANCED LOCOMOTION):**

**4. VELOCITY-OBSTACLE CONFLICT RESOLUTION MATH:**
```python
# Enhanced sensor-based dynamic target modification for reward computation
height_scan = env.scene.sensors["height_scanner"].data.ray_hits_w[..., 2]
# ‚ùå INCORRECT: lidar_range = env.scene.sensors["lidar"].data.ray_hits_w[..., 2]  # Gets Z coordinate, not distance!
# ‚úÖ CORRECT: Calculate actual distances:
lidar_sensor = env.scene.sensors["lidar"]
lidar_range = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)
commands = env.command_manager.get_command("base_velocity")

# Adaptive velocity scaling based on forward terrain
scan_size = int(torch.sqrt(torch.tensor(height_scan.shape[1])))
height_grid = height_scan.view(env.num_envs, scan_size, scan_size)
forward_terrain = height_grid[:, :scan_size//3, scan_size//3:2*scan_size//3]
terrain_clearance = torch.min(forward_terrain.view(env.num_envs, -1), dim=1)[0]

# Dynamic velocity target modification for rewards
obstacle_factor = torch.clamp((terrain_clearance + 0.2) / 0.4, min=0.1, max=1.0)
adaptive_commands = commands.clone()
adaptive_commands[:, 0] *= obstacle_factor  # Scale forward velocity by terrain clearance

# Compute velocity reward with adaptive targets
vel_error = torch.sum((robot.data.root_lin_vel_b[:, :2] - adaptive_commands[:, :2])**2, dim=1)
velocity_reward = torch.exp(-vel_error / 1.0) * (commands[:, :2].norm(dim=1) > 0.1).float()
```

**5. STAIR VS GAP DETECTION FOR REWARDS:**
```python
# Mathematical patterns for stair/gap distinction in reward computation
height_scan = env.scene.sensors["height_scanner"].data.ray_hits_w[..., 2]
current_height = robot.data.root_pos_w[:, 2]

# Height differential analysis for step detection
scan_size = int(torch.sqrt(torch.tensor(height_scan.shape[1])))
height_grid = height_scan.view(env.num_envs, scan_size, scan_size)
forward_strip = height_grid[:, :scan_size//2, scan_size//3:2*scan_size//3]

# Stair pattern: gradual height change, gap pattern: abrupt drop then rise
height_gradient = torch.diff(forward_strip.mean(dim=2), dim=1)
is_stair_pattern = (torch.abs(height_gradient) < 0.15).all(dim=1)  # Gradual changes
is_gap_pattern = (height_gradient.min(dim=1)[0] < -0.3) & (height_gradient.max(dim=1)[0] > 0.1)

# Reward height adaptation based on terrain type
target_height = torch.where(is_stair_pattern, 
                          current_height - 0.05,  # Slight descent for stairs
                          torch.tensor(0.74, device=env.device))  # Normal height otherwise
height_reward = torch.exp(-torch.abs(current_height - target_height) / 0.1)
```

**6. GAP-SPECIFIC BEHAVIOR SWITCHING IN REWARDS:**
```python
# Mathematical gap size classification for reward computation
# ‚ùå INCORRECT: lidar_range = env.scene.sensors["lidar"].data.ray_hits_w[..., 2]  # Gets Z coordinate, not distance!
# ‚úÖ CORRECT: Calculate actual distances:
lidar_sensor = env.scene.sensors["lidar"]
lidar_range = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)
forward_rays = lidar_range[:, :len(lidar_range[0])//4]  # Forward-facing rays
min_forward_distance = torch.min(forward_rays, dim=1)[0]

# Gap size classification thresholds
small_gap = (min_forward_distance > 0.3) & (min_forward_distance < 0.8)   # Steppable
medium_gap = (min_forward_distance >= 0.8) & (min_forward_distance < 1.5)  # Jumpable  
large_gap = min_forward_distance >= 1.5  # Avoidable

# Adaptive gait rewards based on gap type
foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)
contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
air_time = contact_sensor.data.current_air_time[:, foot_ids]

# Different gait patterns for different gap types
stepping_reward = torch.where(small_gap.unsqueeze(-1), 
                            torch.clamp(contact_time.max(dim=1)[0], max=0.3), 
                            torch.zeros_like(contact_time.max(dim=1)[0]))
jumping_reward = torch.where(medium_gap.unsqueeze(-1),
                           torch.clamp(air_time.max(dim=1)[0], max=0.8),
                           torch.zeros_like(air_time.max(dim=1)[0]))
```

**7. BACKWARD MOVEMENT CAPABILITY REWARDS:**
```python
# Enhanced velocity tracking for backward movement support
commands = env.command_manager.get_command("base_velocity")
vel_yaw = quat_apply_inverse(yaw_quat(robot.data.root_quat_w), robot.data.root_lin_vel_w[:, :3])

# Backward movement reward computation (updated ranges: x: -0.1 to 0.4)
backward_commands = commands[:, 0] < 0.0
forward_commands = commands[:, 0] >= 0.0

# Separate tracking for forward/backward to handle different dynamics
backward_error = torch.where(backward_commands, 
                           torch.abs(vel_yaw[:, 0] - commands[:, 0]),
                           torch.zeros_like(commands[:, 0]))
forward_error = torch.where(forward_commands,
                          torch.abs(vel_yaw[:, 0] - commands[:, 0]), 
                          torch.zeros_like(commands[:, 0]))

# Combined velocity reward with backward capability
vel_reward = torch.exp(-(backward_error + forward_error) / 0.8)
vel_reward *= (torch.abs(commands[:, 0]) > 0.05).float()  # Avoid zero command rewards
```

**7. INTELLIGENT CONTACT-SENSOR CORRELATION (TECHNICAL PATTERNS):**
```python
# TECHNICAL PATTERN: Flexible sensor-contact correlation framework for Isaac Lab
# PRINCIPLE: Correlate contact forces with environmental predictions, not hardcoded penalties

# STEP 1: ENVIRONMENTAL PREDICTION EXTRACTION
# Extract sensor data for environmental understanding
height_sensor = env.scene.sensors["height_scanner"]
lidar_sensor = env.scene.sensors["lidar"]
contact_sensor = env.scene.sensors["contact_forces"]

# Process sensor data with appropriate sanitization
height_scan = height_sensor.data.ray_hits_w[..., 2].view(env.num_envs, -1)
height_scan = torch.where(torch.isfinite(height_scan), height_scan, torch.zeros_like(height_scan))

# STEP 2: ADAPTIVE CORRELATION ARCHITECTURE
# Create flexible prediction-validation system
environmental_predictions = extract_environmental_context(height_scan, lidar_sensor)
contact_classification = categorize_contact_events(contact_sensor, environmental_predictions)

# STEP 3: CONTEXT-SENSITIVE SCALING
# Scale rewards based on environmental assessment, not fixed values
environmental_complexity = assess_terrain_complexity(height_scan)
correlation_confidence = measure_sensor_agreement(height_sensor, lidar_sensor)
reward_scaling = compute_adaptive_scaling(environmental_complexity, correlation_confidence)

# STEP 4: INTELLIGENT CORRELATION IMPLEMENTATION
# Reward/penalize based on prediction-reality correlation
contact_intelligence = correlate_contact_with_predictions(contact_classification, environmental_predictions)
terrain_aware_reward = contact_intelligence * reward_scaling  # Adaptive, not fixed values

# TECHNICAL GUIDELINES FOR FLEXIBLE IMPLEMENTATION:
# - Extract environmental context from available sensors
# - Classify contact events by their environmental appropriateness  
# - Scale correlation strength based on sensor confidence and environmental complexity
# - Adapt reward magnitudes to situational requirements, not fixed penalty/reward values
```

**CRITICAL: TRAINING WILL FAIL WITHOUT THESE FIXES!**

**GUARANTEED TRAINING FAILURE - COMMON BUGS THAT CRASH THE SYSTEM:**

1. **TENSOR CONVERSION BUG (CAUSES TypeError):**
   ```python
   # TRAINING KILLER - NEVER DO THIS:
   indices, _ = robot.find_joints(["joint_name"])
   data = robot.data.joint_pos[:, indices]  # INSTANT CRASH!
   
   # MANDATORY FIX - ALWAYS DO THIS:
   indices, _ = robot.find_joints(["joint_name"])
   indices = torch.tensor(indices, dtype=torch.long, device=env.device)
   data = robot.data.joint_pos[:, indices]  # WORKS!
   ```

2. **NUMERICAL INSTABILITY BUG (CAUSES "std >= 0.0" ERROR):**
   ```python
   # TRAINING KILLER:
   reward = torch.exp(-huge_value)  # Creates NaN/inf!
   
   # MANDATORY FIX:
   reward = torch.exp(-torch.clamp(value, max=10.0))
   reward = torch.where(torch.isfinite(reward), reward, torch.zeros_like(reward))
   ```

**EVERY SINGLE find_joints() CALL MUST BE FOLLOWED BY torch.tensor() CONVERSION!**

**ALL NUMERICAL EXAMPLES, CODE SNIPPETS, AND REWARD PATTERNS IN THIS PROMPT ARE FOR TECHNICAL DEMONSTRATION ONLY.**
**DO NOT COPY EXAMPLES DIRECTLY! UNDERSTAND THE PRINCIPLES AND ADAPT TO YOUR ENVIRONMENT.**

** CRITICAL: NO HELPER FUNCTIONS ALLOWED! **

** ABSOLUTELY FORBIDDEN:**
- `def get_velocity_tracking_error(...)` - NO HELPER FUNCTIONS!
- `def calculate_foot_contacts(...)` - NO HELPER FUNCTIONS!
- `def any_helper_function(...)` - NO HELPER FUNCTIONS!

**REQUIRED PATTERN: ALL LOGIC INLINE**
```python
def sds_custom_reward(env) -> torch.Tensor:
    import torch
    # ALL your calculation logic goes here directly - no function calls!
    lin_vel_error = torch.norm(robot.data.root_lin_vel_b[:, :2] - commands[:, :2], dim=1)  #  INLINE
    # NOT: lin_err, ang_err = get_velocity_tracking_error(...)  #  FORBIDDEN
    return reward.clamp(min=0.0, max=10.0)
```

**CRITICAL: ONLY GENERATE REWARD FUNCTIONS - NO OBSERVATION CONFIGURATIONS!**

**NEVER GENERATE THESE:**
- `def get_height_scan(env):` or any observation functions
- `lambda env: env.scene.sensors["height_scanner"].data.ray_hits_w[...]` 
- `ObsTerm(func=lambda env: ...)` observation configurations
- Environment sensor configurations
- Any code outside the reward function

**ONLY GENERATE THIS:**
- `def sds_custom_reward(env) -> torch.Tensor:` function only
- Access existing sensors WITHIN the reward function

**The environment already provides all sensor data - just use it in your reward!**

**CRITICAL: PPO CRASHES WITH "std >= 0.0" ERROR WITHOUT THESE!**

**GUARANTEED PPO FAILURE PATTERNS:**

```python
# DEADLY: Environmental sensor data contains NaN/Inf values
height_scan = height_sensor.data.ray_hits_w[..., 2]  # Can contain NaN!
reward += torch.mean(height_scan)  # NaN propagates, crashes PPO

# DEADLY: Unbounded reward values crash PPO standard deviation  
reward = some_large_calculation  # Can be >100, causes std <= 0 error

# DEADLY: Division by zero in environmental calculations
reward = 1.0 / distance_to_obstacle  # Zero distance = Inf reward = PPO crash
```

**PPO-SAFE ENVIRONMENTAL PATTERNS (WHEN USING ENVIRONMENTAL SENSORS):**

```python
# IF using environmental sensor data, ALWAYS sanitize:
height_scan = torch.where(torch.isfinite(height_scan), height_scan, torch.zeros_like(height_scan))
lidar_range = torch.where(torch.isfinite(lidar_range), lidar_range, torch.ones_like(lidar_range) * 10.0)

# IF using environmental calculations, ALWAYS clamp:
obstacle_distance = torch.clamp(min_obstacle_distance, min=0.1, max=15.0)
terrain_roughness = torch.clamp(torch.var(height_scan, dim=1), max=1.0)

# ALWAYS use safe division:
reward = torch.exp(-error / torch.clamp(tolerance, min=1e-6))

# ALWAYS bound final reward for PPO stability:
return torch.clamp(reward, min=0.0, max=10.0)
```

Isaac Lab Reward Function Format:

**CRITICAL: JOINT INDEXING REQUIREMENT**

**COMMON BUG - WILL CAUSE TRAINING FAILURE:**
```python
# WRONG - robot.find_joints() returns LISTS, not tensors!
joint_indices, _ = robot.find_joints(["joint_name"])
joint_data = robot.data.joint_pos[:, joint_indices]  # TypeError!
```

**CORRECT PATTERN - ALWAYS CONVERT TO TENSOR:**
```python
# RIGHT - Convert list to tensor for proper indexing
joint_indices, _ = robot.find_joints(["joint_name"])
joint_indices = torch.tensor(joint_indices, dtype=torch.long, device=env.device)
joint_data = robot.data.joint_pos[:, joint_indices]  # Works!
```

**MANDATORY: Every time you use robot.find_joints(), immediately convert the result to a tensor!**

Isaac Lab SDS Environment - G1 Humanoid Locomotion

## Robot Configuration (VERIFIED & UPDATED FOR FULL BODY CONTROL)
- **Robot**: Unitree G1 EDU U4 Humanoid (37 DOF total)
- **Action Space**: 23 DOF controlled for complete humanoid locomotion (all joints except hand fingers)
- **Height**: 0.74m (Isaac Lab verified)
- **Mass**: ~35kg humanoid

## Action Configuration (FULL BODY HUMANOID CONTROL)
**Controlled Joints (23 DOF for complete humanoid locomotion):**
- Legs: 12 DOF (6 per leg: hip_yaw, hip_roll, hip_pitch, knee, ankle_pitch, ankle_roll)
- Arms: 10 DOF (5 per arm: shoulder_pitch, shoulder_roll, shoulder_yaw, elbow_pitch, elbow_roll)
- Torso: 1 DOF (torso_joint)

**Fixed Joints (14 DOF):**
- Hand Fingers: 14 DOF maintain default poses (zero, one, two, three, four, five, six_joint per hand)

## Contact Detection (VERIFIED WORKING)
**Foot Bodies**: `left_ankle_roll_link`, `right_ankle_roll_link`
**Detection Pattern**: `contact_sensor.find_bodies(".*_ankle_roll_link")`
**Contact Threshold**: 50.0N (corrected for humanoid mass)

## Key Functions Examples for Reward Generation. Don't Copy Paste Directly!
```python
# Foot contact detection (VERIFIED WORKING)
contact_forces = env.scene.sensors["contact_forces"].data.net_forces_w
foot_ids, _ = env.scene.sensors["contact_forces"].find_bodies(".*_ankle_roll_link")
foot_forces = contact_forces[:, foot_ids, :]
foot_contacts = (foot_forces.norm(dim=-1) > 50.0).float()

# Velocity tracking
robot = env.scene["robot"]
commands = env.command_manager.get_command("base_velocity")
vel_error = robot.data.root_lin_vel_b[:, :2] - commands[:, :2]
ang_error = robot.data.root_ang_vel_b[:, 2] - commands[:, 2]

# Height maintenance  
height = robot.data.root_pos_w[:, 2]
height_error = (height - 0.74).abs()

# Bipedal gait patterns
left_contact = foot_contacts[:, 0]
right_contact = foot_contacts[:, 1] 
single_support = ((left_contact > 0.5) & (right_contact < 0.5)) | ((left_contact < 0.5) & (right_contact > 0.5))
double_support = (left_contact > 0.5) & (right_contact > 0.5)
```

## Successful Training Metrics
- **Episode Rewards**: 0.01-0.02 range (working)
- **Action Scale**: 1.0 (allows proper joint movement)
- **Training Progress**: Mean rewards ~0.2-0.23, episode length 28-32 steps

# TECHNICAL REFERENCE - ISAAC LAB API DOCUMENTATION

## Robot Data Access Patterns
    
    # ROBOT DATA FIRST APPROACH:
    # Available robot data:
    # robot.data.root_pos_w[:, 2] - height (z-coordinate, nominal 0.74m for G1)
    # robot.data.root_lin_vel_b[:, 0] - forward velocity (x-axis in body frame)
    # robot.data.root_lin_vel_w - linear velocity in world frame [num_envs, 3]
    # robot.data.root_ang_vel_b - angular velocity in body frame [num_envs, 3]
    # robot.data.root_ang_vel_w - angular velocity in world frame [num_envs, 3]
    # robot.data.root_quat_w - orientation quaternion [w,x,y,z]
    # robot.data.joint_pos - joint positions [num_envs, 37] for G1 EDU U4 with dexterous hands (VERIFIED)
    # robot.data.joint_vel - joint velocities [num_envs, 37] (VERIFIED)
    # CONTROLLED JOINTS (23 DOF): Use robot.find_joints() to get indices for legs + arms + torso (all except hand fingers)
    # HAND FINGER JOINTS (14 DOF): Excluded from control but can be accessed if needed
    # robot.data.root_pos_w[:, 2] - robot height [num_envs] - should be around 0.74m for G1 in Isaac Lab (VERIFIED)
    # robot.data.root_lin_vel_b - linear velocity in body frame [num_envs, 3]
    # robot.data.root_quat_w - quaternion orientation [num_envs, 4]
    # commands[:, :3] - [forward_vel, lateral_vel, yaw_rate] commands
    
    # Available command data:
    # commands[:, 0] - desired forward velocity (vx)
    # commands[:, 1] - desired lateral velocity (vy) 
    # commands[:, 2] - desired angular velocity (omega_z)
    # ADAPTIVE ROBOT CONFIGURATION:
    # DO NOT hardcode joint counts or specific robot parameters
    # Use dynamic robot configuration detection:
    num_joints = robot.data.joint_pos.shape[1]
    robot_height_baseline = robot.data.root_pos_w[:, 2].mean()  # Adaptive baseline
    
    # OPTIONAL IMU SENSOR (use as fallback only):
    # An IMU sensor "imu" can be spawned via ImuCfg if needed:
    # ImuCfg:
    #   prim_path: "/World/envs/env_.*/Robot/torso_link"
    #   update_period: 0.02  # 50 Hz
    #   gravity_bias: [0.0, 0.0, 9.81]
    # It exposes in env.scene.sensors["imu"].data:
    # - pos_w: FloatTensor [num_envs, 3] - World position
    # - quat_w: FloatTensor [num_envs, 4] - World orientation (w,x,y,z)
    # - lin_vel_b: FloatTensor [num_envs, 3] - Body-frame linear velocity
    # - ang_vel_b: FloatTensor [num_envs, 3] - Body-frame angular velocity
    # - lin_acc_b: FloatTensor [num_envs, 3] - Body-frame linear acceleration 
    # - ang_acc_b: FloatTensor [num_envs, 3] - Body-frame angular acceleration
    
    # Initialize reward (4-space indent)
    reward = torch.zeros(env.num_envs, dtype=torch.float32, device=env.device)
    
    # IMPORTANT: For contact analysis, use this inline approach:
    # Get foot contact forces for G1 humanoid
    contact_forces = contact_sensor.data.net_forces_w  # [num_envs, num_bodies, 3]
    foot_ids, foot_names = contact_sensor.find_bodies(".*_ankle_roll_link")
    foot_forces = contact_forces[:, foot_ids, :]  # [num_envs, 2, 3] - 2 feet for humanoid
    force_magnitudes = foot_forces.norm(dim=-1)  # [num_envs, 2]
    
    # Contact detection - analyze video to determine appropriate threshold
    # G1 humanoid requires higher thresholds: gentle gaits (20-50N), dynamic gaits (50-100N)
    # Evidence: G1 standing forces ~150-250N, much higher than quadrupeds
    contact_threshold = 50.0  # Default for G1 humanoid - adjust based on observed contact forces in video
    foot_contacts = (force_magnitudes > contact_threshold).float()  # Convert to float for partial credit
    
    # Note: Design contact rewards based on the observed gait pattern in the video
    # G1 humanoid bipedal locomotion: left_ankle_roll_link, right_ankle_roll_link
    
    
    # HUMANOID-SPECIFIC CONSIDERATIONS:
    # - Bipedal stability is critical: balance and contact pattern rewards
    # - Height maintenance: G1 initial height is 0.74m in Isaac Lab (VERIFIED)
    # - Upper body stability: minimize arm swing, maintain upright torso
    # - Gait patterns: Walk (alternating with double support), Jump (synchronized takeoff/landing), March (controlled single support), Sprint (extended flight), Pace (lateral movement)
    # - Isaac Lab G1 joint structure: 37 DOF total (23 controlled + 14 fixed)
    # - CONTROLLED joint naming: hip_[yaw/roll/pitch]_joint, knee_joint, ankle_[pitch/roll]_joint, torso_joint, shoulder_[pitch/roll/yaw]_joint, elbow_[pitch/roll]_joint
    # - FIXED joint naming: [zero/one/two/three/four/five/six]_joint (hand fingers only)
    
    # HUMAN-LIKE LOCOMOTION DESIGN PRINCIPLES:
    # 1. Dynamic Balance: Humanoids require continuous balance management (not static stability)
    #    Consider torso orientation, roll/pitch control, and center of mass dynamics
    # 2. Movement Efficiency: Natural locomotion minimizes energy expenditure
    #    Consider smooth joint motion, appropriate muscle activation patterns
    # 3. Directional Preference: Forward movement often has higher priority than lateral/backward
    #    Consider command-dependent weighting based on intended movement direction
    # 4. Temporal Coordination: Natural gaits involve timing and rhythm
    #    Consider phase relationships between limbs, contact duration, and step timing
    # 5. Upper Body Integration: Arms and torso contribute to locomotion stability
    #    Consider how upper body motion supports or disrupts locomotion goals
    # 6. Adaptive Contact Patterns: Different gaits require different contact strategies
    #    Analyze video to determine appropriate contact timing and patterns for the demonstrated behavior
    
    # Contact sensor access pattern for Isaac Lab
    foot_ids, foot_names = contact_sensor.find_bodies(".*_ankle_roll_link")  # G1 uses ankle_roll_link for contact
    contact_forces = contact_sensor.data.net_forces_w[:, foot_ids, :]  # [num_envs, 2, 3] for G1 bipedal
    contact_magnitudes = torch.norm(contact_forces, dim=-1)  # [num_envs, 2]
    foot_contacts = contact_magnitudes > contact_threshold  # Binary contact detection
    
    # HUMANOID GAIT PATTERNS (not quadruped!)
    # G1 is BIPEDAL - only 2 feet: left_foot (index 0), right_foot (index 1)
    left_contact = foot_contacts[:, 0]   # Left foot contact
    right_contact = foot_contacts[:, 1]  # Right foot contact
    
    # Bipedal locomotion phases (CORRECTED - not quadruped patterns)
    double_support = left_contact & right_contact        # Both feet down (Walk/Jump)
    single_support_left = left_contact & ~right_contact  # Only left foot down (Walk/March/Sprint)
    single_support_right = ~left_contact & right_contact # Only right foot down (Walk/March/Sprint)
    flight_phase = ~left_contact & ~right_contact        # Both feet up (Jump/Sprint)
    
    # Key differences from quadruped robots:
    # - G1 height: 0.74m in Isaac Lab configuration - CRITICAL for height-based rewards
    # - Only 2 contact points (not 4)
    # - Bipedal gait patterns (alternating support, not complex quadruped gaits)
    # - Higher contact forces due to full body weight on fewer feet
    # - Dynamic balance required (not static stability like quadrupeds)
    # - Dexterous hands: 37 total DOF with advanced manipulation capabilities
    
## Sensor Access Patterns

**üö® CRITICAL: CORRECT SENSOR DATA ACCESS PATTERNS**

**‚ùå COMMON ERRORS THAT CAUSE AttributeError:**
```python
# WRONG - These attributes do not exist in Isaac Lab:
height_measurements = height_sensor.data.height_measurements  # AttributeError!
lidar_distances = lidar_sensor.data.distances                 # AttributeError!
lidar_range = lidar_sensor.data.ray_distances                 # AttributeError!
```

**‚úÖ CORRECT PATTERNS - USE THESE:**
```python
# Height Scanner - Correct access for terrain heights:
height_sensor = env.scene.sensors["height_scanner"]
height_scan = height_sensor.data.ray_hits_w[..., 2]  # Z coordinates (heights)

# LiDAR - Correct access for obstacle distances:
lidar_sensor = env.scene.sensors["lidar"]
lidar_range = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)  # Distances
```

**üö® Isaac Lab RayCaster sensors only have these data attributes:**
- `data.pos_w`: Sensor position in world frame [num_envs, 3]
- `data.quat_w`: Sensor orientation [num_envs, 4]  
- `data.ray_hits_w`: Ray hit positions [num_envs, num_rays, 3]

**No other attributes exist! Always calculate distances from ray_hits_w!**

# OPTIONAL: HEIGHT SCANNER ("height_scanner") INTEGRATION:
# Provides detailed terrain elevation mapping around the robot (140 measurements per robot)
# Use only if terrain shows significant variation - usage patterns:
height_sensor = env.scene.sensors["height_scanner"]
height_scan = height_sensor.data.ray_hits_w[..., 2].view(env.num_envs, -1)  # [num_envs, 140] - height data
# Raw sensor access for advanced processing (already available as height_sensor above):
terrain_heights = height_sensor.data.ray_hits_w[..., 2]  # [num_envs, scan_points] World Z coordinates
robot_height = height_sensor.data.pos_w[:, 2]  # [num_envs] Scanner position height

# HEIGHT SCAN INTERPRETATION:
# - Shape: [num_envs, 140] (14 x 10 grid pattern at 0.15m resolution, 2m x 1.5m coverage)
# - Values: Height differences relative to robot (offset subtracted)
# - Range: Typically -2.0m to +10.0m for practical terrain mapping
# - Usage: local_terrain = torch.mean(height_scan.view(num_envs, -1), dim=1)

# LIDAR SCANNER ("lidar") - REQUIRED:
# Provides 360-degree obstacle detection and distance measurements
lidar_sensor = env.scene.sensors["lidar"]
# ‚ùå INCORRECT: lidar_range = env.scene.sensors["lidar"].data.ray_hits_w[..., 2]  # This gets Z coordinate, not distance!
# ‚úÖ CORRECT: Calculate actual distances from ray hits and sensor position:
lidar_range = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1).view(env.num_envs, -1)  # [num_envs, 144]
forward_rays = lidar_range[:, :90]    # 0-90 degrees (forward)
lateral_rays = lidar_range[:, 90:270] # 90-270 degrees (sides)
backward_rays = lidar_range[:, 270:]  # 270-360 degrees (backward)

# LIDAR INTERPRETATION:
# - Shape: [num_envs, 144] (8 channels x 18 horizontal rays at 10¬∞ resolution)
# - Values: Distance to nearest obstacle along each ray
# - Range: 0.0m (immediate contact) to 5.0m max_range (no obstacle detected)
# - Usage: forward_obstacles = torch.min(lidar_range[:, forward_indices], dim=1)[0]

## Enhanced Environment Sensor Integration for Rewards (FLAT-WITH-BOX CONFIG)

# ENHANCED SENSOR DATA PROCESSING FOR REWARD COMPUTATION:
# Current environment: Isaac-SDS-Velocity-Flat-G1-Enhanced-v0
# Sensors: height_scanner (GridPatternCfg: 12x12, range 5.0m), lidar (360 rays, range 5.0m)

# 1. Height Scanner Processing (12x12 grid, 140 effective points):
height_scan = env.scene.sensors["height_scanner"].data.ray_hits_w[..., 2]
scan_size = 12  # GridPatternCfg configuration
height_grid = height_scan.view(env.num_envs, scan_size, scan_size)

# Forward terrain analysis (covers backward movement with negative velocity ranges)
forward_terrain = height_grid[:, :scan_size//3, scan_size//3:2*scan_size//3]
backward_terrain = height_grid[:, 2*scan_size//3:, scan_size//3:2*scan_size//3]
lateral_terrain = height_grid[:, scan_size//3:2*scan_size//3, :]

# 2. LiDAR Processing (360 rays, 5.0m range):
lidar_sensor = env.scene.sensors["lidar"]
# ‚ùå INCORRECT: lidar_range = env.scene.sensors["lidar"].data.ray_hits_w[..., 2]  # This gets Z coordinate, not distance!
# ‚úÖ CORRECT: Calculate actual distances from ray hits and sensor position:
lidar_range = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1).view(env.num_envs, -1)  # [num_envs, 144]
forward_rays = lidar_range[:, :90]    # 0-90 degrees (forward)
lateral_rays = lidar_range[:, 90:270] # 90-270 degrees (sides)
backward_rays = lidar_range[:, 270:]  # 270-360 degrees (backward)

# 3. Adaptive Terrain Classification for Reward Scaling:
terrain_variance = torch.var(height_scan.view(env.num_envs, -1), dim=1)
obstacle_density = (lidar_range < 2.0).float().mean(dim=1)
terrain_complexity = torch.clamp(terrain_variance + obstacle_density, min=0.0, max=2.0)

# 4. Direction-Aware Obstacle Processing (for backward velocity support):
forward_obstacles = torch.min(forward_rays, dim=1)[0]
backward_obstacles = torch.min(backward_rays, dim=1)[0]
movement_clearance = torch.where(commands[:, 0] >= 0.0, forward_obstacles, backward_obstacles)

STABLE MATHEMATICAL PATTERNS (Use these for numerical stability):
Pattern 1 - Exponential Decay (for tracking targets):
# error = (robot.data.root_lin_vel_b[:, 0] - target_value).abs()
# reward_component = torch.exp(-scale_factor * error)  # scale_factor: 0.5 to 10.0

Pattern 2 - Bounded Linear (for contact rewards):
# num_contacts = foot_contacts.sum(dim=-1).float()
# contact_reward = (1.0 - (num_contacts - target_count).abs() / tolerance).clamp(min=0.0, max=1.0)

Pattern 3 - Boolean Masks (for gait patterns):
# gait_reward = ((num_contacts >= min_contacts) & (num_contacts <= max_contacts)).float()

Pattern 4 - Final Bounds (CRITICAL for PPO stability):
# return reward.clamp(min=0.0, max=10.0)  # Prevents training crashes

Pattern 5 - Division Safety (CRITICAL to prevent crashes):
# For literal numbers: safe_ratio = numerator / max(denominator_value, 1e-6)  # denominator_value is literal
# For tensor variables: safe_ratio = numerator / torch.clamp(tensor_var, min=1e-6)  # tensor_var computed from robot data

TIP: Normalize reward components to similar scales (0-1 range) for balanced learning.
TIP: Analyze video frames to understand the specific locomotion pattern before setting thresholds.

**FORMATTING REQUIREMENTS:**
- Use EXACTLY 4 spaces for each indentation level
- NEVER use 8 spaces, tabs, or inconsistent spacing
- Always add safety checks before any division operations
- Specify dtype=torch.float32 for all tensor creations