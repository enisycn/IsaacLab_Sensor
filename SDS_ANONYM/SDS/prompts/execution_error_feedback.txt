üåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåü
üåüüåüüåü ENVIRONMENT-AWARE LOCOMOTION REWARD DESIGN GUIDANCE üåüüåüüåü
üåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåü

**CRITICAL PROJECT UNDERSTANDING:**

These guidance explanations are designed for creating **environment-aware locomotion rewards** that enable **intelligent sensor-driven behavior adaptation** for humanoid robots.

**üéØ PRIMARY OBJECTIVE:** 
Create reward functions that make **measurable positive impact when sensors are used** compared to **without sensor usage**.

**üî¨ RESEARCH PURPOSE:**
Enable comparison studies demonstrating:
- **WITHOUT SENSORS:** Basic locomotion behavior 
- **WITH SENSORS:** Adaptive, context-aware behavior that responds to environmental challenges

**üß† DESIGN METHODOLOGY:**
1. **ANALYZE:** Understand the environment (gaps, obstacles, terrain complexity)
2. **THINK:** Determine which sensors are needed and how they should influence behavior
3. **DECIDE:** Implement context-aware behavioral switching (NOT simultaneous conflicting rewards)
4. **IMPACT:** Ensure sensors create observable behavioral differences for scientific comparison

**üìä SUCCESS CRITERIA:**
‚úÖ Robot behaves measurably different with sensors vs. without sensors
‚úÖ Sensor-enabled robot adapts to environmental challenges more effectively
‚úÖ Clear behavioral switching based on environmental context
‚úÖ No conflicting simultaneous behaviors (e.g., walking + jumping simultaneously)

**‚ö†Ô∏è FAILURE INDICATORS:**
‚ùå Robot behaves identically with/without sensors
‚ùå Sensors provide only minor bonuses without changing core behavior
‚ùå Conflicting reward objectives that confuse the policy

---

üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®ÔøΩÔøΩüö®üö®üö®üö®üö®üö®üö®üö®üö®
üö®üö®üö® #1 TRAINING CRASH: SENSOR ATTRIBUTEERROR - READ THIS FIRST üö®üö®üö®
üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®

**MOST COMMON ERROR PATTERN:**

**ATTRIBUTEERROR: 'RayCasterData' object has no attribute 'distances'**
- CAUSE: Using non-existent sensor attributes (.distances, .height_measurements)
- CRASH LINE EXAMPLES:
```python
lidar_range = lidar_sensor.data.distances  # AttributeError!
height_data = height_sensor.data.height_measurements  # AttributeError!
dist = lidar_sensor.data.distances  # INSTANT CRASH!
```
‚úÖ CORRECT SENSOR ACCESS PATTERN (Isaac Lab standard for reward functions):
```python
# Isaac Lab standard raw sensor access for reward functions
height_sensor = env.scene.sensors["height_scanner"]
lidar_sensor = env.scene.sensors["lidar"]

height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5
lidar_distances = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)
```

‚ùå WRONG PATTERNS THAT CAUSE CRASHES:
```python
height_scan = height_sensor.data.distances  # AttributeError: no such attribute
lidar_range = lidar_sensor.data.range_measurements  # AttributeError: no such attribute
```

üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®

Isaac Lab reward functions can encounter various runtime errors. Here's how to diagnose and fix common issues:

**BEHAVIORAL ERRORS - UNNATURAL ROBOT MOVEMENT:**

**HEIGHT HARDCODING ERROR (robots stand still on stairs/platforms):**
- SYMPTOM: Robots learned to stand still on elevated terrain or go downstairs
- CAUSE: Using inappropriate height tracking method for the terrain characteristics
- FIX: Choose height tracking method based on environment analysis:

**ABSOLUTE HEIGHT - For consistent body clearance:**
```python
# Use when: Flat terrain OR gap crossing (large stepping) OR consistent platform heights
height_err = torch.abs(robot.data.root_pos_w[:, 2] - 0.74)  # Target height above world origin
height_reward = torch.exp(-height_err / 0.3)
```

**TERRAIN-RELATIVE HEIGHT - For adaptive surface following:**
```python
# Use when: Variable terrain heights OR climbing stairs OR navigating slopes
height_sensor = env.scene.sensors["height_scanner"]
height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5
avg_terrain_height = height_measurements.mean(dim=-1)  # Average terrain height in meters
# Terrain-relative height tracking with physical measurements
# Robot height above terrain surface (corrected calculation)
robot_terrain_relative_height = robot.data.root_pos_w[:, 2] - avg_terrain_height
height_err = torch.abs(robot_terrain_relative_height - 0.74)  # Maintain 0.74m above terrain
```

**DECISION GUIDE - Let environment analysis guide your choice:**
- Large gaps requiring stepping ‚Üí Absolute height maintains clearance
- Variable surface heights ‚Üí Terrain-relative adapts to surface
- Consistent platform levels ‚Üí Absolute height for smooth transitions
- Climbing/descending behavior needed ‚Üí Terrain-relative for adaptation

**ARM CONTROL ISSUES (weird arm behavior):**
- SYMPTOM: Arms moving unnaturally, crossing body, or excessive movement
- CAUSE: Complex arm control or forcing unnatural positions
- FIX: Use simplified approach keeping arms near defaults:
```python
# ‚úÖ SIMPLE & EFFECTIVE: Keep arms near defaults (0.35)
robot = env.scene["robot"]
shoulder_joints = ["left_shoulder_pitch_joint", "right_shoulder_pitch_joint"]
shoulder_indices, _ = robot.find_joints(shoulder_joints)
shoulder_indices = torch.tensor(shoulder_indices, dtype=torch.long, device=env.device)
shoulder_angles = robot.data.joint_pos[:, shoulder_indices]  # [N, 2]

# Reward staying near default positions (shoulder_pitch = 0.35)
target_shoulder_pos = 0.35  # Asset default position
arm_deviation = torch.mean(torch.abs(shoulder_angles - target_shoulder_pos), dim=1)
arm_reward = torch.exp(-arm_deviation / 0.2) * 0.1  # Simple and stable
```

**TECHNICAL ERROR PATTERNS:**

TENSOR BROADCASTING ERRORS (e.g., "output with shape [N] doesn't match broadcast shape [N,N]"):
- Caused by implicit broadcasting between tensors of incompatible shapes
- FIX: Use explicit .expand() or .unsqueeze() to match dimensions exactly
- Example: desired_value.unsqueeze(1).expand(-1, num_feet) instead of desired_value.unsqueeze(1)

NEGATIVE STANDARD DEVIATION ERRORS ("normal expects all elements of std >= 0.0"):
- Caused by negative values propagating to actor network
- FIX: Apply torch.clamp(reward_component, min=0.0) to ALL reward terms
- Check contact sensor times which can be negative during initialization

RUNTIME TENSOR SHAPE ERRORS:
- Caused by sensor data dimensions not matching expected robot batch size
- FIX: Validate tensor shapes before operations, use proper indexing for foot_ids and joint_ids

DIVISION BY ZERO ERRORS:
- Caused by dividing by potentially zero values
- FIX: Use torch.clamp(denominator, min=1e-6) before division operations

ENVIRONMENTAL SENSING DECISION ERRORS:
- Caused by dismissing sensor data in favor of visual analysis  
- SYMPTOMS: "sensors detect gaps/obstacles but visual shows flat terrain"
- FIX: Always prioritize quantitative sensor measurements over visual interpretation
- RULE: Use environment analysis classification to determine if environmental components are needed

COMMON ERRORS AND FIXES:

SENSOR ACCESS ERRORS:
- SYMPTOMS: AttributeError when accessing .distances or .height_measurements
- CAUSE: Using non-existent sensor attributes
- FIX: Use Isaac Lab standard raw sensor access:
  height_sensor = env.scene.sensors["height_scanner"]
  height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5

TENSOR OPERATION ERRORS:
- SYMPTOMS: RuntimeError during math operations (division by zero, NaN values)
- CAUSE: Unstable mathematical operations
- FIX: Use torch.clamp(denominator, min=1e-6) before division operations

ENVIRONMENTAL SENSING DECISION ERRORS:
- Caused by dismissing sensor data in favor of visual analysis  
- SYMPTOMS: "sensors detect gaps/obstacles but visual shows flat terrain"
- FIX: Always prioritize quantitative sensor measurements over visual interpretation
- RULE: Use environment analysis classification to determine if environmental components are needed

SENSOR BEHAVIORAL ADAPTATION ERRORS:
- SYMPTOMS: Robot behaves identically with/without sensors (no measurable difference)
- CAUSE: Adding minimal sensor bonuses instead of behavioral switching
- FIX: Use context-aware behavioral adaptation:
```python
# ‚ùå WRONG: Minimal sensor impact
total = foundation + sensor_bonus * 0.1  # Robot still walks the same way

# ‚úÖ CORRECT: Behavioral switching
if gap_detected:
    behavior = foundation + gap_crossing_adaptation()  # Different gait
elif obstacle_detected:
    behavior = foundation + obstacle_avoidance_adaptation()  # Different navigation
else:
    behavior = foundation_only()  # Normal walking
```
- RULE: Sensors must create observable behavioral differences for comparison studies

Apply these fixes systematically to prevent future runtime crashes.