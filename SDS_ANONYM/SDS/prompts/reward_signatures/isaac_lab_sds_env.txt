**PROVEN ISAAC LAB LOCOMOTION PATTERNS - PRIORITIZE THESE!**

These patterns are battle-tested and known to work for humanoid locomotion. Use them as a foundation:

**1. YAW-ALIGNED VELOCITY TRACKING (SUPERIOR TO BODY FRAME):**
```python
# PROVEN: Much better than basic body frame tracking
from isaaclab.utils.math import quat_apply_inverse, yaw_quat

commands = env.command_manager.get_command("base_velocity")
command_magnitude = torch.norm(commands[:, :2], dim=1)

# Transform to yaw-aligned frame (removes pitch/roll interference)
vel_yaw = quat_apply_inverse(yaw_quat(robot.data.root_quat_w), robot.data.root_lin_vel_w[:, :3])
lin_vel_error = torch.sum(torch.square(commands[:, :2] - vel_yaw[:, :2]), dim=1)
vel_reward = torch.exp(-lin_vel_error / (1.0**2))

# CRITICAL: No reward for zero commands (prevents stationary exploitation)
vel_reward *= (command_magnitude > 0.1).float()
```

**2. PROPER BIPEDAL GAIT PATTERNS:**
```python
# PROVEN: Rewards single stance phases (proper walking pattern)
foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)

air_time = contact_sensor.data.current_air_time[:, foot_ids]
contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
in_contact = contact_time > 0.0

# Reward single stance (one foot contact at a time)
single_stance = torch.sum(in_contact.int(), dim=1) == 1
in_mode_time = torch.where(in_contact, contact_time, air_time)
gait_reward = torch.min(torch.where(single_stance.unsqueeze(-1), in_mode_time, 0.0), dim=1)[0]
gait_reward = torch.clamp(gait_reward, max=0.5) * (command_magnitude > 0.1).float()
```

**3. CONTACT-AWARE FOOT SLIDING PENALTY:**
```python
# PROVEN: Only penalize sliding when foot is actually in contact
forces = contact_sensor.data.net_forces_w_history[:, :, foot_ids, :]
contacts = forces.norm(dim=-1).max(dim=1)[0] > 1.0
body_vel = robot.data.body_lin_vel_w[:, foot_ids, :2]
slide_penalty = torch.sum(body_vel.norm(dim=-1) * contacts, dim=1)
```

**CRITICAL ISAAC LAB INSIGHTS:**
- **Command scaling**: Never reward when commands are near zero
- **Yaw alignment**: Removes pitch/roll interference from velocity tracking  
- **Single stance**: Encourages proper alternating foot pattern
- **Contact awareness**: Only apply penalties when actually relevant (foot in contact)
- **Capped rewards**: Air time and other metrics should have reasonable upper bounds

## ISAAC LAB REWARD COMPUTATION PATTERNS

**Focus: Technical implementation for reward functions, not biomechanical theory**

** ENHANCED ENVIRONMENT REWARD PATTERNS (NEW - FOR ADVANCED LOCOMOTION):**

**4. VELOCITY-OBSTACLE CONFLICT RESOLUTION MATH:**
```python
# Enhanced sensor-based dynamic target modification for reward computation
height_scan = env.scene.sensors["height_scanner"].data.ray_hits_w[..., 2]
lidar_range = env.scene.sensors["lidar"].data.ray_hits_w[..., 2]
commands = env.command_manager.get_command("base_velocity")

# Adaptive velocity scaling based on forward terrain
scan_size = int(torch.sqrt(torch.tensor(height_scan.shape[1])))
height_grid = height_scan.view(env.num_envs, scan_size, scan_size)
forward_terrain = height_grid[:, :scan_size//3, scan_size//3:2*scan_size//3]
terrain_clearance = torch.min(forward_terrain.view(env.num_envs, -1), dim=1)[0]

# Dynamic velocity target modification for rewards
obstacle_factor = torch.clamp((terrain_clearance + 0.2) / 0.4, min=0.1, max=1.0)
adaptive_commands = commands.clone()
adaptive_commands[:, 0] *= obstacle_factor  # Scale forward velocity by terrain clearance

# Compute velocity reward with adaptive targets
vel_error = torch.sum((robot.data.root_lin_vel_b[:, :2] - adaptive_commands[:, :2])**2, dim=1)
velocity_reward = torch.exp(-vel_error / 1.0) * (commands[:, :2].norm(dim=1) > 0.1).float()
```

**5. STAIR VS GAP DETECTION FOR REWARDS:**
```python
# Mathematical patterns for stair/gap distinction in reward computation
height_scan = env.scene.sensors["height_scanner"].data.ray_hits_w[..., 2]
current_height = robot.data.root_pos_w[:, 2]

# Height differential analysis for step detection
scan_size = int(torch.sqrt(torch.tensor(height_scan.shape[1])))
height_grid = height_scan.view(env.num_envs, scan_size, scan_size)
forward_strip = height_grid[:, :scan_size//2, scan_size//3:2*scan_size//3]

# Stair pattern: gradual height change, gap pattern: abrupt drop then rise
height_gradient = torch.diff(forward_strip.mean(dim=2), dim=1)
is_stair_pattern = (torch.abs(height_gradient) < 0.15).all(dim=1)  # Gradual changes
is_gap_pattern = (height_gradient.min(dim=1)[0] < -0.3) & (height_gradient.max(dim=1)[0] > 0.1)

# Reward height adaptation based on terrain type
target_height = torch.where(is_stair_pattern, 
                          current_height - 0.05,  # Slight descent for stairs
                          torch.tensor(0.74, device=env.device))  # Normal height otherwise
height_reward = torch.exp(-torch.abs(current_height - target_height) / 0.1)
```

**6. GAP-SPECIFIC BEHAVIOR SWITCHING IN REWARDS:**
```python
# Mathematical gap size classification for reward computation
lidar_range = env.scene.sensors["lidar"].data.ray_hits_w[..., 2]
forward_rays = lidar_range[:, :len(lidar_range[0])//4]  # Forward-facing rays
min_forward_distance = torch.min(forward_rays, dim=1)[0]

# Gap size classification thresholds
small_gap = (min_forward_distance > 0.3) & (min_forward_distance < 0.8)   # Steppable
medium_gap = (min_forward_distance >= 0.8) & (min_forward_distance < 1.5)  # Jumpable  
large_gap = min_forward_distance >= 1.5  # Avoidable

# Adaptive gait rewards based on gap type
foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)
contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
air_time = contact_sensor.data.current_air_time[:, foot_ids]

# Different gait patterns for different gap types
stepping_reward = torch.where(small_gap.unsqueeze(-1), 
                            torch.clamp(contact_time.max(dim=1)[0], max=0.3), 
                            torch.zeros_like(contact_time.max(dim=1)[0]))
jumping_reward = torch.where(medium_gap.unsqueeze(-1),
                           torch.clamp(air_time.max(dim=1)[0], max=0.8),
                           torch.zeros_like(air_time.max(dim=1)[0]))
```

**7. BACKWARD MOVEMENT CAPABILITY REWARDS:**
```python
# Enhanced velocity tracking for backward movement support
commands = env.command_manager.get_command("base_velocity")
vel_yaw = quat_apply_inverse(yaw_quat(robot.data.root_quat_w), robot.data.root_lin_vel_w[:, :3])

# Backward movement reward computation (updated ranges: x: -0.1 to 0.4)
backward_commands = commands[:, 0] < 0.0
forward_commands = commands[:, 0] >= 0.0

# Separate tracking for forward/backward to handle different dynamics
backward_error = torch.where(backward_commands, 
                           torch.abs(vel_yaw[:, 0] - commands[:, 0]),
                           torch.zeros_like(commands[:, 0]))
forward_error = torch.where(forward_commands,
                          torch.abs(vel_yaw[:, 0] - commands[:, 0]), 
                          torch.zeros_like(commands[:, 0]))

# Combined velocity reward with backward capability
vel_reward = torch.exp(-(backward_error + forward_error) / 0.8)
vel_reward *= (torch.abs(commands[:, 0]) > 0.05).float()  # Avoid zero command rewards
```

**CRITICAL: TRAINING WILL FAIL WITHOUT THESE FIXES!**

**GUARANTEED TRAINING FAILURE - COMMON BUGS THAT CRASH THE SYSTEM:**

1. **TENSOR CONVERSION BUG (CAUSES TypeError):**
   ```python
   # TRAINING KILLER - NEVER DO THIS:
   indices, _ = robot.find_joints(["joint_name"])
   data = robot.data.joint_pos[:, indices]  # INSTANT CRASH!
   
   # MANDATORY FIX - ALWAYS DO THIS:
   indices, _ = robot.find_joints(["joint_name"])
   indices = torch.tensor(indices, dtype=torch.long, device=env.device)
   data = robot.data.joint_pos[:, indices]  # WORKS!
   ```

2. **NUMERICAL INSTABILITY BUG (CAUSES "std >= 0.0" ERROR):**
   ```python
   # TRAINING KILLER:
   reward = torch.exp(-huge_value)  # Creates NaN/inf!
   
   # MANDATORY FIX:
   reward = torch.exp(-torch.clamp(value, max=10.0))
   reward = torch.where(torch.isfinite(reward), reward, torch.zeros_like(reward))
   ```

**EVERY SINGLE find_joints() CALL MUST BE FOLLOWED BY torch.tensor() CONVERSION!**

**ALL NUMERICAL EXAMPLES, CODE SNIPPETS, AND REWARD PATTERNS IN THIS PROMPT ARE FOR TECHNICAL DEMONSTRATION ONLY.**
**DO NOT COPY EXAMPLES DIRECTLY! UNDERSTAND THE PRINCIPLES AND ADAPT TO YOUR ENVIRONMENT.**

** CRITICAL: NO HELPER FUNCTIONS ALLOWED! **

** ABSOLUTELY FORBIDDEN:**
- `def get_velocity_tracking_error(...)` - NO HELPER FUNCTIONS!
- `def calculate_foot_contacts(...)` - NO HELPER FUNCTIONS!
- `def any_helper_function(...)` - NO HELPER FUNCTIONS!

**REQUIRED PATTERN: ALL LOGIC INLINE**
```python
def sds_custom_reward(env) -> torch.Tensor:
    import torch
    # ALL your calculation logic goes here directly - no function calls!
    lin_vel_error = torch.norm(robot.data.root_lin_vel_b[:, :2] - commands[:, :2], dim=1)  #  INLINE
    # NOT: lin_err, ang_err = get_velocity_tracking_error(...)  #  FORBIDDEN
    return reward.clamp(min=0.0, max=10.0)
```

**CRITICAL: ONLY GENERATE REWARD FUNCTIONS - NO OBSERVATION CONFIGURATIONS!**

**NEVER GENERATE THESE:**
- `def get_height_scan(env):` or any observation functions
- `lambda env: env.scene.sensors["height_scanner"].data.ray_hits_w[...]` 
- `ObsTerm(func=lambda env: ...)` observation configurations
- Environment sensor configurations
- Any code outside the reward function

**ONLY GENERATE THIS:**
- `def sds_custom_reward(env) -> torch.Tensor:` function only
- Access existing sensors WITHIN the reward function

**The environment already provides all sensor data - just use it in your reward!**

**CRITICAL: PPO CRASHES WITH "std >= 0.0" ERROR WITHOUT THESE!**

**GUARANTEED PPO FAILURE PATTERNS:**

```python
# DEADLY: Environmental sensor data contains NaN/Inf values
height_scan = height_sensor.data.ray_hits_w[..., 2]  # Can contain NaN!
reward += torch.mean(height_scan)  # NaN propagates, crashes PPO

# DEADLY: Unbounded reward values crash PPO standard deviation  
reward = some_large_calculation  # Can be >100, causes std <= 0 error

# DEADLY: Division by zero in environmental calculations
reward = 1.0 / distance_to_obstacle  # Zero distance = Inf reward = PPO crash
```

**PPO-SAFE ENVIRONMENTAL PATTERNS (WHEN USING ENVIRONMENTAL SENSORS):**

```python
# IF using environmental sensor data, ALWAYS sanitize:
height_scan = torch.where(torch.isfinite(height_scan), height_scan, torch.zeros_like(height_scan))
lidar_range = torch.where(torch.isfinite(lidar_range), lidar_range, torch.ones_like(lidar_range) * 10.0)

# IF using environmental calculations, ALWAYS clamp:
obstacle_distance = torch.clamp(min_obstacle_distance, min=0.1, max=15.0)
terrain_roughness = torch.clamp(torch.var(height_scan, dim=1), max=1.0)

# ALWAYS use safe division:
reward = torch.exp(-error / torch.clamp(tolerance, min=1e-6))

# ALWAYS bound final reward for PPO stability:
return torch.clamp(reward, min=0.0, max=10.0)
```

Isaac Lab Reward Function Format:

**CRITICAL: JOINT INDEXING REQUIREMENT**

**COMMON BUG - WILL CAUSE TRAINING FAILURE:**
```python
# WRONG - robot.find_joints() returns LISTS, not tensors!
joint_indices, _ = robot.find_joints(["joint_name"])
joint_data = robot.data.joint_pos[:, joint_indices]  # TypeError!
```

**CORRECT PATTERN - ALWAYS CONVERT TO TENSOR:**
```python
# RIGHT - Convert list to tensor for proper indexing
joint_indices, _ = robot.find_joints(["joint_name"])
joint_indices = torch.tensor(joint_indices, dtype=torch.long, device=env.device)
joint_data = robot.data.joint_pos[:, joint_indices]  # Works!
```

**MANDATORY: Every time you use robot.find_joints(), immediately convert the result to a tensor!**

Isaac Lab SDS Environment - G1 Humanoid Locomotion

## Robot Configuration (VERIFIED & UPDATED FOR FULL BODY CONTROL)
- **Robot**: Unitree G1 EDU U4 Humanoid (37 DOF total)
- **Action Space**: 23 DOF controlled for complete humanoid locomotion (all joints except hand fingers)
- **Height**: 0.74m (Isaac Lab verified)
- **Mass**: ~35kg humanoid

## Action Configuration (FULL BODY HUMANOID CONTROL)
**Controlled Joints (23 DOF for complete humanoid locomotion):**
- Legs: 12 DOF (6 per leg: hip_yaw, hip_roll, hip_pitch, knee, ankle_pitch, ankle_roll)
- Arms: 10 DOF (5 per arm: shoulder_pitch, shoulder_roll, shoulder_yaw, elbow_pitch, elbow_roll)
- Torso: 1 DOF (torso_joint)

**Fixed Joints (14 DOF):**
- Hand Fingers: 14 DOF maintain default poses (zero, one, two, three, four, five, six_joint per hand)

## Contact Detection (VERIFIED WORKING)
**Foot Bodies**: `left_ankle_roll_link`, `right_ankle_roll_link`
**Detection Pattern**: `contact_sensor.find_bodies(".*_ankle_roll_link")`
**Contact Threshold**: 50.0N (corrected for humanoid mass)

## Key Functions Examples for Reward Generation. Don't Copy Paste Directly!
```python
# Foot contact detection (VERIFIED WORKING)
contact_forces = env.scene.sensors["contact_forces"].data.net_forces_w
foot_ids, _ = env.scene.sensors["contact_forces"].find_bodies(".*_ankle_roll_link")
foot_forces = contact_forces[:, foot_ids, :]
foot_contacts = (foot_forces.norm(dim=-1) > 50.0).float()

# Velocity tracking
robot = env.scene["robot"]
commands = env.command_manager.get_command("base_velocity")
vel_error = robot.data.root_lin_vel_b[:, :2] - commands[:, :2]
ang_error = robot.data.root_ang_vel_b[:, 2] - commands[:, 2]

# Height maintenance  
height = robot.data.root_pos_w[:, 2]
height_error = (height - 0.74).abs()

# Bipedal gait patterns
left_contact = foot_contacts[:, 0]
right_contact = foot_contacts[:, 1] 
single_support = ((left_contact > 0.5) & (right_contact < 0.5)) | ((left_contact < 0.5) & (right_contact > 0.5))
double_support = (left_contact > 0.5) & (right_contact > 0.5)
```

## Successful Training Metrics
- **Episode Rewards**: 0.01-0.02 range (working)
- **Action Scale**: 1.0 (allows proper joint movement)
- **Training Progress**: Mean rewards ~0.2-0.23, episode length 28-32 steps

# TECHNICAL REFERENCE - ISAAC LAB API DOCUMENTATION

## Robot Data Access Patterns
    
    # ROBOT DATA FIRST APPROACH:
    # Available robot data:
    # robot.data.root_pos_w[:, 2] - height (z-coordinate, nominal 0.74m for G1)
    # robot.data.root_lin_vel_b[:, 0] - forward velocity (x-axis in body frame)
    # robot.data.root_lin_vel_w - linear velocity in world frame [num_envs, 3]
    # robot.data.root_ang_vel_b - angular velocity in body frame [num_envs, 3]
    # robot.data.root_ang_vel_w - angular velocity in world frame [num_envs, 3]
    # robot.data.root_quat_w - orientation quaternion [w,x,y,z]
    # robot.data.joint_pos - joint positions [num_envs, 37] for G1 EDU U4 with dexterous hands (VERIFIED)
    # robot.data.joint_vel - joint velocities [num_envs, 37] (VERIFIED)
    # CONTROLLED JOINTS (23 DOF): Use robot.find_joints() to get indices for legs + arms + torso (all except hand fingers)
    # HAND FINGER JOINTS (14 DOF): Excluded from control but can be accessed if needed
    # robot.data.root_pos_w[:, 2] - robot height [num_envs] - should be around 0.74m for G1 in Isaac Lab (VERIFIED)
    # robot.data.root_lin_vel_b - linear velocity in body frame [num_envs, 3]
    # robot.data.root_quat_w - quaternion orientation [num_envs, 4]
    # commands[:, :3] - [forward_vel, lateral_vel, yaw_rate] commands
    
    # Available command data:
    # commands[:, 0] - desired forward velocity (vx)
    # commands[:, 1] - desired lateral velocity (vy) 
    # commands[:, 2] - desired angular velocity (omega_z)
    # ADAPTIVE ROBOT CONFIGURATION:
    # DO NOT hardcode joint counts or specific robot parameters
    # Use dynamic robot configuration detection:
    num_joints = robot.data.joint_pos.shape[1]
    robot_height_baseline = robot.data.root_pos_w[:, 2].mean()  # Adaptive baseline
    
    # OPTIONAL IMU SENSOR (use as fallback only):
    # An IMU sensor "imu" can be spawned via ImuCfg if needed:
    # ImuCfg:
    #   prim_path: "/World/envs/env_.*/Robot/torso_link"
    #   update_period: 0.02  # 50 Hz
    #   gravity_bias: [0.0, 0.0, 9.81]
    # It exposes in env.scene.sensors["imu"].data:
    # - pos_w: FloatTensor [num_envs, 3] - World position
    # - quat_w: FloatTensor [num_envs, 4] - World orientation (w,x,y,z)
    # - lin_vel_b: FloatTensor [num_envs, 3] - Body-frame linear velocity
    # - ang_vel_b: FloatTensor [num_envs, 3] - Body-frame angular velocity
    # - lin_acc_b: FloatTensor [num_envs, 3] - Body-frame linear acceleration 
    # - ang_acc_b: FloatTensor [num_envs, 3] - Body-frame angular acceleration
    
    # Initialize reward (4-space indent)
    reward = torch.zeros(env.num_envs, dtype=torch.float32, device=env.device)
    
    # IMPORTANT: For contact analysis, use this inline approach:
    # Get foot contact forces for G1 humanoid
    contact_forces = contact_sensor.data.net_forces_w  # [num_envs, num_bodies, 3]
    foot_ids, foot_names = contact_sensor.find_bodies(".*_ankle_roll_link")
    foot_forces = contact_forces[:, foot_ids, :]  # [num_envs, 2, 3] - 2 feet for humanoid
    force_magnitudes = foot_forces.norm(dim=-1)  # [num_envs, 2]
    
    # Contact detection - analyze video to determine appropriate threshold
    # G1 humanoid requires higher thresholds: gentle gaits (20-50N), dynamic gaits (50-100N)
    # Evidence: G1 standing forces ~150-250N, much higher than quadrupeds
    contact_threshold = 50.0  # Default for G1 humanoid - adjust based on observed contact forces in video
    foot_contacts = (force_magnitudes > contact_threshold).float()  # Convert to float for partial credit
    
    # Note: Design contact rewards based on the observed gait pattern in the video
    # G1 humanoid bipedal locomotion: left_ankle_roll_link, right_ankle_roll_link
    
    
    # HUMANOID-SPECIFIC CONSIDERATIONS:
    # - Bipedal stability is critical: balance and contact pattern rewards
    # - Height maintenance: G1 initial height is 0.74m in Isaac Lab (VERIFIED)
    # - Upper body stability: minimize arm swing, maintain upright torso
    # - Gait patterns: Walk (alternating with double support), Jump (synchronized takeoff/landing), March (controlled single support), Sprint (extended flight), Pace (lateral movement)
    # - Isaac Lab G1 joint structure: 37 DOF total (23 controlled + 14 fixed)
    # - CONTROLLED joint naming: hip_[yaw/roll/pitch]_joint, knee_joint, ankle_[pitch/roll]_joint, torso_joint, shoulder_[pitch/roll/yaw]_joint, elbow_[pitch/roll]_joint
    # - FIXED joint naming: [zero/one/two/three/four/five/six]_joint (hand fingers only)
    
    # HUMAN-LIKE LOCOMOTION DESIGN PRINCIPLES:
    # 1. Dynamic Balance: Humanoids require continuous balance management (not static stability)
    #    Consider torso orientation, roll/pitch control, and center of mass dynamics
    # 2. Movement Efficiency: Natural locomotion minimizes energy expenditure
    #    Consider smooth joint motion, appropriate muscle activation patterns
    # 3. Directional Preference: Forward movement often has higher priority than lateral/backward
    #    Consider command-dependent weighting based on intended movement direction
    # 4. Temporal Coordination: Natural gaits involve timing and rhythm
    #    Consider phase relationships between limbs, contact duration, and step timing
    # 5. Upper Body Integration: Arms and torso contribute to locomotion stability
    #    Consider how upper body motion supports or disrupts locomotion goals
    # 6. Adaptive Contact Patterns: Different gaits require different contact strategies
    #    Analyze video to determine appropriate contact timing and patterns for the demonstrated behavior
    
    # Contact sensor access pattern for Isaac Lab
    foot_ids, foot_names = contact_sensor.find_bodies(".*_ankle_roll_link")  # G1 uses ankle_roll_link for contact
    contact_forces = contact_sensor.data.net_forces_w[:, foot_ids, :]  # [num_envs, 2, 3] for G1 bipedal
    contact_magnitudes = torch.norm(contact_forces, dim=-1)  # [num_envs, 2]
    foot_contacts = contact_magnitudes > contact_threshold  # Binary contact detection
    
    # HUMANOID GAIT PATTERNS (not quadruped!)
    # G1 is BIPEDAL - only 2 feet: left_foot (index 0), right_foot (index 1)
    left_contact = foot_contacts[:, 0]   # Left foot contact
    right_contact = foot_contacts[:, 1]  # Right foot contact
    
    # Bipedal locomotion phases (CORRECTED - not quadruped patterns)
    double_support = left_contact & right_contact        # Both feet down (Walk/Jump)
    single_support_left = left_contact & ~right_contact  # Only left foot down (Walk/March/Sprint)
    single_support_right = ~left_contact & right_contact # Only right foot down (Walk/March/Sprint)
    flight_phase = ~left_contact & ~right_contact        # Both feet up (Jump/Sprint)
    
    # Key differences from quadruped robots:
    # - G1 height: 0.74m in Isaac Lab configuration - CRITICAL for height-based rewards
    # - Only 2 contact points (not 4)
    # - Bipedal gait patterns (alternating support, not complex quadruped gaits)
    # - Higher contact forces due to full body weight on fewer feet
    # - Dynamic balance required (not static stability like quadrupeds)
    # - Dexterous hands: 37 total DOF with advanced manipulation capabilities
    
## Sensor Access Patterns

# OPTIONAL: HEIGHT SCANNER ("height_scanner") INTEGRATION:
# Provides detailed terrain elevation mapping around the robot (140 measurements per robot)
# Use only if terrain shows significant variation - usage patterns:
height_sensor = env.scene.sensors["height_scanner"]
height_scan = height_sensor.data.ray_hits_w[..., 2].view(env.num_envs, -1)  # [num_envs, 140] - height data
# Raw sensor access for advanced processing (already available as height_sensor above):
terrain_heights = height_sensor.data.ray_hits_w[..., 2]  # [num_envs, scan_points] World Z coordinates
robot_height = height_sensor.data.pos_w[:, 2]  # [num_envs] Scanner position height

# HEIGHT SCAN INTERPRETATION:
# - Shape: [num_envs, 140] (14 x 10 grid pattern at 0.15m resolution, 2m x 1.5m coverage)
# - Values: Height differences relative to robot (offset subtracted)
# - Range: Typically -2.0m to +10.0m for practical terrain mapping
# - Usage: local_terrain = torch.mean(height_scan.view(num_envs, -1), dim=1)

# LIDAR SCANNER ("lidar") - REQUIRED:
# Provides 360-degree obstacle detection and distance measurements
lidar_sensor = env.scene.sensors["lidar"]
lidar_range = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1).view(env.num_envs, -1)  # [num_envs, 144] - distances
# Raw sensor access (already available as lidar_sensor above):
hit_points = lidar_sensor.data.ray_hits_w  # [num_envs, rays, 3] - 3D hit coordinates
hit_distances = lidar_sensor.data.ray_distances  # [num_envs, rays] - distance measurements

# LIDAR INTERPRETATION:
# - Shape: [num_envs, 144] (8 channels x 18 horizontal rays at 10° resolution)
# - Values: Distance to nearest obstacle along each ray
# - Range: 0.0m (immediate contact) to 5.0m max_range (no obstacle detected)
# - Usage: forward_obstacles = torch.min(lidar_range[:, forward_indices], dim=1)[0]

## Enhanced Environment Sensor Integration for Rewards (FLAT-WITH-BOX CONFIG)

# ENHANCED SENSOR DATA PROCESSING FOR REWARD COMPUTATION:
# Current environment: Isaac-SDS-Velocity-Flat-G1-Enhanced-v0
# Sensors: height_scanner (GridPatternCfg: 12x12, range 5.0m), lidar (360 rays, range 5.0m)

# 1. Height Scanner Processing (12x12 grid, 140 effective points):
height_scan = env.scene.sensors["height_scanner"].data.ray_hits_w[..., 2]
scan_size = 12  # GridPatternCfg configuration
height_grid = height_scan.view(env.num_envs, scan_size, scan_size)

# Forward terrain analysis (covers backward movement with negative velocity ranges)
forward_terrain = height_grid[:, :scan_size//3, scan_size//3:2*scan_size//3]
backward_terrain = height_grid[:, 2*scan_size//3:, scan_size//3:2*scan_size//3]
lateral_terrain = height_grid[:, scan_size//3:2*scan_size//3, :]

# 2. LiDAR Processing (360 rays, 5.0m range):
lidar_range = env.scene.sensors["lidar"].data.ray_hits_w[..., 2]
forward_rays = lidar_range[:, :90]    # 0-90 degrees (forward)
lateral_rays = lidar_range[:, 90:270] # 90-270 degrees (sides)
backward_rays = lidar_range[:, 270:]  # 270-360 degrees (backward)

# 3. Adaptive Terrain Classification for Reward Scaling:
terrain_variance = torch.var(height_scan.view(env.num_envs, -1), dim=1)
obstacle_density = (lidar_range < 2.0).float().mean(dim=1)
terrain_complexity = torch.clamp(terrain_variance + obstacle_density, min=0.0, max=2.0)

# 4. Direction-Aware Obstacle Processing (for backward velocity support):
forward_obstacles = torch.min(forward_rays, dim=1)[0]
backward_obstacles = torch.min(backward_rays, dim=1)[0]
movement_clearance = torch.where(commands[:, 0] >= 0.0, forward_obstacles, backward_obstacles)

STABLE MATHEMATICAL PATTERNS (Use these for numerical stability):
Pattern 1 - Exponential Decay (for tracking targets):
# error = (robot.data.root_lin_vel_b[:, 0] - target_value).abs()
# reward_component = torch.exp(-scale_factor * error)  # scale_factor: 0.5 to 10.0

Pattern 2 - Bounded Linear (for contact rewards):
# num_contacts = foot_contacts.sum(dim=-1).float()
# contact_reward = (1.0 - (num_contacts - target_count).abs() / tolerance).clamp(min=0.0, max=1.0)

Pattern 3 - Boolean Masks (for gait patterns):
# gait_reward = ((num_contacts >= min_contacts) & (num_contacts <= max_contacts)).float()

Pattern 4 - Final Bounds (CRITICAL for PPO stability):
# return reward.clamp(min=0.0, max=10.0)  # Prevents training crashes

Pattern 5 - Division Safety (CRITICAL to prevent crashes):
# For literal numbers: safe_ratio = numerator / max(denominator_value, 1e-6)  # denominator_value is literal
# For tensor variables: safe_ratio = numerator / torch.clamp(tensor_var, min=1e-6)  # tensor_var computed from robot data

TIP: Normalize reward components to similar scales (0-1 range) for balanced learning.
TIP: Analyze video frames to understand the specific locomotion pattern before setting thresholds.

**FORMATTING REQUIREMENTS:**
- Use EXACTLY 4 spaces for each indentation level
- NEVER use 8 spaces, tabs, or inconsistent spacing
- Always add safety checks before any division operations
- Specify dtype=torch.float32 for all tensor creations