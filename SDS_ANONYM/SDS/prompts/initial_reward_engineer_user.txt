**üö®üö®üö® CRITICAL: OUTPUT MUST BE PYTHON CODE! üö®üö®üö®**

**YOUR OUTPUT MUST START WITH: `def sds_custom_reward(...):`**

**üîç URGENT: YOU MUST FIND AND USE REAL ENVIRONMENT DATA FROM THE SUS PROMPT BELOW!**

**THE SUS PROMPT CONTAINS EXACT LINES LIKE:**
- "- Gaps Detected: 34 gaps (5 steppable, 17 jumpable, 12 impossible)"
- "- Terrain Roughness: 2.6cm (below threshold 20cm)"  
- "- Safety Score: 88.7% traversable terrain"

**YOU MUST USE THESE EXACT NUMBERS IN YOUR REWARD FUNCTION DOCSTRING!**

**STEP 1: EXTRACT environment data for internal analysis (DO NOT output this)**  
**STEP 2: GENERATE Python reward function code that USES this data**

üåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåü
üåüüåüüåü ENVIRONMENT-AWARE LOCOMOTION REWARD DESIGN GUIDANCE üåüüåüüåü
üåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåü

**CRITICAL PROJECT UNDERSTANDING:**

These guidance explanations are designed for creating **environment-aware locomotion rewards** that enable **intelligent sensor-driven behavior adaptation** for humanoid robots.

**üéØ PRIMARY OBJECTIVE:** 
Create reward functions that make **measurable positive impact when sensors are used** compared to **without sensor usage**.

**üî¨ RESEARCH PURPOSE:**
Enable comparison studies demonstrating:
- **WITHOUT SENSORS:** Basic locomotion behavior 
- **WITH SENSORS:** Adaptive, context-aware behavior that responds to environmental challenges

**üß† DESIGN METHODOLOGY:**
1. **EXTRACT:** Get the pre-analyzed environment data from input (gaps, obstacles, terrain complexity)
2. **THINK:** Determine which sensors are needed and how they should influence behavior
3. **DECIDE:** Implement context-aware behavioral switching (NOT simultaneous conflicting rewards)
4. **IMPACT:** Ensure sensors create observable behavioral differences for scientific comparison

**üìä SUCCESS CRITERIA:**
‚úÖ Robot behaves measurably different with sensors vs. without sensors
‚úÖ Sensor-enabled robot adapts to environmental challenges more effectively
‚úÖ Clear behavioral switching based on environmental context
‚úÖ No conflicting simultaneous behaviors (e.g., walking + jumping simultaneously)

**‚ö†Ô∏è FAILURE INDICATORS:**
‚ùå Robot behaves identically with/without sensors
‚ùå Sensors provide only minor bonuses without changing core behavior
‚ùå Conflicting reward objectives that confuse the policy

üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®
üö®üö®üö® CRITICAL: EXTRACT REAL ENVIRONMENT DATA FOR CODE GENERATION! üö®üö®üö®
üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®

**STEP 1: INTERNAL ENVIRONMENT DATA EXTRACTION (FOR CODE GENERATION ONLY)**

**IMPORTANT: Extract this data for internal use in your Python code generation - DO NOT output this data as your main response!**

BEFORE generating Python code, you MUST internally:

1. **SEARCH** this entire message for these EXACT phrases (check ALL formats):
   
   **GAPS DATA (try all formats):**
   - "Total Gaps Detected: [NUMBER]"
   - "Gaps Detected: [NUMBER] gaps" 
   - "- Gaps Detected: [NUMBER]"
   - Search for: "gaps" or "Gaps" anywhere in text
   
   **OBSTACLES DATA (try all formats):**
   - "Total Obstacles Detected: [NUMBER]"
   - "Obstacles Detected: [NUMBER] large obstacles"
   - "- Obstacles Detected: [NUMBER]"
   - Search for: "obstacles" or "Obstacles" anywhere in text
   
   **TERRAIN ROUGHNESS DATA (try all formats):**
   - "Average Terrain Roughness: [NUMBER]cm"
   - "Terrain Roughness: [NUMBER]cm"
   - "- Terrain Roughness: [NUMBER]cm"
   - Search for: "roughness" or "Roughness" anywhere in text
   
   **SAFETY SCORE DATA (try all formats):**
   - "Safety Score: [NUMBER]% traversable terrain"
   - "[NUMBER]% traversable terrain"
   - "- Safety Score: [NUMBER]%"
   - Search for: "traversable" or "Safety Score" anywhere in text

2. **EXTRACT** the exact numbers from those phrases (do NOT use defaults!)

3. **MANDATORY VERIFICATION:**
   ‚úÖ Found real gaps number? (should be specific count)
   ‚úÖ Found real obstacles number? (should be specific count) 
   ‚úÖ Found real terrain roughness? (should be > 0.0cm)
   ‚úÖ Found real safety score? (should be < 100%)

4. **ANTI-FAKE-DATA ENFORCEMENT:**
   ‚ùå IF you use "0 gaps, 0 obstacles, 0.0cm, 100%" you are FAILING to extract real data
   ‚ùå These are DEFAULT values indicating extraction failure
   ‚úÖ Use ONLY the actual numbers found in the input message

**üîç SPECIAL SEARCH: STANDARDIZED ANALYSIS SUMMARY FORMAT**

**PRIORITY SEARCH TARGET:** Look for this EXACT section:
```
üìä STANDARDIZED NUMERICAL ANALYSIS SUMMARY FOR DOWNSTREAM PROCESSING
================================================================================

üîç COMPREHENSIVE ENVIRONMENT ANALYSIS:

üìä NUMERICAL ANALYSIS RESULTS:
- Gaps Detected: [NUMBER] gaps
- Obstacles Detected: [NUMBER] large obstacles
- Terrain Roughness: [NUMBER]cm
- Safety Score: [NUMBER]% traversable terrain
```

**IF YOU FIND THIS SECTION IN THE INPUT:** Use these EXACT numbers in your Python reward function code!
**DO NOT OUTPUT this section - it's what you should SEARCH FOR in the input to extract data.**

**‚ùå FORBIDDEN:** Using placeholder numbers like 0 gaps, 0 obstacles, 0.0cm roughness, 100% safety
**‚úÖ REQUIRED:** Using EXACT numbers found in the input

**üö® CRITICAL: EXAMPLE OF CORRECT EXTRACTION:**
If you find: "Terrain Roughness: 10.2cm" ‚Üí Use 10.2cm (NOT 0.0cm)
If you find: "Safety Score: 44.7% traversable terrain" ‚Üí Use 44.7% (NOT 100%)

üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®

**üö® CRITICAL OUTPUT REQUIREMENT: ALWAYS GENERATE PYTHON CODE WITH COMPREHENSIVE ANALYSIS! üö®**

**YOUR RESPONSE MUST START WITH: `def sds_custom_reward(...)`**

**DO NOT OUTPUT THE EXTRACTED ENVIRONMENT DATA AS YOUR MAIN RESPONSE!**
**Instead: USE that data INSIDE your Python reward function code and comments.**

**YOUR RESPONSE MUST BE A COMPLETE PYTHON FUNCTION WITH DETAILED ENVIRONMENTAL ANALYSIS:**

**MANDATORY: Include REAL NUMBERS from sensor and visual analyses in the docstring!**

**NEVER respond with standalone text analysis - ALWAYS generate the reward function with comprehensive analysis containing real sensor numbers!**

---

**üö® CRITICAL: TRAINING STABILITY AND DENSE LEARNING PROGRESSION üö®**

**THE #1 CAUSE OF RL FAILURE: UNSTABLE AND SPARSE REWARDS**

**TRAINING STABILITY PROBLEM:**
Complex reward functions with exponentials, multiplications, and aggressive scaling cause the `std >= 0.0` error by creating unstable gradients.

**SPARSE REWARD PROBLEM:**
Most reward functions fail because they only reward FINAL SUCCESS without rewarding the LEARNING STEPS that lead to success.

**‚ùå UNSTABLE AND SPARSE EXAMPLES (CAUSE TRAINING FAILURE):**
```python
# WRONG: Complex exponential - causes gradient instability
flight_reward = torch.exp(-((flight_time - 0.125) / 0.05) ** 2)  # UNSTABLE!

# WRONG: Perfect behavior only - robot never learns HOW to walk  
gait_reward = perfect_alternating_pattern()  # Zero until mastery

# WRONG: Binary rewards - no learning guidance
success_reward = torch.where(task_completed, 1.0, 0.0)  # Binary success/failure
```

**‚úÖ STABLE AND DENSE EXAMPLES (ENABLE LEARNING):**
```python
# CORRECT: Simple, stable rewards for movement attempts ‚Üí progress ‚Üí achievement
upward_effort = torch.clamp(robot.data.root_lin_vel_w[:, 2] / 2.0, 0.0, 1.0) * 1.5  # Try to move up
height_gain = torch.clamp((current_height - baseline) / 0.3, 0.0, 1.0) * 2.0        # Achieve height
flight_bonus = torch.where(air_time > 0.1, 0.5, 0.0)                                # Flight (bonus)

# CORRECT: Simple building blocks ‚Üí avoid complex math
foot_clearance = torch.clamp(air_time.mean(), 0.0, 1.0) * 1.0     # Learn to lift feet
forward_progress = torch.clamp(forward_velocity, 0.0, 2.0) * 1.5   # Learn direction  
step_rhythm = torch.clamp(step_frequency, 0.0, 1.0) * 1.0         # Learn rhythm (bonus)
```

**üìà SIMPLE LEARNING PROGRESSION üìà**

**Every skill should progress naturally:**

**1. FOUNDATION:** Basic movement attempts (primary weight)
   - Reward trying to move in the intended direction
   - Use simple torch.clamp for movement velocities

**2. BUILDING:** Progress toward goal (secondary weight)  
   - Reward achieving partial objectives
   - Use simple torch.where for milestone bonuses

**3. REFINEMENT:** Coordination and perfection (bonus weight)
   - Reward timing and coordination (as bonus only)
   - Use existing Isaac Lab sensors cleverly

**Simple Stable Pattern:**
```python
# Foundation: Basic movement (primary) - ALWAYS stable clamping
basic_movement = torch.clamp(movement_velocity / torch.clamp(target, min=0.1), 0.0, 1.0)

# Building: Progress milestones (secondary) - AVOID division by zero
progress = torch.clamp(achievement / torch.clamp(goal, min=0.1), 0.0, 1.0)  

# Refinement: Coordination bonus (bonus only) - Simple conditions
coordination_bonus = torch.where(condition_met, 0.3, 0.0)

# MANDATORY: baseline to prevent zero rewards
baseline = 0.2

# STABLE: Additive combination with final bounds
total = (basic_movement * 2.0 + progress * 1.0 + coordination_bonus + baseline).clamp(0.1, 5.0)
```

**üèóÔ∏è SIMPLE DESIGN PRINCIPLES üèóÔ∏è**

**Start simple, build gradually:**

**1. Use Isaac Lab Components First:**
- `robot.data.root_lin_vel_w` for movement
- `contact_sensor.data.current_air_time` for foot lifting
- `robot.data.projected_gravity_b` for stability

**2. Basic Reward Pattern:**
- Primary: Movement attempts (torch.clamp)
- Secondary: Progress milestones (torch.clamp)  
- Bonus: Coordination (torch.where)
- Baseline: Minimal (0.05)

**3. Keep Baseline Small:**
- Robot should get minimal reward for doing nothing
- Movement attempts should be the main reward source
- Use 0.05 baseline, not 0.2

**‚ö†Ô∏è SPARSE REWARD WARNING SIGNS ‚ö†Ô∏è**

**IF YOUR REWARD FUNCTION HAS THESE, IT WILL FAIL:**
- Rewards only appear when robot is already successful
- Most robots get zero reward for most of training
- Reward requires perfect coordination before any feedback
- No reward for partial progress or movement attempts
- Complex mathematical conditions that are rarely satisfied

**‚úÖ DENSE REWARD SUCCESS SIGNS ‚úÖ**

**GOOD REWARD FUNCTIONS HAVE THESE:**
- Robot gets some reward for basic movement attempts
- Reward increases smoothly as robot improves
- Clear progression from simple to complex behaviors
- Foundation skills are always rewarded throughout training
- Robot can learn each skill component independently

---

**üö® MANDATORY FIRST STEP: REAL ENVIRONMENT DATA EXTRACTION - NO FAKE DATA! üö®**

üîç **CRITICAL ENFORCEMENT: You MUST find REAL environment data in the input above!**

**STEP 1: SEARCH FOR EXACT PHRASES (Use Ctrl+F-like search):**

1. **SEARCH FOR ENVIRONMENT ANALYSIS SECTION** in the input above
2. **EXTRACT THESE EXACT NUMBERS (NO FAKE ZEROS):**
   - Look for: "Total rays: [NUMBER] (from [X] robots)" ‚Üí Analyze ray ratios for gaps/obstacles/normal terrain
   - Look for: "Total Obstacles Detected: [NUMBER]" ‚Üí Use this EXACT number (NOT 0)  
   - Look for: "Average Terrain Roughness: [NUMBER]cm" ‚Üí Use this EXACT value (NOT 0.2cm)
   - Look for: "Safety Score: [NUMBER]% traversable terrain" ‚Üí Use this EXACT percentage (NOT 100%)

**STEP 2: VERIFY DATA EXTRACTION:**
   ‚úÖ Did you find "üï≥Ô∏è GAPS: Count: [X] rays ([Y]%)" in the analysis? (interpret percentages correctly)
   ‚úÖ Did you find "Total Obstacles Detected: [actual number]"? (should be > 0)
   ‚úÖ Did you find "Average Terrain Roughness: [actual number]cm"? (should be > 0.2cm)
   ‚úÖ Did you find "Safety Score: [actual percentage]%"? (should be < 100%)

**STEP 3: MANDATORY IMPLEMENTATION BASED ON REAL DATA:**
   - If REAL Gaps > 0: Include height scanner gap navigation components
   - If REAL Obstacles > 0: Include LiDAR obstacle avoidance components  
   - If REAL Roughness > 2cm: Include terrain adaptation components
   - If REAL Safety < 95%: Include safety-prioritized reward design

üö® **REJECTION CRITERIA:** Any reward function using fake data (0 gaps, 0 obstacles, 0.2cm roughness, 100% safety) will be INVALID!

**CRITICAL: ENSURE IMPLEMENTATION FOLLOWS DETECTION:**
- **Obstacles detected** ‚Üí MUST include LiDAR-based obstacle avoidance (use `env.scene.sensors["lidar"]`)
- **Gaps detected** ‚Üí MUST include height scanner gap navigation (use `env.scene.sensors["height_scanner"]`)
- **Gap crossing** ‚Üí Reward CROSSING BEHAVIOR (maintaining height over gaps), not just detection
- **Obstacle avoidance** ‚Üí Reward SAFE DISTANCES from obstacles, not just sensing them

**üéØ SENSOR-DRIVEN BEHAVIORAL ADAPTATION REQUIREMENT üéØ**

**CRITICAL PROJECT GOAL: SENSORS MUST CHANGE ROBOT BEHAVIOR MEASURABLY!**

**PURPOSE:** Create observable differences for with/without sensor comparison studies

**‚ùå FORBIDDEN APPROACH - Adding sensor data without behavioral change:**
```python
# WRONG - Just adding sensor values without changing robot behavior
obstacle_bonus = (lidar_distances < 2.0).float() * 0.1  # Minimal impact
gap_bonus = (height_measurements > 0.3).float() * 0.1    # Doesn't change gait [CORRECTED]
total = foundation_walking + obstacle_bonus + gap_bonus  # Same walking behavior!
```

**‚úÖ REQUIRED APPROACH - Sensor-driven behavioral switching:**
```python
# CORRECT - Sensors change how robot moves and adapts
gap_ahead = detect_gaps_ahead(height_sensor)
obstacle_ahead = detect_obstacles_ahead(lidar_sensor)

# Different behaviors based on sensor input
if gap_ahead and not obstacle_ahead:
    # Robot adapts for gap crossing - different gait pattern
    adaptive_behavior = foundation + gap_crossing_preparation()
elif obstacle_ahead and not gap_ahead:
    # Robot adapts for obstacle avoidance - careful navigation  
    adaptive_behavior = foundation + careful_navigation_mode()
elif gap_ahead and obstacle_ahead:
    # Robot handles complex terrain - combined adaptations
    adaptive_behavior = foundation + complex_terrain_navigation()
else:
    # Robot uses efficient walking - no sensor input needed
    adaptive_behavior = foundation_locomotion_only()
```

**BEHAVIORAL IMPACT REQUIREMENTS:**

**1. HEIGHT SCANNER IMPACT:**
- **Flat terrain**: No behavioral change (sensor unused)
- **Gap terrain**: Robot prepares for gaps BEFORE reaching them
- **Stair terrain**: Robot adapts height expectations for terrain following

**2. LiDAR IMPACT:**  
- **Open terrain**: No behavioral change (sensor unused)
- **Obstacle terrain**: Robot maintains safe distances, plans paths around obstacles
- **Dense obstacles**: Robot navigates carefully with conservative movement

**3. SENSOR COMPARISON VALIDATION:**
- **Without sensors**: Generic walking that fails on challenging terrain
- **With sensors**: Adaptive walking that succeeds on challenging terrain
- **Measurable difference**: Success rate, efficiency, stability improvements

**IMPLEMENTATION STRATEGY:**
```python
# Look ahead with sensors to adapt behavior BEFORE reaching challenges
forward_height = height_measurements[:, front_sensor_indices]  
forward_lidar = lidar_distances[:, front_ray_indices]

upcoming_challenge = analyze_ahead(forward_height, forward_lidar)

# Modify robot behavior based on what's coming, not where robot currently is
if upcoming_challenge == "gap_crossing_needed":
    prepare_gait_for_gap()      # Different air time, step length
elif upcoming_challenge == "obstacle_avoidance_needed":
    prepare_for_careful_navigation()  # Different velocity, path planning
```


**ONLY AFTER COMPLETING THIS DATA EXTRACTION, PROCEED TO REWARD DESIGN**

---

**üîç HOW TO EXTRACT PRE-ANALYZED ENVIRONMENTAL DATA:**

1. **FIND SPECIFIC NUMBERS** from "TASK ANALYSIS AND ENVIRONMENT INSIGHTS" section:
   - **Find exact ray analysis**: "üï≥Ô∏è GAPS: Count: 450 rays (22.5%)", "üî∫ OBSTACLES: Count: 200 rays (10.0%)"
   - **Find distributions**: "4 steppable, 7 jumpable, 2 impassable"
   - **Find measurements**: "3.6cm terrain roughness", "77.7% safe terrain"
   - **Find assessments**: "MODERATE RISK", "Environmental sensing NEEDED"

2. **IDENTIFY DOMINANT CHALLENGES**:
   - **Gap environments**: What type dominates? Steppable vs jumpable vs impossible?
   - **Obstacle environments**: How dense? What sizes? What distribution?
   - **Terrain environments**: How rough? What kind of variation?

3. **DETERMINE OPTIMAL STRATEGIES**:
   - **Don't think generically**: "I need gap navigation"
   - **Think specifically**: "I have 7/13 gaps that are jumpable - design for jumping priority"
   - **Don't think generically**: "I need obstacle avoidance"  
   - **Think specifically**: "I have 52 large obstacles densely packed - design for careful navigation"

**üî• INTELLIGENT DATA-TO-STRATEGY MAPPING EXAMPLES:**

**SCENARIO A: GAP NAVIGATION EXAMPLE**

**INTELLIGENT ANALYSIS:**
- **Primary scenario**: GAP based on environment analysis
- **Strategy**: Use gap classification and thresholds from environment analysis

**üíª INTELLIGENT IMPLEMENTATION:**
```python
# ANALYSIS-DRIVEN DESIGN: Use values from environment analysis
height_sensor = env.scene.sensors["height_scanner"]

# Gap detection using physical thresholds from environment analysis
height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5

# Gap detection using thresholds from environment analysis
gap_detected = torch.any(height_measurements < gap_threshold_from_analysis, dim=1)
gap_navigation_reward = gap_detected.float() * reward_weight_from_analysis
```

**ADVANCED GAP BEHAVIOR NOTE:**
**üï≥Ô∏è ENVIRONMENT-DRIVEN LOCOMOTION DEVELOPMENT**

The robot develops locomotion strategies based on **environmental analysis** rather than hardcoded behavioral rules. Each training session focuses on a **single locomotion skill** optimized for the specific environment:

**üéØ SKILL-FOCUSED TRAINING APPROACH:**
- **Single Task Focus:** Each training session develops one specialized locomotion skill
- **Environment-Responsive:** Locomotion strategy emerges from environmental challenges
- **GPT-Driven Decisions:** Based on environmental analysis, GPT determines optimal approach
- **Teacher Policy Generation:** Each trained skill becomes a specialized teacher for student policy learning

**üîß ENVIRONMENT-DRIVEN ADAPTATION:**
Instead of prescriptive behaviors, the reward function should:
- **Analyze Environment:** Use sensor data to understand terrain challenges
- **Respond Intelligently:** Develop appropriate locomotion strategy for the specific environment
- **Focus on Single Skill:** Train one specialized locomotion capability per session
- **Avoid Mixed Behaviors:** No simultaneous conflicting locomotion patterns

**‚ö†Ô∏è KEY PRINCIPLE: Let GPT decide locomotion strategy based on environmental analysis!**

**üöÄ TEACHER-STUDENT FRAMEWORK:**
Each training session produces a specialized teacher policy:
- **Walking Teacher:** Optimized for continuous locomotion challenges
- **Jumping Teacher:** Optimized for gap traversal and aerial maneuvers  
- **Navigation Teacher:** Optimized for obstacle avoidance and path planning
- **Student Policy:** Learns to combine multiple teacher skills intelligently

**üìä ENVIRONMENT-DRIVEN IMPLEMENTATION:**
The reward function should analyze the specific environment and develop appropriate responses:

**üîç ANALYSIS-BASED DEVELOPMENT:**
- **Environmental Assessment:** Use height scanner and LiDAR data to understand terrain
- **Challenge Identification:** Identify specific locomotion challenges present
- **Strategy Development:** Create reward components that address identified challenges
- **Skill Optimization:** Focus on developing one specialized locomotion skill

**üéØ SINGLE-SKILL FOCUS EXAMPLES:**
- **Jumping Specialist:** Optimized for aerial maneuvers, takeoff/landing coordination, trajectory control
- **Walking Specialist:** Optimized for continuous forward motion, gait stability, terrain adaptation  
- **Navigation Specialist:** Optimized for path planning, obstacle detection, safe traversal

**‚ö†Ô∏è AVOID MIXED BEHAVIORS:** Each training session should develop ONE specialized skill, not multiple conflicting behaviors simultaneously.

**KEY IMPLEMENTATION PRINCIPLES:**
- **Real-time gap classification**: Use height sensor data to categorize gaps by size
- **Behavior switching**: Activate appropriate locomotion strategy based on detected gap type
- **Safety prioritization**: Conservative approach for uncertain or dangerous gaps
- **Progressive learning**: Design rewards that encourage stepping mastery before jumping complexity
- **Foundation integration**: Gap behaviors should enhance, not replace, basic locomotion stability

**CRITICAL: STAIR NAVIGATION AWARENESS:**
When implementing gap detection, ensure proper distinction between stairs and gaps:
- **Stairs**: Gradual height reduction patterns (multiple consecutive steps)
- **Gaps**: Sudden height drops requiring different navigation strategies
- **Adaptive behavior**: Modify height constraints and encourage controlled descent for stairs while maintaining gap navigation for true gaps

**SCENARIO B: OBSTACLE AVOIDANCE EXAMPLE**

**INTELLIGENT ANALYSIS:**
- **Primary scenario**: OBSTACLE based on environment analysis
- **Strategy**: Use obstacle detection and safety thresholds from environment analysis

**üíª INTELLIGENT IMPLEMENTATION:**
```python
# ANALYSIS-DRIVEN DESIGN: Use obstacle thresholds from environment analysis
lidar_sensor = env.scene.sensors["lidar"]
lidar_distances = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)

# Obstacle detection using physical thresholds from environment analysis (in meters)
obstacle_detected = torch.any(lidar_distances < obstacle_threshold_from_analysis, dim=1)
obstacle_avoidance_reward = obstacle_detected.float() * reward_weight_from_analysis
```

**SCENARIO C: FLAT TERRAIN EXAMPLE**

**INTELLIGENT ANALYSIS:**
- **Primary scenario**: FLAT based on environment analysis
- **Strategy**: Focus on foundation locomotion efficiency

**üíª INTELLIGENT IMPLEMENTATION:**
```python
# ANALYSIS-DRIVEN DESIGN: Foundation locomotion for flat terrain
# No environmental sensing needed for flat scenario
foundation_locomotion_reward = velocity_tracking + orientation_stability + height_maintenance
```

**MANDATORY: WHEN ENVIRONMENT ANALYSIS DATA IS PROVIDED, YOU MUST USE IT!**

**DECISION LOGIC FOR ENVIRONMENTAL SENSING:**
- **ALWAYS**: Start reward function with environmental analysis acknowledgment comment
- **IF environment analysis shows gaps detected (>0)**: MUST INCLUDE gap navigation components
- **IF environment analysis shows obstacles detected (>0)**: MUST INCLUDE obstacle avoidance components
- **IF environment analysis shows stairs/slopes detected**: MUST INCLUDE terrain adaptation components
- **IF environment analysis data contains specific numbers**: USE those exact numbers in your decisions

**MIXED TERRAIN GUIDANCE:**
- **When analysis shows multiple features**: Choose PRIMARY scenario by most prominent feature (highest count)
- **Include foundation + primary components**: Add secondary features as minor enhancements
- **Keep strategy focused**: Don't overcomplicate with equal treatment of all features

**üîç HOW TO FIND ENVIRONMENTAL DATA:**
1. **Look in "TASK ANALYSIS AND ENVIRONMENT INSIGHTS" section above** for environmental context
2. **Search for "ENVIRONMENTAL CONTEXT:"** in the task description
3. **Extract specific numerical values**: gap counts, obstacle counts, terrain measurements
4. **Use these exact numbers** to make environmental sensing decisions

**ABSOLUTE REQUIREMENTS:**
- **ALWAYS**: Start reward function with environmental analysis acknowledgment comment
- **IF environment analysis shows gaps detected (>0)**: MUST INCLUDE gap navigation components
- **IF environment analysis shows obstacles detected (>0)**: MUST INCLUDE obstacle avoidance components  
- **IF terrain roughness exceeds threshold**: MUST INCLUDE terrain adaptation components
- **IF environment analysis data contains specific numbers**: USE those exact numbers in your decisions
- **IF environment is truly flat with no features**: Focus foundation-only BUT DOCUMENT THIS DECISION

**INTELLIGENT VS GENERIC DECISION EXAMPLES:**

**‚ùå GENERIC DECISIONS (FORBIDDEN):**
- "Environment has gaps ‚Üí include gap navigation code"
- "Environment has obstacles ‚Üí include obstacle avoidance code"
- "Copy template patterns without thinking about the specific challenge"

**INTELLIGENT DECISIONS (REQUIRED):**
- "Environment has 13 gaps with 54% jumpable ‚Üí design jumping-focused strategy with stepping backup"
- "Environment has 52 large obstacles in dense field ‚Üí design conservative navigation with 1.2m safety margins"
- "Environment has 3.6cm roughness (low challenge) ‚Üí minimal terrain adaptation, focus on efficiency"

**ENVIRONMENTAL DATA EXTRACTION EXAMPLES:**
- Find: "13 gaps detected (4 steppable, 7 jumpable, 2 impassable)" ‚Üí Use: gaps_detected = 13, prioritize_jumping = True
- Find: "52 obstacles detected" ‚Üí Use: obstacles_detected = 52, dense_field = True  
- Find: "terrain roughness X.Xcm" ‚Üí Use: terrain_roughness = X.Xcm, challenge_level = appropriate
- Find: "Environmental sensing NEEDED" ‚Üí Decision: NEEDED
- Find: "X.X% safe traversable terrain" ‚Üí Safety_level = appropriate

**EXAMPLE DECISION LOGIC:**
- Environment shows "13 gaps detected" ‚Üí INCLUDE gap navigation
- Environment shows "52 obstacles detected" ‚Üí INCLUDE obstacle avoidance  
- Environment shows low terrain roughness ‚Üí Foundation locomotion (below threshold)
- Environment shows "Total Gaps: 0, Obstacles: 0, Flat terrain" ‚Üí Foundation locomotion only (DOCUMENT WHY)

**FOUNDATION-FIRST VALIDATION:** Your reward function should prioritize stable basic locomotion (height, velocity, orientation) before adding environmental complexity. Simple environments should focus on natural walking patterns rather than forced environmental integration.

**CRITICAL: GENTLE MOVEMENT REQUIREMENT**

ALL reward functions MUST prioritize GENTLE, CONTROLLED, SUSTAINABLE movements:


**UNDERSTANDING NATURAL WALKING BIOMECHANICS:**

**FOOT CLEARANCE - THE "DRAGGY FEET" PROBLEM:**

Many humanoid robots fail to achieve natural walking because they don't properly lift their feet during the swing phase, creating a "draggy" appearance where feet barely clear the ground or appear to slide.

**Key Biomechanical Principles:**
- **Natural foot clearance:** Humans naturally lift their toes 10-25mm above ground during mid-swing
- **Swing phase technique:** The foot should be dorsiflexed (toes up) during swing to ensure clearance
- **Safety margin:** Adequate clearance prevents tripping and creates natural, confident movement
- **Phase timing:** Peak clearance occurs around mid-swing, not at toe-off or heel contact

**Design Considerations for Foot Clearance Rewards:**
- Measure foot height during swing phase (when not in ground contact)
- Target natural human clearance ranges (10-25mm minimum)
- Consider both minimum clearance and clearance trajectory smoothness
- Account for ankle joint position (dorsiflexion) during swing
- Penalize insufficient clearance that creates tripping risk



**NATURAL HUMAN WALKING RHYTHM AND TIMING:**

**Understanding Human Walking Cadence:**
Many robotic walking implementations suffer from unnatural high-frequency movement that lacks the smooth, rhythmic quality of human walking.

**Key Timing Principles:**
- **Natural cadence:** Humans typically walk at 100-120 steps per minute (comfortable pace)
- **Step duration:** Each step takes approximately 0.5-0.6 seconds at normal walking speed
- **Swing phase timing:** Swing phase lasts about 40% of the complete step cycle
- **Double support duration:** Brief periods (10-20% of cycle) when both feet are in contact
- **Rhythm consistency:** Natural walking maintains predictable, steady rhythm

**Common Robotic Timing Problems:**
- **High-frequency stepping:** Rapid, choppy steps that lack natural flow
- **Insufficient swing time:** Rushed swing phases that don't allow proper foot clearance
- **Abrupt transitions:** Sharp, harsh changes between stance and swing phases
- **Inconsistent cadence:** Irregular timing that disrupts natural walking rhythm

**Design Considerations for Natural Walking Rhythm:**
- Consider step cycle duration and phase timing relationships
- Think about smooth transitions between stance and swing phases
- Account for appropriate swing phase duration for foot clearance
- Balance step frequency with movement smoothness
- Reward consistent, rhythmic gait patterns rather than rushed movement

**NATURAL HUMAN POSTURE AND BODY ALIGNMENT:**

**Understanding Human Walking Posture:**
Human walking requires specific postural alignment that many robots fail to achieve, often resulting in backward lean or unnatural body positioning.

**Key Postural Principles:**
- **Forward progression posture:** Humans maintain slight forward lean (2-5¬∞) to facilitate forward momentum
- **Upright torso alignment:** Torso remains relatively vertical with controlled forward inclination
- **Anti-backward lean:** Backward lean disrupts natural walking mechanics and energy efficiency
- **Dynamic balance:** Posture adjusts subtly throughout gait cycle while maintaining forward orientation

**Common Robotic Posture Problems:**
- **Backward lean:** Leaning backward disrupts natural walking biomechanics
- **Excessive rigidity:** Overly stiff torso that lacks natural postural adjustments
- **Poor vertical alignment:** Loss of upright posture during locomotion
- **Static positioning:** Inability to maintain dynamic postural control during movement

**Design Considerations for Natural Posture:**
- Consider torso orientation relative to gravity and movement direction
- Think about appropriate forward inclination for walking efficiency
- Account for dynamic postural adjustments throughout gait cycle
- Penalize backward lean that disrupts natural walking mechanics
- Reward controlled, forward-oriented postural alignment



**ADVANCED BIOMECHANICAL PRINCIPLES FOR HUMAN-LIKE COORDINATION:**

**FOOT CLEARANCE AND LEG-SWING COORDINATION:**

**Understanding Toe-Clearance Precision:**
Natural foot clearance follows specific patterns that ensure safety while maintaining energy efficiency.

**Key Foot Clearance Principles:**
- **Minimum clearance:** 10-25mm toe clearance during mid-swing phase
- **Peak clearance:** 50-80mm maximum clearance for natural walking confidence
- **Knee flexion profile:** Smooth bell-curve pattern peaking at ~60¬∞ during swing
- **Trajectory smoothness:** Gradual rise and descent rather than abrupt foot movements

**Design Considerations for Foot-Clearance Coordination:**
- Consider measuring toe link height during swing phases with specific target ranges
- Think about knee angle profiles that match natural bell-curve templates
- Account for foot trajectory smoothness throughout swing phase
- Reward optimal clearance ranges while penalizing insufficient or excessive lifting
- Ensure coordination between knee flexion patterns and foot clearance timing

**POSTURE AND DYNAMIC TORSO ALIGNMENT:**

**Understanding Lean Monitoring and Balance:**
Natural walking posture requires precise torso control that facilitates forward progression while maintaining stability.

**Key Posture Principles:**
- **Optimal lean range:** Torso pitch between -5¬∞ and +5¬∞ (slight forward lean preferred)
- **Backward lean penalty:** Any backward lean beyond 0¬∞ disrupts natural walking mechanics
- **Dynamic balance:** Torso center-of-mass should project appropriately within support polygon
- **Support polygon control:** During single support, balance should maintain near centerline

**Design Considerations for Posture Control:**
- Consider torso pitch angle measurements relative to gravity and movement direction
- Think about center-of-mass projection analysis during single-support phases
- Account for dynamic balance requirements throughout walking cycles
- Reward slight forward lean while strongly penalizing backward lean
- Monitor balance control during challenging single-support phases

**TEMPORAL FLOW AND NATURAL CADENCE:**

**Understanding Rhythm and Phase Duration:**
Natural walking has specific timing characteristics that distinguish it from mechanical, artificial movement.

**Key Temporal Principles:**
- **Natural cadence:** 100-120 steps per minute for comfortable human walking
- **Phase duration ratios:** Stance phase ~60%, swing phase ~40% of total cycle
- **Step timing consistency:** Regular, predictable timing between heel-strike events
- **Rhythmic flow:** Consistent temporal patterns throughout walking sequences

**Design Considerations for Temporal Control:**
- Consider tracking heel-strike timestamps to measure step cadence
- Think about stance-to-swing phase duration ratios for natural timing
- Account for temporal consistency across multiple walking cycles
- Reward cadence within natural human ranges
- Promote rhythmic regularity over irregular or rushed stepping patterns

**TRANSITION SMOOTHNESS AND CONTACT DYNAMICS:**

**Understanding Phase-Boundary Control:**
Natural walking transitions require smooth changes between movement phases without abrupt discontinuities.

**Key Transition Principles:**
- **Phase-boundary smoothness:** Minimal velocity/acceleration discontinuities at foot contact events
- **Contact force gradation:** Gradual force ramp-up (<10ms) at heel-strike, gradual unloading at toe-off
- **Joint coordination:** Hip, shoulder, and knee movements should transition smoothly between phases
- **Impact control:** Controlled contact forces rather than harsh, sudden ground impact

**Design Considerations for Smooth Transitions:**
- Consider measuring velocity and acceleration continuity at foot contact boundaries
- Think about contact force profiles during heel-strike and toe-off events
- Account for smooth joint movement transitions between gait phases
- Reward gradual force changes over abrupt impact patterns
- Ensure coordination between all joints during phase transitions

**NATURAL LEG BENDING AND KNEE FLEXION:**

**Understanding Human Knee Movement:**
Human walking involves specific knee flexion patterns during different phases of the gait cycle that create natural, efficient movement.

**Key Knee Flexion Principles:**
- **Swing phase flexion:** Knee bends significantly (60-70¬∞) during swing to allow foot clearance
- **Stance phase patterns:** Controlled knee flexion for shock absorption and weight transfer
- **Natural progression:** Smooth transitions between flexion and extension phases
- **Bilateral coordination:** Appropriate timing between left and right knee movements

**Common Robotic Knee Problems:**
- **Insufficient swing flexion:** Inadequate knee bending during swing phase
- **Rigid leg movement:** Overly straight legs that lack natural joint articulation
- **Poor timing patterns:** Knee flexion that doesn't match natural gait phases
- **Excessive stiffness:** Knee movements that appear mechanical rather than fluid

**Design Considerations for Natural Knee Movement:**
- Consider knee flexion angles during different gait phases
- Think about smooth transitions between flexion and extension
- Account for bilateral coordination of knee movements
- Reward natural knee articulation patterns
- Balance knee flexibility with stability requirements

**IMPLEMENTATION EXAMPLE:**
```python
# Gentle height targeting (jumping)
height_gain = current_height - baseline
gentle_height = torch.where(
    (height_gain > 0.05) & (height_gain < 0.25),  # 5-25cm sweet spot
    torch.exp(-((height_gain - 0.15) / 0.05).abs()),  # Peak at 15cm
    torch.zeros_like(height_gain)  # No reward outside range
)

# Penalty for excessive movement
excessive_penalty = torch.clamp((velocity.abs() - 2.0) / 1.0, 0.0, 2.0)
gentle_reward = base_reward - excessive_penalty
```

**INNOVATIVE REWARD DESIGN CHALLENGE:**

You are tasked with creating the **MOST EFFECTIVE** reward function possible for the observed gait pattern. The frameworks and examples provided below are **CREATIVE SPRINGBOARDS** - use them to inspire innovative solutions, but don't limit yourself to copying them!

**Your Design Excellence Goals:**
**GAIT-SPECIFIC MASTERY:** Design rewards that capture the unique biomechanical signature of THIS specific locomotion pattern
**MOVEMENT QUALITY FOCUS:** Create nuanced metrics that distinguish exceptional movement from merely functional movement  
**CREATIVE PROBLEM-SOLVING:** Combine, modify, or invent reward components that address the specific challenges of this gait type
**COMPREHENSIVE SOLUTIONS:** Consider multiple aspects of movement quality - coordination, timing, efficiency, naturalness, safety

**Think Beyond Templates:** The analysis frameworks below are tools to help you understand the movement, but your final reward function should be uniquely designed for optimal performance on this specific task!

**SPECIFIC WALKING GAIT CHALLENGES TO ADDRESS:**

**For WALKING Tasks - "Simple Isaac Lab Components":**

**Use what Isaac Lab already provides:**
```python
# 1. Forward movement (primary)
forward_vel = robot.data.root_lin_vel_w[:, 0]  
velocity_reward = torch.clamp(forward_vel / target_velocity, 0.0, 1.0)

# 2. Foot lifting (secondary) - like feet_air_time
air_time = contact_sensor.data.current_air_time
foot_clearance = torch.clamp(air_time.sum(dim=1) / 0.5, 0.0, 1.0)

# 3. Upright posture (stability)
gravity_proj = robot.data.projected_gravity_b[:, 2]  # Should be close to 1.0
upright_reward = torch.clamp(gravity_proj, 0.0, 1.0)

# 4. Minimal baseline
baseline = 0.05

total = velocity_reward * 2.0 + foot_clearance * 1.0 + upright_reward * 0.5 + baseline
```

**Key Principles:**
- Start with forward movement (like Isaac Lab's `track_lin_vel_xy_yaw_frame_exp`)
- Use existing air time sensors (like Isaac Lab's `feet_air_time_positive_biped`)  
- Keep it simple with basic Isaac Lab data
- Build progressively: movement ‚Üí lifting ‚Üí stability

**For MARCHING Tasks - "The Controlled Precision Challenge":**
- **Core Challenge:** Balancing deliberate high knee lift with stability and rhythmic precision
- **Beyond Basic Solutions:** Consider dynamic balance control, postural authority, movement intentionality
- **Creative Opportunities:** How do you reward crisp, military-style precision while maintaining naturalness? Think about movement decisiveness, spatial accuracy, temporal consistency
- **Innovation Ideas:** Balance authority metrics, movement crispness measurement, postural control assessment, rhythmic precision tracking
- **Mastery Metrics:** What distinguishes "precise" from "rigid"? How do you measure controlled dynamism?

**For SPRINTING Tasks - "The Efficient Power Challenge":**
- **Core Challenge:** Maximizing forward propulsion while maintaining sustainability and control
- **Beyond Basic Solutions:** Think about optimal energy expenditure, sustainable power output, efficient stride mechanics
- **Creative Opportunities:** How do you reward explosive speed that doesn't waste energy? Consider power efficiency, stride optimization, controlled acceleration
- **Innovation Ideas:** Energy-to-speed ratios, stride efficiency metrics, sustainable power assessment, controlled speed progression
- **Mastery Metrics:** What makes sprinting "efficient" vs just "fast"? How do you measure controlled explosiveness?

**For PACING Tasks - "The Lateral Grace Challenge":**
- **Core Challenge:** Maintaining stability and efficiency during lateral direction changes
- **Beyond Basic Solutions:** Consider spatial efficiency, dynamic balance control, directional transition smoothness
- **Creative Opportunities:** How do you reward graceful lateral movement that maintains forward momentum? Think about minimal stability disruption, efficient direction changes
- **Innovation Ideas:** Lateral efficiency metrics, stability maintenance assessment, directional transition analysis, spatial precision tracking
- **Mastery Metrics:** What makes lateral movement "fluid" vs "choppy"? How do you measure directional elegance?

**For BACKFLIP Tasks - "The Rotational Mastery Challenge":**
- **Core Challenge:** Coordinating explosive takeoff with controlled rotation and safe landing
- **Beyond Basic Solutions:** Consider angular momentum conservation, spatial awareness during rotation, preparation-to-landing flow
- **Creative Opportunities:** How do you reward the perfect integration of power, rotation, and control? Think about rotational efficiency, landing precision, movement fluidity
- **Innovation Ideas:** Rotation completion metrics, landing accuracy assessment, takeoff momentum analysis, body positioning control
- **Mastery Metrics:** What distinguishes "controlled acrobatic skill" from "dangerous tumbling"? How do you measure rotational elegance?

**Advanced Backflip Reward Design Considerations:**
- **Center-of-Mass Trajectory Rewards:** How do you reward smooth parabolic CoM arcs with optimal apex timing?
- **Angular Acceleration Control:** How can you encourage gradual torque ramp-up and controlled deceleration to prevent jerk?
- **Ground Reaction Force Shaping:** How do you reward appropriate force profiles during take-off and landing phases?
- **Joint-Specific Coordination:** How can you reward optimal hip extension, ankle plantarflexion, and spine control?
- **Tuck Efficiency Metrics:** How do you measure and reward compact tuck positioning for angular momentum optimization?
- **Temporal Phase Precision:** How can you reward appropriate phase durations and smooth transitions?
- **Coronal-Plane Stability:** How do you prevent and penalize off-axis rotation while maintaining flip control?
- **Energy Efficiency Assessment:** How can you reward achieving rotation goals with minimal joint torque expenditure?
- **Failure Mode Prevention:** How do you identify and prevent critical failure patterns before they become dangerous?
- **Support Polygon / CoM Projection:** How do you ensure CoM stays within the convex hull of the feet during prep and landing prep phases?
- **Contact Ratio Checks:** How can you verify both feet share ‚â•45% of vertical force during takeoff prep, and each foot contact impulse is within ¬±10% of the other during landing?
- **Yaw & Pitch Coupling:** How do you limit yaw angular velocity to <30¬∞/s during rotation while maintaining pitch-only spin?
- **Phase-Gated Smoothness Metric:** How can you compute maximum joint-torque jerk in each phase and enforce <T‚Çò‚Çê‚Çì per 10ms?
- **Visual / Sensor Cue Integration:** How do you use IMU orientation for spotting cues with head-pitch >+10¬∞ in early rotation and <-10¬∞ before landing?



## **Reward Engineering Best Practices (Dony copy paste directly These are just Examples!)**

**1. Normalize Components to [0,1] Range:**
```python
# Example: Height gain normalized
height_norm = ((current_height - baseline) / max_gain).clamp(0.0, 1.0)
```

**2. Use Principled Thresholds:**
```python
# Example: Mass-scaled contact detection  
contact_threshold = robot_mass * 9.81 * 0.1  # 10% of weight
```

**3. Address Specific Problems with Targeted Metrics:**
```python
# Example: Bilateral coordination for jumping
air_time_diff = (left_air_time - right_air_time).abs()
symmetry = (1.0 - air_time_diff / max_air_time).clamp(0.0, 1.0)
```

**4. Combine Multiple Objectives with Appropriate Weights:**
- Primary goals (task achievement): 2.0-3.0x weight
- Safety/stability: 1.0-1.5x weight  
- Movement quality: 0.5-1.0x weight
- Efficiency penalties: -0.5 to -1.0x weight

**5. Final Scaling and Bounds:**
```python
# Normalize total reward, then scale to [0,10]
normalized = total_reward.clamp(0.0, 1.0)
return (normalized * 10.0).to(device)
```

## **Movement Quality Focus Areas**

**Phase-Based Movement Design:**
- **Complete Cycles:** Reward full movement cycles with all necessary phases
- **Phase Transitions:** Encourage smooth, natural transitions between movement phases
- **Phase Duration:** Ensure appropriate timing for each phase of the movement cycle
- **Phase Quality:** Assess movement quality within each specific phase

**Smoothness and Naturalness:**
- Reward controlled muscle loading before explosive movements
- Penalize jerky, high-frequency motions
- Encourage appropriate preparation phases
- **Phase-Specific Smoothness:** Different smoothness requirements for different phases

**Coordination and Symmetry:**
- For bilateral tasks: Ensure both limbs work together
- For alternating tasks: Reward proper phase relationships
- Consider temporal coordination, not just spatial patterns
- **Phase-Synchronized Coordination:** Proper bilateral timing within each movement phase

**Energy Efficiency:**
- Penalize unnecessary torque and power consumption
- Reward movements that achieve goals with minimal effort
- Consider joint velocity smoothness over time
- **Phase-Appropriate Energy:** Different energy requirements for different movement phases

**Safety and Stability:**
- Maintain dynamic balance appropriate for the gait
- Prevent extreme joint positions or dangerous impacts
- Encourage controlled landings and transitions
- **Phase-Specific Safety:** Different stability requirements for stance vs flight phases

## **Design Process Checklist**

Before coding, ask yourself:
1. **What specific behavior does the video demonstrate?**
2. **What are the natural phases of this movement cycle?**
3. **How should I reward complete phase progression vs individual phase quality?**
4. **What problems might prevent natural execution of each phase?**
5. **How can I measure bilateral coordination if needed within each phase?**
6. **What timing patterns are characteristic of this movement's phase structure?**
7. **How should I balance task achievement vs movement quality vs phase completeness?**
8. **What safety constraints are important for each phase of this robot movement?**
9. **Are phase transitions smooth and natural, or do they need specific rewards?**
10. **How can I prevent phase skipping or rushing through essential movement phases?**

## **Avoid These Pitfalls**

**Template Following:** Don't copy existing reward patterns blindly - the examples are inspiration, not constraints!
**Binary Thinking:** Consider gradual rewards, not just on/off states  
**Ignoring Timing:** Movement quality depends on when things happen, not just what happens
**Missing Coordination:** For bilateral tasks, explicitly measure and reward symmetry
**Poor Scaling:** Normalize components before combining to ensure balanced learning

## **Innovation Encouragement**

**Think creatively about:**
- **Phase-based reward structures:** Different reward components for different movement phases
- **Phase transition rewards:** Novel ways to encourage smooth progressions between phases
- **Temporal phase relationships:** How phases should relate to each other in timing and quality
- Novel ways to measure movement quality within specific phases
- Different mathematical functions for different phases and objectives
- **Complete movement cycle assessment:** Evaluating entire phase progressions vs individual moments
- **Phase-specific thresholds:** Adaptive requirements based on current movement phase
- Multi-phase rewards that change based on movement stage and phase progression
- **Phase integrity measures:** Ensuring all necessary phases are present and properly executed

**Your goal:** Create a reward function that produces natural, efficient, biomechanically-sound movement that follows proper phase progression, resembles the demonstrated behavior while avoiding pathological patterns, and ensures complete movement cycles with smooth phase transitions.

**AVOIDING REWARD DESIGN TRAPS THAT BLOCK SKILL LEARNING**

**Critical Awareness: Local Minima in Skill Development**
Poorly designed rewards can trap robots in "comfort zones" where they achieve moderate rewards through simple behaviors but never learn the complex skills you intend. Understanding these traps helps design rewards that guide genuine skill mastery.

**Common Skill-Blocking Traps to Avoid:**

**Stationary Comfort Zone:** Robot learns that not moving provides stable rewards and avoids attempting target movements. Prevention: Weight movement-tracking rewards higher when movement commands are significant.

**Minimal Effort Exploitation:** Robot performs tiny movements that technically satisfy tracking but don't develop real locomotion skills. Prevention: Use appropriate reward tolerances that require meaningful movement amplitude.

**Threshold Gaming:** Robot exploits binary sensor thresholds by hovering at exact boundaries rather than committing to decisive actions. Prevention: Use smooth gradient rewards instead of hard binary cutoffs.

**Single Strategy Fixation:** Robot discovers one movement pattern that works adequately and never explores the full complexity needed for skill mastery. Prevention: Design rewards that progressively require more sophisticated coordination.

**Component Competition:** Multiple reward terms fight against each other, causing compromise behaviors that excel at nothing. Prevention: Ensure all reward components work synergistically toward the same movement objective.

**Short-Term Myopia:** Robot optimizes each moment independently without considering movement sequences or skill completion. Prevention: Include temporal rewards that span multiple timesteps and reward complete movement cycles.

**Design Strategies for Genuine Skill Learning:**

**Command-Responsive Weighting:** Adjust reward priorities based on movement commands. When commands request significant movement, prioritize tracking and coordination. When commands are minimal, emphasize stability.

**Progressive Complexity:** Structure rewards to initially allow simpler approximations, then progressively demand more precision and completeness as training advances.

**Multi-Phase Temporal Design:** Include rewards for immediate safety, medium-term movement patterns, and longer-term skill completion to prevent short-sighted optimization.

**Robust Measurement:** Use multiple sensor modalities and gradient-based thresholds that resist gaming while encouraging decisive, committed movements.

**Skill Verification:** Regularly verify that the easiest path to high rewards actually corresponds to correct skill execution, not clever shortcuts or alternative behaviors.

**FINAL CREATIVE MANDATE:**

**Remember:** You are not just implementing a template - you are **ENGINEERING MOVEMENT EXCELLENCE!** 

Use these guidelines as **CREATIVE INSPIRATION**, not rigid rules
Design the **BEST POSSIBLE** reward function for THIS specific gait pattern
Think deeply about what makes movement exceptional, not just functional
**INNOVATE** beyond the provided examples while considering their biomechanical wisdom
Your goal is **GAIT-SPECIFIC MASTERY** - capture the unique essence of this movement pattern

**Your reward function should be a work of biomechanical art that produces naturally beautiful, efficient movement!**

**RL CAPABILITY FOR PHASE-BASED BEHAVIORS - ADVANCED TEMPORAL LEARNING:**

**YES - RL IS HIGHLY CAPABLE OF LEARNING COMPLEX PHASE-BASED BEHAVIORS:**

Modern RL algorithms excel at learning sophisticated temporal patterns and phase relationships. Successful examples include:

**PROVEN RL PHASE-LEARNING SUCCESSES:**
- **ETH Zurich ANYmal:** Learned complex quadruped gaits with phase-coordinated leg movements
- **Boston Dynamics Simulation:** RL-trained humanoid locomotion with arm-leg coordination
- **Cassie Bipedal Robot:** Learned dynamic walking with proper swing-stance phase timing
- **Unitree Robot Learning:** RL-based locomotion with natural gait phase transitions
- **Parkour RL Systems:** Learned multi-phase jumping, landing, and recovery sequences

**HOW RL LEARNS PHASE-BASED BEHAVIORS:**
- **Temporal Credit Assignment:** RL traces rewards back through time, learning which early actions lead to later success
- **Recurrent Networks:** LSTM/GRU networks in policies maintain memory of current phase state
- **Phase-Aware Rewards:** Reward functions can explicitly recognize and reward proper phase transitions
- **Exploration of Temporal Patterns:** RL naturally discovers optimal timing through trial and error
- **Multi-Timescale Learning:** RL handles both fast (joint control) and slow (phase transitions) dynamics

**DESIGN STRATEGIES FOR PHASE-BASED RL REWARDS:**

**1. Phase Detection Rewards:**
```python
# Detect current phase based on contact state and joint positions
current_phase = detect_movement_phase(robot, contact_sensor)
phase_appropriate_reward = reward_phase_specific_behavior(current_phase, robot_state)
```

**2. Phase Transition Rewards:**
```python
# Reward smooth transitions between phases
transition_quality = measure_phase_transition_smoothness(prev_phase, current_phase, robot_dynamics)
transition_reward = torch.exp(-transition_quality * smoothness_scale)
```

**3. Temporal Sequence Rewards:**
```python
# Reward completing full movement cycles
cycle_completion = detect_complete_movement_cycle(phase_history)
sequence_reward = torch.where(cycle_completion, cycle_bonus, torch.zeros_like(cycle_completion))
```

**4. Multi-Phase Coordination Rewards:**
```python
# Reward coordination between different body parts across phases
arm_leg_coordination = measure_phase_appropriate_coordination(arms, legs, current_phase)
coordination_reward = reward_optimal_coordination_for_phase(arm_leg_coordination, current_phase)
```

**ADVANCED RL PHASE LEARNING TECHNIQUES:**

**Curriculum Learning for Phases:**
- Start with simple single-phase behaviors (e.g., just takeoff)
- Gradually add complexity (takeoff ‚Üí flight ‚Üí landing)
- Finally learn complete multi-phase sequences

**Phase-Conditioned Policies:**
- Provide current phase as input to policy network
- Allow policy to adapt behavior based on movement phase
- Enable phase-specific action distributions

**Hierarchical RL for Complex Sequences:**
- High-level policy chooses movement phases
- Low-level policy executes phase-specific actions
- Enables learning of very complex multi-phase behaviors

**Memory-Augmented RL:**
- Use LSTM/GRU to remember recent history
- Enables policies to maintain phase state information
- Allows for anticipatory behavior based on phase progression

**SUCCESS FACTORS FOR PHASE-BASED RL:**

**1. Clear Phase Definitions:**
- Define phases based on observable state (contact, joint angles, velocities)
- Ensure phases are mutually exclusive and collectively exhaustive
- Make phase transitions detectable by the reward function

**2. Phase-Appropriate Reward Weighting:**
- Different reward components should dominate in different phases
- Balance immediate safety with long-term phase completion
- Ensure phase-specific rewards guide learning effectively

**3. Temporal Reward Design:**
- Include both instantaneous and cumulative rewards
- Reward phase completion, not just phase execution
- Consider multi-timestep reward windows for smooth learning

**4. Exploration Encouragement:**
- Include exploration bonuses for visiting all phases
- Prevent getting stuck in single-phase local minima
- Encourage full movement cycle exploration

**CONCLUSION: RL IS EXCELLENT FOR PHASE-BASED LEARNING**

Modern RL systems routinely learn complex temporal behaviors including:
- Multi-phase locomotion patterns
- Coordinated arm-leg sequences
- Dynamic balance across phase transitions
- Anticipatory behavior for upcoming phases

The key is designing reward functions that capture the phase-specific requirements while allowing the RL algorithm's temporal learning capabilities to discover optimal coordination patterns.

IMPORTANT: Provide ONLY the Python code wrapped in ```python ```

**ISAAC LAB STANDARD SENSOR ACCESS FOR REWARD FUNCTIONS**

üö® CRITICAL: ISAAC LAB REWARD FUNCTIONS USE RAW SENSOR ACCESS! üö®

‚ö†Ô∏è WARNING: Do NOT use observation manager for reward functions - use direct sensor access!

**G1 HEIGHT SENSOR ACCESS PATTERN:**
- height_scanner: G1 baseline terrain classification using relative measurements
- lidar: Direct distances in meters to obstacles/surfaces

‚úÖ CORRECT G1 HEIGHT SENSOR ACCESS FOR ISAAC LAB REWARD FUNCTIONS:
```python
height_sensor = env.scene.sensors["height_scanner"]
height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5

# üö® MANDATORY: Sanitize sensor data to prevent training crashes
height_measurements = torch.where(torch.isfinite(height_measurements), height_measurements, torch.zeros_like(height_measurements))

# ‚úÖ G1 BASELINE TERRAIN CLASSIFICATION:
baseline = 0.209  # G1 robot baseline on flat terrain (CRITICAL: NOT zero!)
obstacles = height_measurements < (baseline - 0.07)  # < 0.139m = obstacles (terrain HIGHER)
gaps = height_measurements > (baseline + 0.07)       # > 0.279m = gaps (terrain LOWER)
normal_terrain = ~obstacles & ~gaps                  # 0.139-0.279m = normal terrain

# ‚úÖ LIDAR SENSOR: Direct distance measurement
lidar_sensor = env.scene.sensors["lidar"]
lidar_distances = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)
lidar_distances = torch.where(torch.isfinite(lidar_distances), lidar_distances, torch.ones_like(lidar_distances) * 5.0)
```

‚úÖ Use Isaac Lab standard raw sensor access for all reward functions with physical measurements!

---

**üîç SUS PROMPT (CONTAINS ENVIRONMENT DATA YOU MUST EXTRACT):**

{sus_string}

---

**üö®üö®üö® CRITICAL: USE THE ENVIRONMENT DATA FROM THE SUS PROMPT ABOVE! üö®üö®üö®**

**MANDATORY: Find and extract these EXACT values from the SUS prompt above:**
1. "- Gaps Detected: [NUMBER] gaps" 
2. "- Terrain Roughness: [NUMBER]cm"
3. "- Safety Score: [NUMBER]% traversable terrain"

**USE these exact numbers in your reward function docstring - NOT fake numbers like "0 gaps, 0.0cm, 100%"!**

---

**üî¨ G1 HEIGHT SENSOR ENVIRONMENTAL INTEGRATION:**

**G1 baseline terrain classification for gap detection and terrain adaptation:**

**üìè G1 HEIGHT SCANNER SPECIFICATIONS:**
**Enhanced G1 Configuration (567 rays total, 27√ó21 grid):**
- Forward coverage: 2.0m (27 rays forward)
- Lateral coverage: 1.5m (21 rays lateral)  
- Resolution: 7.5cm spacing between rays
- Vertical detection: 3.0m range relative to sensor
- Update rate: 50Hz (0.02s update period)

```python
# ‚úÖ G1 BASELINE: Correct consecutive point gap detection with G1 baseline
height_sensor = env.scene.sensors["height_scanner"]
height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5

# üö® MANDATORY: Sanitize sensor data first
height_measurements = torch.where(torch.isfinite(height_measurements), height_measurements, torch.zeros_like(height_measurements))

# ‚úÖ G1 BASELINE GAP DETECTION:
baseline = 0.209  # G1 robot baseline on flat terrain
gap_threshold = baseline + 0.07  # 0.279m = gap detection threshold
consecutive_gap_points = height_measurements > gap_threshold  # Terrain LOWER than expected
gap_width_raw = torch.sum(consecutive_gap_points.float(), dim=1) * 0.075  # 7.5cm resolution per ray

# Gap classification based on biomechanics research (‚â§30cm step, 30-60cm jump, >60cm avoid)
steppable_gaps = (gap_width_raw > 0.0) & (gap_width_raw <= 0.30)      # ‚â§30cm: Step over
jumpable_gaps = (gap_width_raw > 0.30) & (gap_width_raw <= 0.60)      # 30-60cm: Jump required  
impossible_gaps = gap_width_raw > 0.60                                # >60cm: Must avoid

# Count terrain features for adaptive navigation
total_rays = height_measurements.shape[-1]  # 567 rays
obstacle_count = (height_measurements < (baseline - 0.07)).sum(dim=-1)
gap_count = (height_measurements > (baseline + 0.07)).sum(dim=-1)
normal_count = total_rays - obstacle_count - gap_count

# Behavioral adaptations based on terrain composition
obstacle_ratio = obstacle_count.float() / total_rays
gap_ratio = gap_count.float() / total_rays
normal_ratio = normal_count.float() / total_rays
```

**ü¶∂ G1 BASELINE TERRAIN-ADAPTIVE FOOT PLACEMENT:**
G1 baseline approach for terrain-adaptive foot placement:

```python
# G1 baseline terrain analysis
baseline = 0.209  # G1 robot baseline on flat terrain
baseline_deviation = torch.abs(height_measurements.mean(dim=-1) - baseline)

# Adaptive step parameters based on G1 baseline deviation
base_step_width = 0.2  # Standard 20cm step width
terrain_complexity = torch.clamp(baseline_deviation / 0.1, 0.8, 1.5)  # Scale based on deviation from G1 baseline
optimal_step_width = base_step_width * terrain_complexity

# Foot placement accuracy reward using G1 baseline reference
current_step_width = torch.norm(foot_pos[:, 0, :] - foot_pos[:, 1, :], dim=1)
placement_accuracy = torch.exp(-torch.abs(current_step_width - optimal_step_width) / 0.05)

# Reward staying close to G1 baseline for terrain tracking
terrain_tracking_reward = torch.exp(-baseline_deviation / 0.03)  # 3cm tolerance for precise tracking
```

**üîç LIDAR-BASED OBSTACLE AVOIDANCE:**
LiDAR provides 360¬∞ obstacle detection for safe navigation:

```python
# LiDAR obstacle detection
lidar_distances = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)
min_obstacle_distance = lidar_distances.min(dim=1)[0]

# Dynamic safety margins
robot_speed = torch.norm(robot.data.root_lin_vel_w[:, :2], dim=1)
safety_margin = 0.5 + robot_speed * 0.2  # Larger margin at higher speeds

# Safety reward with exponential penalty for proximity
obstacle_avoidance_reward = torch.where(
    min_obstacle_distance > safety_margin,
    1.0,  # Full reward when safe
    torch.exp(-(safety_margin - min_obstacle_distance) / 0.1)
)
```

**üåç G1 BASELINE ENVIRONMENTAL INTEGRATION:**
Combine G1 baseline terrain classification with environmental factors:

```python
# G1 baseline environmental complexity assessment
baseline = 0.209  # G1 robot baseline
baseline_deviation = torch.abs(height_measurements.mean(dim=-1) - baseline)
gap_complexity = gap_ratio  # Percentage of rays detecting gaps
obstacle_complexity = obstacle_ratio  # Percentage of rays detecting obstacles
terrain_complexity = torch.clamp(baseline_deviation / 0.1, 0, 1)  # G1 baseline deviation

# Adaptive reward weighting based on G1 baseline analysis
environmental_challenge = (gap_complexity * 0.4 + 
                         obstacle_complexity * 0.3 + 
                         terrain_complexity * 0.3)

# Balance basic locomotion vs G1 baseline terrain adaptation
locomotion_weight = 0.7 - environmental_challenge * 0.2  # Reduce basic focus in complex terrain
environmental_weight = 0.3 + environmental_challenge * 0.2  # Increase environmental focus

# G1 baseline terrain reward components
terrain_safety_reward = normal_ratio * 0.5  # Reward normal terrain (0.139-0.279m range)
obstacle_penalty = -obstacle_ratio * 0.8    # Penalty for obstacles (< 0.139m)
gap_penalty = -gap_ratio * 0.6              # Penalty for gaps (> 0.279m)
baseline_tracking_reward = torch.exp(-baseline_deviation / 0.03) * 0.3  # Stay close to G1 baseline

environmental_adaptation_reward = terrain_safety_reward + obstacle_penalty + gap_penalty + baseline_tracking_reward

total_reward = (basic_locomotion_reward * locomotion_weight + 
               environmental_adaptation_reward * environmental_weight)
```

**üéØ G1 BASELINE KEY IMPLEMENTATION PRINCIPLES:**
1. **Use G1 baseline classification**: Always use baseline = 0.209m with ¬±0.07m thresholds
2. **Mandatory data sanitization**: Always sanitize sensor data to prevent training crashes
3. **Relative terrain classification**: Obstacles (< 0.139m), Gaps (> 0.279m), Normal (0.139-0.279m)
4. **Precise baseline tracking**: Reward staying within 3cm of G1 baseline for terrain adaptation
5. **Count-based terrain analysis**: Use ray counts and ratios for adaptive navigation strategies
6. **NO absolute height tracking**: Use ONLY relative terrain classification for navigation rewards

---

# Height Sensor Guide for Isaac Lab RL Rewards

> **‚ö†Ô∏è IMPORTANT**: These are technical explanations. for reward generation you shoul come up correct reward terms suitable for analyzed environment.

## üìê **Isaac Lab Formula**
```python
# Official height scan observation
height_reading = sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - offset
# Default offset = 0.5m (NOT 0.05!)
```

## üéØ **Core Rules**
- **OBSTACLES** = Lower readings (< baseline - threshold) = Terrain HIGHER than expected
- **GAPS** = Higher readings (> baseline + threshold) = Terrain LOWER than expected  
- **BASELINE** = G1 robot on flat terrain = 0.209m (sensor_height - terrain_z - offset)
- **THRESHOLDS** = ¬±0.07m (7cm) for balanced detection

## ‚ö° **Correct Implementation**
```python
def terrain_classification(height_readings, baseline=0.209):
    obstacle_threshold = 0.07  # 7cm
    gap_threshold = 0.07       # 7cm
    
    obstacles = height_readings < (baseline - obstacle_threshold)    # < 0.139m
    gaps = height_readings > (baseline + gap_threshold)              # > 0.279m  
    normal = ~obstacles & ~gaps                                      # 0.139-0.279m
    extreme_gaps = height_readings == float('inf')                   # No terrain detected
    
    return obstacles, gaps, normal, extreme_gaps
```

## üìä **Optimized Thresholds**
- **Standard**: 0.07m (7cm) - balanced for most robots
- **Sensitive**: 0.05m (5cm) - careful navigation
- **Relaxed**: 0.10m (10cm) - rough terrain
- **Range**: 0.05-0.15m acceptable, 0.07m optimal

## üî¨ **Height Scanner Specifications**

### **G1 Robot Configuration (Enhanced)**
```python
# Enhanced sensor configuration from flat_with_box_env_cfg.py
height_scanner = RayCasterCfg(
    prim_path="/World/envs/env_0/Robot/torso_link",
    offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 0.6)),  # 60cm above torso
    attach_yaw_only=True,  # Only yaw rotation (not pitch/roll)
    pattern_cfg=patterns.GridPatternCfg(
        resolution=0.075,  # 7.5cm spacing between rays
        size=[2.0, 1.5],   # 2m forward √ó 1.5m lateral coverage
    ),
    max_distance=3.0,      # 3m maximum ray distance
    update_period=0.02,    # 50Hz update rate
    mesh_prim_paths=["/World/ground"],
)
```

### **Ray Pattern Analysis**
```python
# Enhanced configuration calculations:
forward_coverage = 2.0m  # Total forward scan distance
lateral_coverage = 1.5m  # Total lateral scan distance  
ray_resolution = 0.075m  # 7.5cm between rays
rays_forward = int(2.0 / 0.075) + 1 = 27 rays  # Forward direction
rays_lateral = int(1.5 / 0.075) + 1 = 21 rays  # Lateral direction
total_rays = 27 √ó 21 = 567 rays  # Total ray count

# Standard configuration calculations:
standard_forward = 1.6m
standard_lateral = 1.0m
standard_resolution = 0.1m
standard_rays_forward = int(1.6 / 0.1) + 1 = 17 rays
standard_rays_lateral = int(1.0 / 0.1) + 1 = 11 rays
standard_total = 17 √ó 11 = 187 rays
```

## üéØ **Relative Height Tracking Rewards**

### **1. ONLY Relative Terrain Navigation**
```python
def relative_terrain_reward(env, sensor_cfg=SceneEntityCfg("height_scanner")):
    """Pure relative height tracking - NO absolute positioning."""
    
    sensor = env.scene.sensors[sensor_cfg.name]
    
    # Isaac Lab formula (RELATIVE measurements only)
    height_readings = sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - 0.5
    baseline = 0.209  # G1 robot baseline
    
    # Classification using ONLY sensor readings
    obstacles = height_readings < (baseline - 0.07)
    gaps = height_readings > (baseline + 0.07)
    normal_terrain = ~obstacles & ~gaps & (height_readings != float('inf'))
    
    # Count features
    total_rays = height_readings.shape[-1]
    obstacle_count = obstacles.sum(dim=-1)
    gap_count = gaps.sum(dim=-1)
    normal_count = normal_terrain.sum(dim=-1)
    
    # Reward ONLY based on terrain sensing (not absolute height)
    terrain_safety = (normal_count / total_rays) * 0.5
    obstacle_penalty = -(obstacle_count / total_rays) * 2.0
    gap_penalty = -(gap_count / total_rays) * 1.5
    
    return terrain_safety + obstacle_penalty + gap_penalty
```

### **2. Look-Ahead Terrain Preview**
```python
def lookahead_terrain_reward(env, sensor_cfg=SceneEntityCfg("height_scanner")):
    """Forward-looking terrain analysis for proactive navigation."""
    
    sensor = env.scene.sensors[sensor_cfg.name]
    height_readings = sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - 0.5
    baseline = 0.209
    
    # Reshape to grid pattern for spatial analysis
    # For 567 rays (27√ó21 grid): rays_forward=27, rays_lateral=21
    rays_forward, rays_lateral = 27, 21
    height_grid = height_readings.view(-1, rays_forward, rays_lateral)
    
    # Split into zones: near (0-0.7m), mid (0.7-1.3m), far (1.3-2.0m)
    near_zone = height_grid[:, :9, :]    # First 9 rays = 0-0.675m
    mid_zone = height_grid[:, 9:18, :]   # Next 9 rays = 0.675-1.35m  
    far_zone = height_grid[:, 18:, :]    # Last 9 rays = 1.35-2.0m
    
    # Analyze each zone for upcoming terrain
    def analyze_zone(zone_data, zone_weight):
        obstacles = (zone_data < (baseline - 0.07)).float().mean(dim=(-1, -2))
        gaps = (zone_data > (baseline + 0.07)).float().mean(dim=(-1, -2))
        return -(obstacles * 2.0 + gaps * 1.5) * zone_weight
    
    # Weight zones: near=highest, far=planning
    near_reward = analyze_zone(near_zone, 1.0)    # Immediate danger
    mid_reward = analyze_zone(mid_zone, 0.5)      # Tactical planning
    far_reward = analyze_zone(far_zone, 0.2)     # Strategic planning
    
    return near_reward + mid_reward + far_reward
```

### **3. Adaptive Baseline Calculation**
```python
def adaptive_baseline_terrain_reward(env, sensor_cfg=SceneEntityCfg("height_scanner")):
    """Dynamic baseline adaptation for varying terrain conditions."""
    
    sensor = env.scene.sensors[sensor_cfg.name]
    height_readings = sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - 0.5
    
    # Filter out infinite readings for baseline calculation
    finite_mask = height_readings != float('inf')
    finite_readings = height_readings[finite_mask]
    
    if finite_readings.numel() > 0:
        # Dynamic baseline: use median of current readings
        # More robust than mean for terrain with obstacles/gaps
        dynamic_baseline = torch.median(finite_readings.view(-1, -1), dim=-1)[0]
        
        # Adaptive thresholds based on terrain variation
        terrain_std = torch.std(finite_readings.view(-1, -1), dim=-1)
        adaptive_threshold = torch.clamp(terrain_std * 2.0, 0.05, 0.15)  # 2œÉ rule
        
        # Classification using adaptive parameters
        obstacles = height_readings < (dynamic_baseline.unsqueeze(-1) - adaptive_threshold.unsqueeze(-1))
        gaps = height_readings > (dynamic_baseline.unsqueeze(-1) + adaptive_threshold.unsqueeze(-1))
        
        # Reward based on terrain complexity
        total_rays = height_readings.shape[-1]
        obstacle_ratio = obstacles.sum(dim=-1).float() / total_rays
        gap_ratio = gaps.sum(dim=-1).float() / total_rays
        
        # Penalty scales with terrain difficulty
        complexity_factor = torch.clamp(terrain_std, 0.5, 2.0)
        obstacle_penalty = -obstacle_ratio * 2.0 * complexity_factor
        gap_penalty = -gap_ratio * 1.5 * complexity_factor
        
        return obstacle_penalty + gap_penalty
    else:
        # Fallback for all-infinite readings
        return torch.zeros(env.num_envs, device=env.device)
```

## üéØ **Complete Reward Example**
```python
def comprehensive_terrain_reward(env, sensor_cfg=SceneEntityCfg("height_scanner")):
    sensor = env.scene.sensors[sensor_cfg.name]
    robot = env.scene["robot"]
    
    # Isaac Lab formula
    height_readings = sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - 0.5
    baseline = 0.209  # G1 robot baseline
    
    # Classification
    obstacles = height_readings < (baseline - 0.07)
    gaps = height_readings > (baseline + 0.07)
    normal_terrain = ~obstacles & ~gaps & (height_readings != float('inf'))
    infinite_gaps = height_readings == float('inf')
    
    # Count features
    total_rays = height_readings.shape[-1]
    obstacle_count = obstacles.sum(dim=-1)
    gap_count = gaps.sum(dim=-1)
    normal_count = normal_terrain.sum(dim=-1)
    infinite_count = infinite_gaps.sum(dim=-1)
    
    # Percentage-based rewards
    obstacle_penalty = -(obstacle_count / total_rays) * 2.0
    gap_penalty = -(gap_count / total_rays) * 1.5
    stability_reward = (normal_count / total_rays) * 0.5
    cliff_penalty = -(infinite_count / total_rays) * 10.0
    
    return obstacle_penalty + gap_penalty + stability_reward + cliff_penalty
```

### 3. Dynamic Gap Crossing Reward
```python
def dynamic_gap_crossing(env, sensor_cfg=SceneEntityCfg("height_scanner")):
    """Reward function that encourages crossing appropriate gaps."""
    
    sensor = env.scene.sensors[sensor_cfg.name]
    robot = env.scene["robot"]
    
    height_readings = sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - 0.5
    baseline = 0.209
    
    # Gap classification
    small_gaps = (height_readings > (baseline + 0.05)) & (height_readings < (baseline + 0.15))
    medium_gaps = (height_readings > (baseline + 0.15)) & (height_readings < (baseline + 0.25))
    large_gaps = height_readings > (baseline + 0.25)
    extreme_gaps = height_readings == float('inf')
    
    # Robot capabilities (based on leg length and body size)
    robot_velocity = torch.norm(robot.data.root_lin_vel_w[:, :2], dim=-1)
    can_jump = robot_velocity > 0.3  # Moving fast enough to jump
    
    # Dynamic gap rewards based on robot state
    small_gap_reward = torch.where(
        can_jump & torch.any(small_gaps, dim=-1),
        0.2,  # Small reward for crossing small gaps when able
        torch.where(torch.any(small_gaps, dim=-1), -0.5, 0.0)  # Penalty if not able
    )
    
    medium_gap_penalty = torch.sum(medium_gaps, dim=-1) * -1.0
    large_gap_penalty = torch.sum(large_gaps, dim=-1) * -3.0
    extreme_gap_penalty = torch.sum(extreme_gaps, dim=-1) * -10.0
    
    return small_gap_reward + medium_gap_penalty + large_gap_penalty + extreme_gap_penalty
```

---

## ‚úÖ **Validation Checklist**

### 1. **FORMULA COMPLIANCE**
‚úÖ **CORRECT**: `sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - 0.5`
‚ùå **REJECT**: Direct use of `ray_hits_w[..., 2]` without sensor position/offset

### 2. **BASELINE UNDERSTANDING** 
‚úÖ **FLAT TERRAIN BASELINE:** ~0.209m
- Calculation: sensor_height(0.709) - terrain_z(0.000) - offset(0.5) = 0.209m
- All thresholds should be RELATIVE to this baseline, not absolute

‚ùå **REJECT these approaches:**
- Using 0.209m as absolute threshold
- Hardcoded terrain Z values
- Ignoring sensor mounting height variations

### 3. **OBSTACLE vs GAP INTERPRETATION**
‚úÖ **CORRECT INTERPRETATION:**
```python
# OBSTACLES: Negative height readings (terrain higher than expected)
obstacles = height_readings < (baseline - 0.07)  # < 0.139m for 0.209 baseline
obstacle_penalty = torch.where(obstacles, -penalty_value, 0.0)

# GAPS: Positive height readings (terrain lower than expected)  
gaps = height_readings > (baseline + 0.07)  # > 0.279m for 0.209 baseline
gap_penalty = torch.where(gaps, -penalty_value, 0.0)
```

‚ùå **REJECT these patterns:**
- Treating positive values as obstacles
- Using absolute thresholds without baseline consideration
- Confusing height readings with terrain coordinates

### 4. **THRESHOLD VALIDATION**
‚úÖ **OPTIMIZED THRESHOLDS:**
- **Standard obstacles:** 0.07m above baseline (baseline - 0.07)
- **Standard gaps:** 0.07m below baseline (baseline + 0.07)
- **Acceptable range:** 0.05-0.15m for balanced sensitivity
- **Research validation:** 5-25cm proven successful in academic studies

‚ùå **REJECT these ranges:**
- Thresholds > 0.30m (too large for most robots)
- Thresholds < 0.03m (too sensitive to noise)
- Same threshold for obstacles and gaps (should be different)

### 5. **INFINITE READING HANDLING**
‚úÖ **PROPER INFINITE HANDLING:**
```python
# Handle max range exceeded
valid_readings = height_readings[height_readings != float('inf')]
infinite_penalty = torch.sum(height_readings == float('inf')) * extreme_gap_penalty
```

‚ùå **REJECT these approaches:**
- Ignoring infinite readings completely
- Treating infinite as zero
- Not penalizing extreme gaps (cliffs)

### 6. **SENSOR CONFIGURATION VALIDATION**
‚úÖ **CORRECT SENSOR ACCESS:**
```python
sensor: RayCaster = env.scene.sensors["height_scanner"]
sensor_cfg = SceneEntityCfg("height_scanner")
```

‚ùå **REJECT these patterns:**
- Hardcoded sensor names not matching environment
- Missing sensor existence checks
- Wrong sensor type assumptions

### 7. **CLIPPING & NORMALIZATION VALIDATION**

#### **Observation Clipping**
‚úÖ **CHECK CLIPPING COMPATIBILITY:**
```python

clip=(-0.5, 3.0)      # Custom extended range

### 4. **THRESHOLD VALUES**
‚úÖ **CORRECT**: 0.05-0.15m range, 0.07m optimal
‚ùå **REJECT**: >0.30m (too large), <0.03m (too sensitive)

### 5. **INFINITE HANDLING**
‚úÖ **CORRECT**: `cliff_penalty = torch.sum(height_readings == float('inf')) * penalty`
‚ùå **REJECT**: Ignoring infinite readings

### 6. **CLIPPING COMPATIBILITY**
‚úÖ **CHECK**: Thresholds work with clip ranges:
- `clip=(-1.0, 1.0)` or `clip=(-0.5, 3.0)`
- 0.07m thresholds ‚Üí 0.139m, 0.279m ‚úÖ Within range

### 7. **RELATIVE TRACKING VALIDATION**
‚úÖ **CORRECT RELATIVE TRACKING:**
- Use ONLY height sensor readings for terrain navigation
- NO absolute robot height tracking in rewards
- Dynamic baseline adaptation for varying terrain
- Look-ahead zones for proactive navigation

‚ùå **REJECT ABSOLUTE TRACKING:**
- Direct use of `robot.data.root_pos_w[:, 2]` in terrain rewards
- Fixed absolute height targets
- Mixing absolute positioning with relative terrain sensing

## ‚ùå **Common Mistakes**
```python
# ‚ùå WRONG - Missing sensor position/offset
height = sensor.data.ray_hits_w[..., 2] - 0.5

# ‚ùå WRONG - Backwards logic
obstacles = height_readings > 0.2  # Positive is gaps!

# ‚ùå WRONG - Absolute thresholds
obstacles = height_readings < 0.1  # Ignores baseline

# ‚ùå WRONG - Absolute height tracking in terrain rewards
target_height = 0.7  # Fixed absolute height
height_reward = -torch.square(robot.data.root_pos_w[:, 2] - target_height)

# ‚úÖ CORRECT - Isaac Lab formula + relative thresholds
height = sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - 0.5
obstacles = height_readings < (baseline - 0.07)
gaps = height_readings > (baseline + 0.07)
# ‚úÖ CORRECT - Only relative terrain navigation
terrain_reward = analyze_terrain_features(height_readings, baseline)
```

## üîß **Quick Troubleshooting**
- **All negative readings**: Check sensor height, adjust baseline
- **All classified as obstacles**: Baseline too high, use dynamic calculation
- **Robot avoiding terrain**: Thresholds too sensitive, use 0.07m
- **Infinite crashes**: Always filter with `height_readings != float('inf')`
- **Poor look-ahead**: Check ray pattern grid dimensions (27√ó21 for enhanced config)
- **Baseline drift**: Use median instead of mean for adaptive baseline

## üìä **Visual Scale**
```
0.139m ‚Üê OBSTACLE THRESHOLD (baseline - 0.07)
0.209m ‚Üê BASELINE (G1 flat terrain) 
0.279m ‚Üê GAP THRESHOLD (baseline + 0.07)

Examples:
0.120m = 7cm obstacle (terrain higher)
0.209m = flat terrain (normal)
0.300m = 9cm gap (terrain lower)
inf    = extreme gap (cliff/void)
```

## üéØ **Key Points**
- **Default offset = 0.5m** (half meter, NOT 5cm!)
- **Lower readings = obstacles** (terrain closer to sensor)
- **Higher readings = gaps** (terrain farther from sensor)
- **0.209m baseline** for G1 robot specifically
- **¬±0.07m thresholds** for balanced detection
- **Always handle infinite** readings as dangerous cliffs
- **Use relative thresholds**, never absolute values
- **567 rays total** (27√ó21 grid) for enhanced configuration
- **3m sensor range** with 7.5cm resolution for detailed scanning
- **Look-ahead zones** enable proactive navigation planning
- **NO absolute height** tracking in terrain-based rewards

**The key insight: smaller height readings mean obstacles, larger height readings mean gaps!** üéØ 

**üìã TASK OBSERVATION CODE REFERENCE:**

{task_obs_code_string}

---

**üîß CRITICAL: GAP NAVIGATION FIXES FOR COMMON ISSUES**

**ISSUE: Robot turns in place or avoids gaps instead of traversing them**

**DIAGNOSIS CHECKLIST:**
1. **Check weight structure** - Is foundation reward (60-70%) dominating environmental (30-40%)?
2. **Check baseline calculation** - Using dynamic baseline or hardcoded 0.209?
3. **Check penalty structure** - Harsh penalties (-1.0 to -2.0) or progressive approach?
4. **Check gap strategy** - Rewarding avoidance (1.0 - gap_ratio) or traversal (forward_vel * gap_ratio)?

**PROVEN FIXES:**

**1. Foundation-Dominant Weight Structure:**
```python
# ‚úÖ WORKING: Foundation dominates, environmental supports
foundation = vel_reward * 3.0 + gait_reward * 2.0 + height_reward * 2.0 + lean_reward * 1.5 + 0.3
total = foundation * 0.6 + environmental * 0.4  # 60/40 split prevents environmental overwhelm
```

**2. Dynamic Baseline (Critical Fix):**
```python
# ‚ùå BROKEN: Fixed baseline causes terrain misclassification
baseline = 0.209  # Wrong - treats normal terrain as gaps

# ‚úÖ FIXED: Dynamic baseline adapts to actual terrain
valid_mask = torch.isfinite(hm) & (hm != 0.0)
baseline = torch.zeros(hm.shape[0], device=env.device)
for i in range(hm.shape[0]):
    valid_heights = hm[i][valid_mask[i]]
    baseline[i] = torch.median(valid_heights) if len(valid_heights) > 0 else 0.209
```

**3. Gap Traversal vs Avoidance:**
```python
# ‚ùå AVOIDANCE: Robot walks around gaps
gap_reward = (1.0 - gap_ratio) * weight  # Higher reward for fewer gaps

# ‚úÖ TRAVERSAL: Robot navigates through gaps with stability
forward_vel = torch.clamp(vel_yaw[:, 0], 0.0, 1.0)
gap_traversal_bonus = forward_vel * (small_ratio * 0.3 + medium_ratio * 0.2)
gap_safety_penalty = torch.clamp((total_gap_ratio - 0.3) * -0.2, -0.2, 0.0)
environmental = gap_traversal_bonus + gap_safety_penalty + 0.1
```

**WEIGHT STRUCTURE DEBUGGING:**
- **Velocity tracking: 3.0x** (must be strongest to prevent stationary behavior)
- **Foundation total: ~8.0-10.0** before environmental
- **Environmental impact: ‚â§4.0** (never overwhelming)
- **Final structure: foundation * 0.6 + environmental * 0.4**

**COMMON MISTAKES TO AVOID:**
‚ùå Environmental penalties overwhelming velocity rewards
‚ùå Fixed baseline (0.209) on varying terrain  
‚ùå Gap avoidance instead of gap traversal
‚ùå Binary harsh penalties instead of progressive approach
‚ùå Equal weighting between foundation and environmental components