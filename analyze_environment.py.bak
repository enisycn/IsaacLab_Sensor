#!/usr/bin/env python3

"""
RL Policy Height Scanner Monitor
Loads a trained RL policy and monitors terrain height while robot walks.
Uses Isaac Lab's standard policy loading mechanism.
"""

import argparse
import time
import torch
import gymnasium as gym
import numpy as np
import os

from isaaclab.app import AppLauncher

# add argparse arguments
parser = argparse.ArgumentParser(description="RL Policy Height Scanner Monitor")
parser.add_argument("--num_envs", type=int, default=4, help="Number of environments")
parser.add_argument("--task", type=str, default="Isaac-SDS-Velocity-Rough-G1-Enhanced-v0", help="Task name")
parser.add_argument("--checkpoint", type=str, required=True, help="Path to RL checkpoint file")
parser.add_argument(
    "--disable_fabric", action="store_true", default=False, help="Disable fabric and use USD I/O operations."
)
# append AppLauncher cli args
AppLauncher.add_app_launcher_args(parser)
# parse the arguments
args_cli = parser.parse_args()

# launch omniverse app
app_launcher = AppLauncher(args_cli)
simulation_app = app_launcher.app

"""Rest everything follows."""

import isaaclab_tasks  # noqa: F401
from isaaclab_tasks.utils import parse_env_cfg

# Import RSL-RL components (same as play.py)
from rsl_rl.runners import OnPolicyRunner
from isaaclab.envs import DirectMARLEnv, multi_agent_to_single_agent
from isaaclab.utils.assets import retrieve_file_path
from isaaclab_rl.rsl_rl import RslRlOnPolicyRunnerCfg, RslRlVecEnvWrapper

# Import cli_args from the scripts directory
import sys
sys.path.append("scripts/reinforcement_learning/rsl_rl")
import cli_args

def load_rsl_rl_policy(env, task_name, checkpoint_path):
    """Load RSL-RL policy using Isaac Lab's standard method."""
    print(f"ü§ñ Loading RSL-RL policy from: {checkpoint_path}")
    
    try:
        # Parse agent configuration (same as play.py)
        # Create minimal args for cli_args.parse_rsl_rl_cfg
        class MinimalArgs:
            def __init__(self):
                self.experiment_name = None
                self.run_name = None
                self.resume = False
                self.load_run = None
                self.checkpoint = checkpoint_path
                self.logger = None
                self.log_project_name = None
                self.device = args_cli.device
        
        minimal_args = MinimalArgs()
        agent_cfg: RslRlOnPolicyRunnerCfg = cli_args.parse_rsl_rl_cfg(task_name, minimal_args)
        
        # Get the checkpoint path
        resume_path = retrieve_file_path(checkpoint_path)
        log_dir = os.path.dirname(resume_path)
        
        print(f"[INFO]: Loading model checkpoint from: {resume_path}")
        
        # Wrap environment for RSL-RL (same as play.py)
        env_wrapped = RslRlVecEnvWrapper(env, clip_actions=agent_cfg.clip_actions)
        
        # Load policy using OnPolicyRunner (same as play.py)
        ppo_runner = OnPolicyRunner(env_wrapped, agent_cfg.to_dict(), log_dir=None, device=agent_cfg.device)
        ppo_runner.load(resume_path)
        
        # Get inference policy (same as play.py)
        policy = ppo_runner.get_inference_policy(device=env.unwrapped.device)
        
        print(f"‚úÖ RSL-RL policy loaded successfully!")
        
        return policy, env_wrapped
        
    except Exception as e:
        print(f"‚ùå Failed to load RSL-RL policy: {e}")
        import traceback
        traceback.print_exc()
        return None, env

def main():
    """Main monitoring function with RL policy."""
    
    # Parse environment configuration
    env_cfg = parse_env_cfg(
        args_cli.task,
        device=args_cli.device,
        num_envs=args_cli.num_envs,
        use_fabric=not args_cli.disable_fabric
    )
    
    # Create environment (gravity ENABLED for realistic walking)
    env = gym.make(args_cli.task, cfg=env_cfg)
    
    # Convert to single-agent if needed (same as play.py)
    if isinstance(env.unwrapped, DirectMARLEnv):
        env = multi_agent_to_single_agent(env)
    
    print("üîç RL POLICY HEIGHT SCANNER MONITOR")
    print("="*70)
    print(f"Environment: {args_cli.task}")
    print(f"Number of robots: {args_cli.num_envs}")
    print(f"Checkpoint: {args_cli.checkpoint}")
    print("Monitoring terrain height while robot walks with RL policy...")
    print("Press Ctrl+C to stop")
    print("="*70)
    
    # Get height scanner sensor BEFORE wrapping environment
    if hasattr(env.unwrapped.scene, 'sensors') and "height_scanner" in env.unwrapped.scene.sensors:
        height_scanner = env.unwrapped.scene.sensors["height_scanner"]
        print(f"‚úÖ Height scanner found!")
        print(f"   Number of rays per robot: {height_scanner.num_rays}")
        print(f"   Total rays across all robots: {height_scanner.num_rays * args_cli.num_envs}")
    else:
        print("‚ùå No height scanner found in this environment!")
        env.close()
        simulation_app.close()
        return
    
    # Load RL policy using Isaac Lab's standard method
    policy, env_wrapped = load_rsl_rl_policy(env, args_cli.task, args_cli.checkpoint)
    
    if policy is None:
        print("‚ùå Failed to load policy! Exiting.")
        env.close()
        simulation_app.close()
        return
    
    # Get the correct number of environments
    num_envs = args_cli.num_envs
    
    # Reset environment and get initial observations (same as play.py)
    obs, _ = env_wrapped.get_observations()
    
    # Monitoring loop
    last_print_time = 0
    print_interval = 2.0  # Print every 2 seconds
    
    try:
        step_count = 0
        episode_rewards = torch.zeros(num_envs, device=env.unwrapped.device)
        
        while simulation_app.is_running():
            start_time = time.time()
            
            # Run everything in inference mode (same as play.py)
            with torch.inference_mode():
                # Get actions from policy (same as play.py)
                actions = policy(obs)
                # Step environment (same as play.py)
                obs, rewards, terminated, truncated = env_wrapped.step(actions)
            
            # Track rewards
            if rewards is not None:
                episode_rewards += rewards
            
            current_time = time.time()
            
            # Print height data every 2 seconds
            if current_time - last_print_time >= print_interval:
                # Get current height scanner data
                height_scanner_data = height_scanner.data
                sensor_pos_z = height_scanner_data.pos_w[:, 2]  # Sensor z position
                ray_hits_z = height_scanner_data.ray_hits_w[..., 2]  # Hit points z coordinates
                
                print(f"\n‚è∞ Time: {current_time:.1f}s | Step: {step_count}")
                print(f"üéÆ Episode Rewards: Mean={episode_rewards.mean():.2f}, Min={episode_rewards.min():.2f}, Max={episode_rewards.max():.2f}")
                print(f"üìã COMPREHENSIVE FINAL ENVIRONMENT ANALYSIS FOR AI AGENT")
                print(f"üìç TERRAIN HEIGHT DATA - ALL {num_envs} ROBOTS COMBINED:")
                
                # Collect ALL rays from ALL robots
                all_sensor_heights = []
                all_height_readings = []
                
                for env_idx in range(num_envs):
                    sensor_height = sensor_pos_z[env_idx].item()
                    hit_points = ray_hits_z[env_idx]  # All ray hits for this robot
                    
                    # Calculate RL observation height readings (Isaac Lab formula)
                    offset = 0.5
                    height_readings = sensor_height - hit_points - offset
                    
                    # Add all this robot's rays to the combined pool
                    all_height_readings.extend(height_readings.tolist())
                    all_sensor_heights.extend([sensor_height] * len(height_readings))
                
                # Calculate total rays
                total_rays = len(all_height_readings)
                
                if all_height_readings:
                    # Convert to tensor for easy processing
                    all_height_readings = torch.tensor(all_height_readings)
                    valid_rays = len(all_height_readings)
                    
                    # Calculate average across ALL rays from ALL robots
                    avg_height_reading = all_height_readings.mean().item()
                    
                    # Simple classification using deviation from global average
                    deviation_threshold = 0.1
                    deviations = all_height_readings - avg_height_reading
                    
                    # Count rays by classification
                    obstacles = deviations < -deviation_threshold
                    gaps = deviations > deviation_threshold
                    normal = (deviations >= -deviation_threshold) & (deviations <= deviation_threshold)
                    
                    obstacle_count = obstacles.sum().item()
                    gap_count = gaps.sum().item()
                    normal_count = normal.sum().item()
                    
                    # Calculate percentages of total rays
                    obstacle_pct = (obstacle_count / total_rays) * 100
                    gap_pct = (gap_count / total_rays) * 100
                    normal_pct = (normal_count / total_rays) * 100
                    
                    # Find extreme values across all robots
                    min_height = all_height_readings.min().item()
                    max_height = all_height_readings.max().item()
                    
                    # Find deepest gap and highest obstacle
                    deepest_gap = None
                    highest_obstacle = None
                    deepest_gap_reading = None
                    highest_obstacle_reading = None
                    shallowest_gap_reading = None
                    actual_obstacle_height = None
                    actual_gap_depth = None
                    actual_shallowest_gap_depth = None
                    
                    if gap_count > 0:
                        gap_deviations = deviations[gaps]
                        deepest_gap = gap_deviations.max().item()  # Most positive deviation
                        deepest_gap_reading = all_height_readings[gaps].max().item()  # Actual height reading
                        shallowest_gap_reading = all_height_readings[gaps].min().item()  # Shallowest gap
                        # Convert to intuitive gap depths: how much lower the terrain is
                        actual_gap_depth = deepest_gap_reading - avg_height_reading
                        actual_shallowest_gap_depth = shallowest_gap_reading - avg_height_reading
                    
                    if obstacle_count > 0:
                        obstacle_deviations = deviations[obstacles]
                        highest_obstacle = obstacle_deviations.min().item()  # Most negative deviation
                        highest_obstacle_reading = all_height_readings[obstacles].min().item()  # Actual height reading
                        # Convert to intuitive obstacle height: how much higher the terrain is
                        actual_obstacle_height = avg_height_reading - highest_obstacle_reading
                    
                    # Simple output with clear explanations
                    print(f"   üìä COMBINED RAY ANALYSIS:")
                    print(f"     Total rays: {total_rays} (from {num_envs} robots)")
                    print(f"     Height readings: {min_height:.3f}m to {max_height:.3f}m (avg: {avg_height_reading:.3f}m)")
                    print(f"     üìã Note: Negative readings = Obstacles | Positive readings = Gaps")
                    print(f"   ---")
                    print(f"   üî∫ OBSTACLES (terrain higher than expected):")
                    print(f"     Count: {obstacle_count} rays ({obstacle_pct:.1f}%)")
                    if actual_obstacle_height is not None:
                        print(f"     Highest obstacle: {actual_obstacle_height:.3f}m above normal terrain")
                        print(f"       (Height reading: {highest_obstacle_reading:.3f}m, Expected: {avg_height_reading:.3f}m)")
                    
                    print(f"   üï≥Ô∏è  GAPS (terrain lower than expected):")
                    print(f"     Count: {gap_count} rays ({gap_pct:.1f}%)")
                    if actual_gap_depth is not None:
                        print(f"     Deepest gap: {actual_gap_depth:.3f}m below normal terrain")
                        print(f"       (Height reading: {deepest_gap_reading:.3f}m, Expected: {avg_height_reading:.3f}m)")
                        print(f"     Shallowest gap: {actual_shallowest_gap_depth:.3f}m below normal terrain")
                        print(f"       (Height reading: {shallowest_gap_reading:.3f}m, Expected: {avg_height_reading:.3f}m)")
                    
                    print(f"   üèûÔ∏è  NORMAL TERRAIN: {normal_count} rays ({normal_pct:.1f}%)")
                    
                    # Updated safety assessment: dangerous if gap ratio > 1%
                    if gap_pct > 1.0 or obstacle_pct > 20:
                        safety = "üî¥ DANGEROUS TERRAIN"
                    elif obstacle_pct > 10:
                        safety = "üü° CAUTION"
                    else:
                        safety = "üü¢ SAFE TERRAIN"
                    print(f"   üö® Overall: {safety}")
                else:
                    print(f"   ‚ùå No valid terrain data!")
                
                last_print_time = current_time
            
            step_count += 1
            
            # Note: Reset logic removed due to different return formats
            
    except KeyboardInterrupt:
        print(f"\n\nüõë Monitoring stopped by user")
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # Clean shutdown
        print(f"\nÔøΩÔøΩ Shutting down...")
        try:
            env.close()
            print(f"‚úÖ Environment closed")
        except Exception as e:
            print(f"‚ö†Ô∏è Environment close warning: {e}")

if __name__ == "__main__":
    main()
    # Close sim app (same as play.py)
    simulation_app.close() 