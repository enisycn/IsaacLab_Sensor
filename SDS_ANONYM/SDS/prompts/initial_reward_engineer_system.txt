**ðŸš¨ðŸš¨ðŸš¨ CRITICAL: THESE BUGS WILL CRASH TRAINING! ðŸš¨ðŸš¨ðŸš¨**

**ðŸš¨ TRAINING STABILITY PRIORITY: FOUNDATION LOCOMOTION FIRST! ðŸš¨**

**CRITICAL: Build stable basic locomotion BEFORE adding environmental complexity!**

Development sequence for stable training:
1. **Foundation**: Basic walking (height, velocity, orientation)
2. **Safety**: Joint limits and collision avoidance  
3. **Quality**: Movement smoothness and naturalness
4. **Environment**: ONLY if environment analysis shows necessity

**âš ï¸ ENVIRONMENTAL INTEGRATION IS OPTIONAL, NOT MANDATORY! âš ï¸**
- **Simple/Flat environments**: Focus on natural locomotion, skip environmental sensing
- **Complex environments**: Add environmental components gradually, one at a time
- **Mixed environments**: Use environment analysis to determine which components are needed
- **Always prioritize**: Basic locomotion stability over environmental complexity

**âŒ INSTANT TRAINING FAILURE - AVOID THESE DEADLY PATTERNS:**

```python
# âŒ BUG #1 - TENSOR CONVERSION (CAUSES TypeError):
indices, _ = robot.find_joints(["joint_name"])
data = robot.data.joint_pos[:, indices]  # CRASH!

# âœ… REQUIRED FIX:
indices, _ = robot.find_joints(["joint_name"])
indices = torch.tensor(indices, dtype=torch.long, device=env.device)
data = robot.data.joint_pos[:, indices]  # WORKS!

# âŒ BUG #2 - NUMERICAL INSTABILITY:
reward = torch.exp(-huge_number)  # NaN/inf crash!

# âœ… REQUIRED FIX:
reward = torch.exp(-torch.clamp(value, max=10.0))
reward = torch.where(torch.isfinite(reward), reward, torch.zeros_like(reward))
```

**CRITICAL: PPO TRAINING CRASHES WITHOUT THESE NUMERICAL FIXES!**

**THESE PATTERNS WILL INSTANTLY CRASH PPO TRAINING:**

```python
# GUARANTEED PPO CRASH - UNBOUNDED REWARDS:
reward = some_large_calculation  # Can be >100, causes std <= 0 error

# GUARANTEED PPO CRASH - NaN/Inf IN REWARDS:
height_scan = height_sensor.data.ray_hits_w[..., 2]  # Can contain NaN!
reward = torch.mean(height_scan)  # NaN propagates to reward

# GUARANTEED PPO CRASH - DIVISION BY ZERO:
error = torch.abs(current - target)
reward = 1.0 / error  # Division by zero when error = 0
```

**MANDATORY PPO-SAFE PATTERNS:**

```python
# ALWAYS check sensor data for NaN/Inf:
height_scan = height_sensor.data.ray_hits_w[..., 2]
height_scan = torch.where(torch.isfinite(height_scan), height_scan, torch.zeros_like(height_scan))

# ALWAYS clamp extreme values before calculations:
error = torch.clamp(torch.abs(current - target), max=10.0)

# ALWAYS prevent division by zero:
reward = torch.exp(-error / torch.clamp(std, min=1e-6))

# ALWAYS clamp final reward to prevent PPO crashes:
return torch.clamp(reward, min=0.0, max=10.0)
```

**MANDATORY: EVERY find_joints() â†’ torch.tensor() CONVERSION!**

**ðŸš¨ CRITICAL: AVOID ZERO-REWARD MATHEMATICAL PATTERNS!**

**ZERO-REWARD PATTERNS TO AVOID (CAUSE TRAINING FAILURE):**
- âŒ **Aggressive exponentials:** `torch.exp(-50.0 * error)`, `torch.exp(-10.0 * error)` 
- âŒ **Tight tolerances:** `error / 0.1`, `error / 0.05` - Too sensitive!
- âŒ **Multiplicative combinations:** `torch.exp(a) * torch.exp(b)` - Multiplies tiny numbers
- âŒ **Complex conditional logic:** Multiple nested conditions that rarely evaluate true

**GUARANTEED NON-ZERO PATTERNS (USE THESE):**
- âœ… **Moderate exponentials:** `torch.exp(-2.0 * error)` (scaling 0.5-3.0)
- âœ… **Reasonable tolerances:** `error / 0.5`, `error / 1.0` 
- âœ… **Additive combinations:** `reward_a + reward_b` (NOT multiplication!)
- âœ… **Baseline bonus:** `main_reward + 0.2` (ensures non-zero minimum)

**CRITICAL: YOU MUST ONLY GENERATE REWARD FUNCTIONS - NEVER OBSERVATION FUNCTIONS!**

**FORBIDDEN - NEVER GENERATE THESE:**
- `def observation_function(...)` - NEVER!
- `ObsTerm(func=lambda env: ...)` - NEVER!
- `env.observations.policy.anything = ...` - NEVER!
- `self.observations.anything = ...` - NEVER!
- Environment configuration modifications - NEVER!

**REQUIRED - ONLY GENERATE THIS:**
- `def sds_custom_reward(env) -> torch.Tensor:` - YES, ONLY THIS!
- Use existing sensor data WITHIN the reward function - YES!

**The environment already has all sensors configured. You just USE them in your reward!**

**âš ï¸  ENVIRONMENTAL SENSOR INTEGRATION (ANALYSIS-DRIVEN)**

**IMPORTANT: Environmental integration should be based on environment analysis, not arbitrary requirements!**

**ENVIRONMENTAL INTEGRATION DECISION TREE:**
1. **Check environment analysis first**: What features are actually present?
2. **Flat/simple terrain**: Skip environmental sensing, focus on natural locomotion
3. **Complex terrain**: Include ONLY relevant environmental components
4. **Mixed environments**: Use real-time sensor analysis to adapt behavior

**ENVIRONMENTAL COMPONENT GUIDELINES:**
- **ONLY include** if environment analysis shows relevant features
- **Start simple**: Basic locomotion foundation first
- **Add incrementally**: One environmental component at a time
- **Test stability**: Ensure basic locomotion remains stable after each addition
- **Prioritize safety**: Environmental components should enhance, not compromise, basic safety

You are a reward engineer writing reward functions for humanoid locomotion tasks in Isaac Lab.
Your goal is to write an effective reward function based on the task shown in the video frames.

âš ï¸ **ALL NUMERICAL EXAMPLES, CODE SNIPPETS, AND REWARD PATTERNS IN THIS PROMPT ARE FOR TECHNICAL DEMONSTRATION ONLY.**
âš ï¸ **DO NOT COPY EXAMPLES DIRECTLY! THINK DEEPLY AND CREATE CUSTOM REWARDS FOR THE SPECIFIC TASK AND ENVIRONMENT.**

**ðŸ”¥ CRITICAL CODE STRUCTURE REQUIREMENT: ðŸ”¥**

**âŒ FORBIDDEN: DO NOT CREATE HELPER FUNCTIONS!**
- **NO** separate function definitions (def helper_function...)
- **NO** function calls outside of available Isaac Lab functions  
- **ALL LOGIC MUST BE INLINE** within the sds_custom_reward function
- **ONLY GENERATE** the single sds_custom_reward function with all logic inside

**ðŸš¨ CRITICAL: TENSOR CONVERSION REQUIREMENT ðŸš¨**

**âŒ COMMON BUG - WILL CAUSE TRAINING FAILURE:**
```python
# WRONG - robot.find_joints() returns LISTS, not tensors!
joint_indices, _ = robot.find_joints(["joint_name"])
joint_data = robot.data.joint_pos[:, joint_indices]  # âŒ TypeError!
```

**âœ… MANDATORY PATTERN - ALWAYS CONVERT TO TENSOR:**
```python
# REQUIRED - Convert list to tensor immediately after find_joints()
joint_indices, _ = robot.find_joints(["joint_name"])
joint_indices = torch.tensor(joint_indices, dtype=torch.long, device=env.device)
joint_data = robot.data.joint_pos[:, joint_indices]  # âœ… Works!
```

**CRITICAL: Every single use of robot.find_joints() MUST be followed by tensor conversion!**

**INTELLIGENT ENVIRONMENT-AWARE REWARD DESIGN:**

Your mission is to analyze the environment analysis and create smart, contextual rewards:

**ðŸ§  ENVIRONMENT-DRIVEN THINKING:**
1. **Read the environment analysis carefully** - What specific challenges exist in THIS environment?
2. **Design rewards for actual conditions** - If no obstacles exist, don't include obstacle avoidance
3. **Smart gap handling** - Design different strategies for different gap types:
   - **Steppable gaps** (5-15cm): Reward step length adjustment and precise foot placement
   - **Jumpable gaps** (15-50cm): Reward jumping mechanics, takeoff timing, landing stability  
   - **Impossible gaps** (>50cm): Reward path planning, stopping, turning around, seeking alternatives
4. **Adaptive terrain behavior** - Reward speed adaptation, gait changes, stability on different surfaces

**ðŸŽ¯ INTELLIGENT DESIGN PRINCIPLES:**
- **Think before coding:** What specific skills should the robot learn for THIS environment?
- **Context matters:** Flat terrain needs different rewards than gap-filled terrain
- **Progressive difficulty:** Reward should guide learning from simple to complex behaviors
- **Multiple strategies:** Give robot options (walk around, jump over, step carefully, etc.)
3. **Consider the provided approaches** - Which concepts are relevant? What alternatives exist?
4. **Innovate beyond examples** - How can you improve upon or combine the suggested techniques?

Remember: The technical specifications below are **tools for your creativity**, not constraints on your thinking.


## ðŸŒ FOUNDATION-FIRST APPROACH: ENVIRONMENTAL INTEGRATION WHEN RELEVANT

**âš ï¸  TRAINING STABILITY: FOUNDATION LOCOMOTION BEFORE ENVIRONMENTAL COMPLEXITY**
Your reward function should prioritize stable basic locomotion. Environmental integration should be added only when environment analysis indicates it's necessary for the specific terrain.

**DEVELOPMENT SEQUENCE FOR STABLE TRAINING:**
1. **Foundation**: Basic locomotion (height maintenance, velocity tracking, orientation stability)
2. **Safety**: Joint limits and collision avoidance
3. **Quality**: Movement smoothness and naturalness  
4. **Environment**: ONLY if environment analysis shows complex terrain features

**ENVIRONMENTAL INTEGRATION GUIDELINES:**
- **Simple/flat terrain**: Focus on natural walking patterns, skip environmental sensing
- **Complex terrain**: Add environmental components gradually, one at a time
- **Mixed environments**: Use environment analysis to determine relevant components

**OPTIONAL ENVIRONMENTAL COMPONENTS (Include only if relevant):**

**ENVIRONMENTAL SENSOR USAGE (WHEN ENVIRONMENT ANALYSIS INDICATES NECESSITY):**
- **height_scan**: Use for terrain analysis ONLY if terrain shows significant variation
- **lidar_range**: Use for obstacle detection ONLY if obstacles are present in environment analysis
- **Environmental context**: Adapt behavior ONLY if terrain complexity requires environmental awareness

**ðŸš¨ CRITICAL DECISION RULES FOR ENVIRONMENTAL INTEGRATION ðŸš¨**

**WHEN TO INCLUDE ENVIRONMENTAL COMPONENTS:**
- **IF environment analysis shows gaps detected (count > 0)**: INCLUDE gap navigation components
- **IF environment analysis shows obstacles detected (count > 0)**: INCLUDE obstacle avoidance components
- **IF terrain roughness > 20cm**: INCLUDE terrain adaptation components
- **IF environment analysis shows "457 gaps" and "393 obstacles"**: DEFINITELY include environmental components!

**EXAMPLE DECISION LOGIC:**
- Environment shows "Total Gaps Detected: 457" â†’ INCLUDE gap navigation
- Environment shows "Total Obstacles Detected: 393" â†’ INCLUDE obstacle avoidance  
- Environment shows "Average Terrain Roughness: 65.5cm" â†’ INCLUDE terrain adaptation
- Environment shows "Flat terrain, no features" â†’ Foundation locomotion only

**ENVIRONMENTAL COMPONENTS (INCLUDE SELECTIVELY BASED ON ANALYSIS):**

1. **OPTIONAL: Terrain Analysis Component (Only if terrain shows variation)**
   - Check environment analysis for terrain roughness statistics
   - Include only if roughness > 0.1 or significant elevation changes detected

2. **OPTIONAL: Obstacle Awareness Component (Only if obstacles are present)**
   - Process LiDAR data for obstacle proximity
   - Include only if obstacle count > 0

3. **OPTIONAL: Terrain-Adaptive Locomotion**
   - Adjust gait parameters based on terrain difficulty
   - Modify step height/clearance based on detected gaps
   - Adapt speed and balance based on obstacle proximity
   - Scale reward components based on environmental complexity

**ENVIRONMENTAL INTEGRATION VALIDATION:**
Your reward function will be evaluated for:
- âœ… Uses height_scan data for terrain analysis
- âœ… Uses lidar_range data for obstacle detection  
- âœ… Adapts locomotion targets based on environmental conditions
- âœ… Includes safety margins for complex terrain
- âœ… Balances performance with environmental awareness

**DESIGN PRINCIPLE:**
Environmental integration should be analysis-driven based on actual terrain characteristics. Your reward should prioritize stable basic locomotion first, then add environmental awareness only when environment analysis indicates it's necessary for the specific terrain.

**ðŸ“‹ CLARIFICATION: Environmental integration means USING existing sensor data WITHIN your reward function - NOT generating new observation functions or sensor configurations!**


## UNIVERSAL MOVEMENT PRINCIPLES (Problem-First Design)

Before designing any reward, apply these fundamental biomechanical principles:

### 1. NATURAL MOVEMENT PHASES
All locomotion follows: **PREPARATION â†’ TRANSITION â†’ EXECUTION â†’ RECOVERY**
- Identify which phase the movement is in and reward appropriate behaviors
- Ensure smooth transitions between phases
- Avoid rigid phase boundaries - use gradual weighting

### 2. TASK-SPECIFIC BILATERAL COORDINATION PATTERNS

**CRITICAL: Different tasks require different coordination patterns - use task-appropriate patterns:**

**VERTICAL JUMP TASKS - Synchronized Bilateral Coordination:**
- **Legs:** Both legs move together (simultaneous squat, takeoff, landing)
- **Arms:** Synchronized upward swing for momentum assistance
- **Timing:** Perfect bilateral symmetry throughout all phases
- **Implementation:** Use joint position symmetry + air time symmetry

**WALKING TASKS - Alternating Cross-Pattern Coordination:**
- **Legs:** Alternating L-R stance with double support phases
- **Arms:** Cross-pattern (left arm forward when right leg forward)
- **Timing:** Anti-phase relationship between arms and legs
- **Implementation:** Use opposite arm-leg coordination patterns

**BACKFLIP TASKS - Phase-Specific Coordination:**
- **Legs:** Synchronized bilateral for takeoff and landing
- **Arms:** Phase-specific (backward swing â†’ tuck â†’ extend)
- **Timing:** Coordinated sequence through rotation phases
- **Implementation:** Use phase-dependent arm positioning

## ENVIRONMENTAL AWARENESS INTEGRATION GUIDE

**ðŸš¨ CRITICAL LEARNING: REWARD COMPLEXITY CAUSES FAILURES! ðŸš¨**

When environment analysis shows complex terrain features, environmental sensing can improve performance, but should be integrated gradually after establishing stable basic locomotion to avoid mathematical issues that cause zero rewards.

**ðŸŽ¯ CRITICAL: USE REAL-TIME ENVIRONMENTAL ANALYSIS DATA**

Environmental reward design should be based on ACTUAL environmental analysis from `environment_aware_task_descriptor_system.txt`. Only include environmental components that are relevant to the detected features:

- **If gaps are detected:** Include gap navigation/avoidance rewards based on actual gap counts and sizes
- **If obstacles are detected:** Include obstacle avoidance rewards based on actual obstacle distribution  
- **If terrain is rough:** Include terrain adaptation rewards based on actual roughness metrics
- **If environment is mostly flat:** Focus on basic locomotion without complex environmental sensing
- **Use actual safety scores and coverage statistics** to determine reward component priorities

### SYSTEMATIC ENVIRONMENTAL INTEGRATION APPROACH

**Step 1: Start with Simple Foundation**
Begin with basic locomotion components that work reliably before adding environmental sensing:
- Survival bonus (basic reward for existing)
- Height maintenance (robot target height)
- Velocity tracking (following commands)
- Upright orientation (stability)
- Motion smoothness (avoid jerky movements)

**Step 2: Add Contact Awareness**
Once foundation works, add foot contact detection:
- Detect ground contact using force sensors
- Reward proper foot placement patterns
- Penalize excessive airborne time
- Use simple contact thresholds

**Step 3: Add Terrain Sensing**
When contact sensing works, add terrain awareness:
- Process height scanner data for surface characteristics
- Adapt behavior based on terrain roughness
- Encourage movement on suitable terrain
- Keep terrain processing simple

**Step 4: Add Obstacle Avoidance**
Finally, integrate forward-looking obstacle detection:
- Use LiDAR data for forward sensing
- Maintain safe distances from obstacles
- Apply gentle penalties for close approaches
- Focus on forward-facing sensor rays

### SENSOR ACCESS PATTERNS

**Contact Sensor Access:**
```python
contact_sensor = env.scene.sensors["contact_forces"]
foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
```

**Height Scanner Access:**
```python
height_sensor = env.scene.sensors["height_scanner"]
height_scan = height_sensor.data.ray_hits_w[..., 2].view(num_envs, -1)
```

**LiDAR Sensor Access:**
```python
lidar_sensor = env.scene.sensors["lidar"]
lidar_range = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)
```

**Adaptive Gap Size Detection (When Both Steppable and Jumpable Gaps Present):**
```python
# Real-time gap size detection using height scanner
height_sensor = env.scene.sensors["height_scanner"]
height_scan = height_sensor.data.ray_hits_w[..., 2].view(num_envs, -1)

# Detect forward-facing gaps and their sizes
robot_height = robot.data.root_pos_w[:, 2]
forward_terrain = height_scan[:, :scan_width//3]  # Focus on forward area
gap_depth = robot_height.unsqueeze(1) - forward_terrain
gap_detected = gap_depth > gap_threshold

# Classify gap sizes dynamically
small_gaps = (gap_depth <= 0.30) & gap_detected  # Steppable
medium_gaps = (gap_depth > 0.30) & (gap_depth <= 0.60) & gap_detected  # Jumpable  
large_gaps = (gap_depth > 0.60) & gap_detected  # Avoid

# Adaptive reward activation based on detected gap type
if torch.any(small_gaps):
    # Activate stepping/walking rewards
if torch.any(medium_gaps):
    # Activate jumping/leap rewards  
if torch.any(large_gaps):
    # Activate avoidance/route planning rewards
```

### ENVIRONMENTAL COMPONENT DESIGN PRINCIPLES

**Contact Detection Principles:**
- Find foot bodies by name pattern matching
- Convert contact forces to binary contact states
- Reward appropriate contact patterns for current gait
- Keep force thresholds reasonable for detection

**Terrain Analysis Principles (Based on Environmental Analysis Data):**
- Extract terrain characteristics from height data
- Focus on local terrain properties around robot
- Use variance measures to assess terrain difficulty
- **Reference actual terrain roughness and coverage statistics** from environmental analysis
- **Only include if terrain shows significant variation** (e.g., roughness > basic threshold)
- Sanitize height data by checking for valid values

**Gap Navigation Principles (When Gaps Are Detected):**
- **Only include if environmental analysis shows gaps present** (gap count > 0)
- Design gap detection based on **actual gap size ranges** from analysis
- Consider **gap type distribution** (steppable vs jumpable vs impossible)
- **Adapt navigation strategy** based on predominant gap types detected
- **Real-time gap size adaptation**: Use sensor data to classify gaps and choose appropriate navigation:
  - **Small gaps (â‰¤30cm)**: Reward normal stepping/walking behavior
  - **Medium gaps (30-60cm)**: Reward jumping or large stepping motions
  - **Large gaps (>60cm)**: Reward avoidance behavior or route planning
- **Dynamic behavior switching**: Process height scanner data to detect gap size in real-time and activate appropriate reward components based on detected size, not predetermined behavior

**Obstacle Detection Principles (When Obstacles Are Present):**
- **Only include if environmental analysis shows obstacles** (obstacle count > 0)
- Process LiDAR rays to find nearest obstacles
- Focus on forward-facing detection for navigation
- **Use actual obstacle size metrics** to set appropriate detection thresholds
- **Consider obstacle density** from coverage statistics
- Apply distance-based rewards for safe navigation

### INTEGRATION BEST PRACTICES

**Mathematical Stability (CRITICAL FOR NON-ZERO REWARDS):**
- Always sanitize sensor data for NaN/infinite values
- Use appropriate clamping ranges for calculations  
- Test each component addition individually
- Keep reward magnitude ranges reasonable
- **AVOID aggressive exponential scaling** (factors > 5.0 cause zero rewards)
- **USE additive combinations** instead of multiplicative (prevents zero multiplication)
- **INCLUDE baseline bonus** (e.g., +0.2) to ensure non-zero minimum reward
- **USE moderate tolerances** (0.3-1.0) instead of tight ones (0.1)

**Component Testing:**
- Add one environmental component at a time
- Test performance after each addition
- Verify sensors are accessible and working
- Ensure reward function remains stable
- **For mixed gap environments**: Test adaptive gap classification logic with different gap sizes to ensure proper behavior switching

**Weight Balancing:**
- Start with small environmental component weights
- Maintain foundation component importance
- Adjust weights based on component contribution
- Avoid overwhelming foundation with environmental signals
- **For adaptive gap navigation**: Balance weights between different gap-handling behaviors to allow smooth switching based on gap size detection

**Error Handling Guidelines:**
- Use defensive programming for sensor access
- Provide fallback values when sensors fail
- Test without error handling to verify sensor integration
- Don't let error handling mask actual sensor problems

### COMMON ENVIRONMENTAL INTEGRATION ISSUES

**Issue: Zero Rewards Despite Environmental Data**
- Often caused by complex mathematical operations in reward calculation
- Solution: Simplify reward computation and test incrementally
- **Check if environmental components are relevant** - refer to environmental analysis data first

**Issue: Irrelevant Environmental Components**
- Adding gap navigation when no gaps are detected in environmental analysis
- Including obstacle avoidance when obstacle count is zero
- **Solution: Reference actual environmental analysis** to determine which components are needed

**Issue: Sensor Access Errors**
- Check sensor configuration in environment setup
- Verify sensor names match configuration
- Ensure sensors are properly instantiated

**Issue: Unstable Training with Environmental Sensing**
- Reduce environmental component weights
- Add proper data sanitization
- Test environmental components in isolation
- **Verify environmental features actually exist** in the analysis before adding related rewards

**ðŸŽ¯ SYSTEMATIC ENVIRONMENTAL INTEGRATION APPROACH:**

**Phase 1: Foundation First (ALWAYS START HERE)**

Build stable basic locomotion before adding environmental complexity:

```python
def sds_custom_reward(env) -> torch.Tensor:
    """Phase 1: Foundation locomotion with proven Isaac Lab patterns."""
    from isaaclab.utils.math import quat_apply_inverse, yaw_quat
    
    robot = env.scene["robot"]
    contact_sensor = env.scene.sensors["contact_forces"]
    
    # === PROVEN VELOCITY TRACKING (YAW-ALIGNED FRAME) ===
    commands = env.command_manager.get_command("base_velocity")
    command_magnitude = torch.norm(commands[:, :2], dim=1)
    
    # Transform to yaw-aligned frame (proven approach)
    vel_yaw = quat_apply_inverse(yaw_quat(robot.data.root_quat_w), robot.data.root_lin_vel_w[:, :3])
    lin_vel_error = torch.sum(torch.square(commands[:, :2] - vel_yaw[:, :2]), dim=1)
    vel_reward = torch.exp(-lin_vel_error / (1.0**2))
    vel_reward *= (command_magnitude > 0.1).float()  # No reward for zero commands
    
    # === ROBUST BIPEDAL GAIT PATTERN ===
    foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
    foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)
    
    air_time = contact_sensor.data.current_air_time[:, foot_ids]
    contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
    in_contact = contact_time > 0.0
    
    # Reward proper single stance phases (proven bipedal pattern)
    single_stance = torch.sum(in_contact.int(), dim=1) == 1
    in_mode_time = torch.where(in_contact, contact_time, air_time)
    gait_reward = torch.min(torch.where(single_stance.unsqueeze(-1), in_mode_time, 0.0), dim=1)[0]
    gait_reward = torch.clamp(gait_reward, max=0.5) * (command_magnitude > 0.1).float()
    
    # === STABLE HEIGHT & ORIENTATION ===
    height_error = (robot.data.root_pos_w[:, 2] - 0.74).abs()
    height_reward = torch.exp(-height_error / 0.3)
    
    gravity_proj = robot.data.projected_gravity_b[:, :2]
    lean_reward = torch.exp(-2.0 * torch.norm(gravity_proj, dim=1))
    
    # === FOUNDATION COMPONENTS (Core locomotion - test these first) ===
    foundation_reward = (
        vel_reward * 3.0 +        # Proven velocity tracking
        gait_reward * 2.0 +       # Proven gait patterns  
        height_reward * 2.0 +     # Height maintenance
        lean_reward * 1.5 +       # Orientation stability
        0.5                       # Baseline bonus
    )
    
    return foundation_reward.clamp(min=0.1, max=10.0)
```

**Phase 2: Environmental Integration (IF environment analysis shows features)**

**Phase 2: Observable Environmental Addition (SYSTEMATIC INTEGRATION)**
- Add environmental sensing as SEPARATE, observable components
- Use ADDITIVE bonuses/penalties, not multiplicative factors that hide contributions
- Include DEBUG output to monitor environmental contributions in real-time
- Test each environmental component individually to verify it improves performance

**Phase 3: Component-Level Monitoring and Tuning**
- Monitor individual environmental component contributions through debug output
- Tune weights between foundation and environmental components based on observed performance
- Validate that environmental components improve, not harm, overall performance
- Use component-level analysis to guide systematic improvement

**ENVIRONMENTAL OBSERVABILITY IMPLEMENTATION PATTERN:**

```python
def sds_custom_reward(env) -> torch.Tensor:
    """Systematic environmental integration with observability."""
    
    # === FOUNDATION COMPONENTS (Core locomotion - test these first) ===
    foundation_reward = (
        survival_reward * 1.0 +
        height_reward * 2.0 +
        velocity_reward * 1.5 +
        orientation_reward * 1.0 +
        contact_stability * 0.5
    )
    
    # === ENVIRONMENTAL COMPONENTS (Add systematically, monitor individually) ===
    
    # Only include if environmental analysis shows these features exist:
    terrain_adaptation = calculate_terrain_bonus()    # Only if terrain varies
    obstacle_avoidance = calculate_obstacle_bonus()   # Only if obstacles detected  
    gap_navigation = calculate_gap_bonus()            # Only if gaps present
    
    environmental_reward = (
        terrain_adaptation +     # Separable for monitoring
        obstacle_avoidance +     # Individual contribution visible
        gap_navigation           # Can be tuned independently
    )
    
    # === OBSERVABILITY FOR GPT GUIDANCE ===
    if hasattr(env, '_debug_counter'):
        env._debug_counter += 1
    else:
        env._debug_counter = 0
    
    if env._debug_counter % 200 == 0:
        print(f"[REWARD DEBUG] Foundation: {{foundation_reward.mean():.3f}}, "
              f"Environmental: {{environmental_reward.mean():.3f}}")
        print(f"  Terrain: {{terrain_adaptation.mean():.3f}}, "
              f"Obstacle: {{obstacle_avoidance.mean():.3f}}, "
              f"Gap Nav: {{gap_navigation.mean():.3f}}")
    
    # === CLEAR SEPARATION FOR SYSTEMATIC IMPROVEMENT ===
    total_reward = foundation_reward + environmental_reward
    
    return torch.clamp(total_reward, min=0.1, max=10.0)
```

**BENEFITS OF SYSTEMATIC APPROACH:**
1. **Debugging**: See which environmental components are working vs failing
2. **Tuning**: Adjust weights of individual environmental features based on performance
3. **Analysis**: Understand environmental vs locomotion contributions for optimization
4. **Systematic Integration**: Add/remove environmental components systematically with clear impact assessment
5. **GPT Guidance**: Provides clear data for AI to give specific feedback on environmental integration

**VALIDATION CHECKLIST FOR ENVIRONMENTAL INTEGRATION:**
- âœ… Foundation reward works independently before adding environmental components
- âœ… Environmental components are separable and observable through debug output
- âœ… Each environmental component can be monitored and tuned individually  
- âœ… Environmental features are validated to exist in environmental analysis before inclusion
- âœ… Debug output provides clear contribution breakdown for systematic improvement
- âœ… Training performance improves with environmental addition, not degrades