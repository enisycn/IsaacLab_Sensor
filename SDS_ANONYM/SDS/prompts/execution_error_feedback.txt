ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ
ğŸŒŸğŸŒŸğŸŒŸ ENVIRONMENT-AWARE LOCOMOTION REWARD DESIGN GUIDANCE ğŸŒŸğŸŒŸğŸŒŸ
ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ

**CRITICAL PROJECT UNDERSTANDING:**

These guidance explanations are designed for creating **environment-aware locomotion rewards** that enable **intelligent sensor-driven behavior adaptation** for humanoid robots.

**ğŸ¯ PRIMARY OBJECTIVE:** 
Create reward functions that make **measurable positive impact when sensors are used** compared to **without sensor usage**.

**ğŸ”¬ RESEARCH PURPOSE:**
Enable comparison studies demonstrating:
- **WITHOUT SENSORS:** Basic locomotion behavior 
- **WITH SENSORS:** Adaptive, context-aware behavior that responds to environmental challenges

**ğŸ§  DESIGN METHODOLOGY:**
1. **UNDERSTAND:** Use provided task requirements and sensor capabilities
2. **THINK:** Determine which sensors are needed and how they should influence behavior
3. **DECIDE:** Implement context-aware behavioral switching (NOT simultaneous conflicting rewards)
4. **IMPACT:** Ensure sensors create observable behavioral differences for scientific comparison

**ğŸ“Š SUCCESS CRITERIA:**
âœ… Robot behaves measurably different with sensors vs. without sensors
âœ… Sensor-enabled robot adapts to environmental challenges more effectively
âœ… Clear behavioral switching based on environmental context
âœ… No conflicting simultaneous behaviors (e.g., walking + jumping simultaneously)

**âš ï¸ FAILURE INDICATORS:**
âŒ Robot behaves identically with/without sensors
âŒ Sensors provide only minor bonuses without changing core behavior
âŒ Conflicting reward objectives that confuse the policy

---

**ğŸš¨ EXECUTION ERROR DETAILS:**

{traceback_msg}

---

ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
ğŸš¨ğŸš¨ğŸš¨ #1 TRAINING CRASH: STD >= 0.0 RUNTIME ERROR - READ THIS FIRST ğŸš¨ğŸš¨ğŸš¨
ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨

**CRITICAL PPO TRAINING FAILURE:**

**RUNTIMEERROR: normal expects all elements of std >= 0.0**
- CAUSE: Complex reward functions create unstable gradients that make policy distribution invalid
- CRASH: Policy network outputs negative standard deviation for action sampling
- SOLUTION: Simplify reward function with stable patterns

**âœ… IMMEDIATE FIX PATTERN:**
```python
def sds_custom_reward(env) -> torch.Tensor:
    # Use simple, stable components only
    velocity = torch.clamp(velocity_component, 0.0, 2.0)
    height = torch.clamp(height_component, 0.0, 2.0)
    stability = torch.clamp(stability_component, 0.0, 2.0)
    
    # ADDITIVE combination with small baseline
    total = velocity * 2.0 + height * 1.5 + stability * 1.0 + 0.1
    
    # MANDATORY: Final bounds
    return total.clamp(min=0.0, max=5.0)
```

ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
ğŸš¨ğŸš¨ğŸš¨ #2 TRAINING CRASH: SENSOR ATTRIBUTEERROR - READ THIS SECOND ğŸš¨ğŸš¨ğŸš¨
ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨

**MOST COMMON ERROR PATTERN:**

**ATTRIBUTEERROR: 'RayCasterData' object has no attribute 'distances'**
- CAUSE: Using non-existent sensor attributes (.distances, .height_measurements, .range_data)
- CRASH LINE EXAMPLES:
```python
lidar_range = lidar_sensor.data.distances  # AttributeError!
height_data = height_sensor.data.height_measurements  # AttributeError!
dist = lidar_sensor.data.distances  # INSTANT CRASH!
terrain_data = height_sensor.data.range_data  # AttributeError!
```

**HEIGHT SENSOR COMMON ERRORS:**
```python
# âŒ WRONG: Non-existent attributes that cause crashes
height_scan = height_sensor.data.height_scan  # AttributeError!
terrain_map = height_sensor.data.terrain_data  # AttributeError! 
elevation = height_sensor.data.elevation  # AttributeError!

# âœ… CORRECT: Isaac Lab standard formula only
height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5
```
âœ… CORRECT SENSOR ACCESS PATTERN (Isaac Lab standard for reward functions):
```python
# Isaac Lab standard raw sensor access for reward functions
height_sensor = env.scene.sensors["height_scanner"]
lidar_sensor = env.scene.sensors["lidar"]

# Height sensor: 567 rays (27Ã—21 grid), 2.0Ã—1.5m coverage, 7.5cm resolution, 3m range
height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5
# CRITICAL: Sanitize sensor data - rays can return NaN/Inf when missing terrain
height_measurements = torch.where(torch.isfinite(height_measurements), height_measurements, torch.zeros_like(height_measurements))

# LiDAR sensor: 152 rays, 180Â° FOV, 5m range  
lidar_distances = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)
# Handle infinite readings correctly - they indicate no obstacles (valuable for exploration)
# CRITICAL: Infinite values mean rays exceeded 5.0m max range (no obstacles detected)
finite_mask = torch.isfinite(lidar_distances)
lidar_clamped = torch.where(finite_mask, lidar_distances, torch.tensor(5.0, device=lidar_distances.device))
open_space_ratio = (~finite_mask).sum(dim=-1).float() / lidar_distances.shape[-1]
```

âŒ WRONG PATTERNS THAT CAUSE CRASHES:
```python
height_scan = height_sensor.data.distances  # AttributeError: no such attribute
lidar_range = lidar_sensor.data.range_measurements  # AttributeError: no such attribute
terrain_info = height_sensor.data.height_scan  # AttributeError: no such attribute

# âŒ WRONG: Trying to access processed data that doesn't exist
gap_data = height_sensor.data.gaps  # AttributeError!
obstacle_data = height_sensor.data.obstacles  # AttributeError!
```

**CRITICAL HEIGHT SENSOR BASELINE ERRORS:**
```python
# âŒ WRONG: Using hardcoded values without understanding baseline
obstacles = height_readings < 0.1  # Ignores G1 robot baseline!
gaps = height_readings > 0.3       # Wrong threshold reference!

# âœ… CORRECT: Use G1 robot baseline (0.209m) with relative thresholds  
baseline = 0.209  # G1 robot on flat terrain
obstacles = height_readings < (baseline - 0.07)  # < 0.139m = obstacles
gaps = height_readings > (baseline + gap_threshold)       # Above baseline = gaps
```

**CRITICAL LIDAR SENSOR MAX RANGE ERRORS:**
```python
# âŒ WRONG: Not handling infinite values from max range exceeded
lidar_distances = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)
min_distance = torch.min(lidar_distances, dim=1)[0]  # Can be infinite!

# âœ… CORRECT: Handle 5.0m max range - infinite means no obstacles beyond range
lidar_distances = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)
# Replace infinite values with max range (5.0m) - indicates obstacle-free space
lidar_distances = torch.where(torch.isfinite(lidar_distances), lidar_distances, torch.tensor(5.0, device=lidar_distances.device))
min_distance = torch.min(lidar_distances, dim=1)[0]  # Now safely bounded [0.0, 5.0]
```

ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨

Isaac Lab reward functions can encounter various runtime errors. Here's how to diagnose and fix common issues:

**BEHAVIORAL ERRORS - UNNATURAL ROBOT MOVEMENT:**

**ğŸ¥ TRAINING FOOTAGE ANALYSIS - MANDATORY BEHAVIOR DIAGNOSIS:**

**ğŸš¨ STUCK ROBOT SYNDROME:**
- **VISUAL SYMPTOM**: Robot appears stationary across video frames, minimal position change
- **METRICS SYMPTOM**: Very low velocity tracking rewards, high baseline rewards
- **ROOT CAUSE**: Baseline reward too high relative to movement rewards
```

**ğŸš¨ WRONG GAIT PATTERNS:**
- **VISUAL SYMPTOM**: Robot shuffling, sliding, unnatural leg movements
- **METRICS SYMPTOM**: Poor gait rewards, inconsistent contact patterns
- **ROOT CAUSE**: Air time and contact rewards poorly balanced
```

**ğŸš¨ TURNING IN PLACE SYNDROME:**
- **VISUAL SYMPTOM**: Robot rotating but not moving forward
- **METRICS SYMPTOM**: Good yaw tracking, poor forward velocity
- **ROOT CAUSE**: Yaw rewards too high relative to forward motion
- 
```

**HEIGHT HARDCODING ERROR (robots stand still on stairs/platforms):**
- SYMPTOM: Robots learned to stand still on elevated terrain or go downstairs
- CAUSE: Using inappropriate height tracking method for the terrain characteristics
- FIX: Choose height tracking method based on task requirements:

**ABSOLUTE HEIGHT - For consistent body clearance:**
```python
# Use when: Flat terrain OR gap crossing (large stepping) OR consistent platform heights
height_err = torch.abs(robot.data.root_pos_w[:, 2] - 0.74)  # Target height above world origin
height_reward = torch.clamp(1.0 - (height_err / 0.3), 0.0, 2.0)
```

**HEIGHT SENSOR TYPE SELECTION - Choose based on task requirements:**

ğŸ“Š **SENSOR SELECTION GUIDELINES:**
| Task Type | Recommended Sensor | Reason |
|-----------|-------------------|---------|
| **Stair Climbing** | âœ… **Absolute** | Navigation requires world coordinates for clear goal direction |
| **Obstacle Avoidance** | âœ… **Relative** | Safety requires clearance measurements above terrain |
| **Path Planning** | âœ… **Absolute** | Need actual terrain elevations for route decisions |
| **Foot Placement** | âœ… **Relative** | Need clearance measurements for safe step placement |

**ğŸ”§ SENSOR FORMULAS:**
```python
height_sensor = env.scene.sensors["height_scanner"]

# âœ… ABSOLUTE HEIGHT (world coordinates for navigation):
terrain_heights = height_sensor.data.ray_hits_w[..., 2]  # Direct world Z coordinates

# âœ… RELATIVE HEIGHT (Isaac Lab standard clearance for safety):
height_clearance = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5

# CRITICAL: Always sanitize sensor data
terrain_heights = torch.where(torch.isfinite(terrain_heights), terrain_heights, torch.zeros_like(terrain_heights))
```

**DECISION GUIDE - Let task requirements guide your choice:**
- **Stair climbing/navigation** â†’ Use absolute heights for clear directional goals
- **Obstacle avoidance/safety** â†’ Use relative heights for clearance measurements
- **Mixed terrain navigation** â†’ Use absolute heights for path planning
- **Foot placement precision** â†’ Use relative heights for surface clearance



**TECHNICAL ERROR PATTERNS:**

TENSOR BROADCASTING ERRORS (e.g., "output with shape [N] doesn't match broadcast shape [N,N]"):
- Caused by implicit broadcasting between tensors of incompatible shapes
- FIX: Use explicit .expand() or .unsqueeze() to match dimensions exactly
- Example: desired_value.unsqueeze(1).expand(-1, num_feet) instead of desired_value.unsqueeze(1)

NEGATIVE STANDARD DEVIATION ERRORS ("normal expects all elements of std >= 0.0"):
- Caused by NaN/Inf values propagating to policy network
- VERIFIED CRASH CAUSES: 
  * Unprotected sensor data: height_sensor/lidar rays return NaN when missing
  * Explosive exponentials: torch.exp(-terrain_var * 100.0) creates -âˆ
  * NaN variance: torch.var() on NaN data returns NaN
  * Near-zero division: 1.0/torch.min(distances) when distances â‰ˆ 0
- FIX: MANDATORY sensor sanitization:
  ```python
  sensor_data = torch.where(torch.isfinite(sensor_data), sensor_data, fallback_value)
  terrain_var = torch.clamp(torch.var(height_data), max=0.01)
  min_dist = torch.clamp(torch.min(lidar_data), min=0.05)
  total = torch.where(torch.isfinite(total), total, torch.ones_like(total) * 0.5)
  ```

RUNTIME TENSOR SHAPE ERRORS:
- Caused by sensor data dimensions not matching expected robot batch size
- FIX: Validate tensor shapes before operations, use proper indexing for foot_ids and joint_ids

DIVISION BY ZERO ERRORS:
- Caused by dividing by potentially zero values
- FIX: Use torch.clamp(denominator, min=1e-6) before division operations

ENVIRONMENTAL SENSING DECISION ERRORS:
- Caused by dismissing sensor data in favor of visual analysis  
- SYMPTOMS: "sensors detect gaps/obstacles but visual shows flat terrain"
- FIX: Always prioritize quantitative sensor measurements over visual interpretation
- RULE: Use provided task requirements to determine if environmental components are needed

COMMON ERRORS AND FIXES:

SENSOR ACCESS ERRORS:
- SYMPTOMS: AttributeError when accessing .distances or .height_measurements
- CAUSE: Using non-existent sensor attributes
- FIX: Use Isaac Lab standard raw sensor access:
  height_sensor = env.scene.sensors["height_scanner"]
  height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5
  # MANDATORY: Sanitize immediately after calculation
  height_measurements = torch.where(torch.isfinite(height_measurements), height_measurements, torch.zeros_like(height_measurements))

TENSOR OPERATION ERRORS:
- SYMPTOMS: RuntimeError during math operations (division by zero, NaN values)
- CAUSE: Unstable mathematical operations
- FIX: Use torch.clamp(denominator, min=1e-6) before division operations

ENVIRONMENTAL SENSING DECISION ERRORS:
- Caused by dismissing sensor data in favor of visual analysis  
- SYMPTOMS: "sensors detect gaps/obstacles but visual shows flat terrain"
- FIX: Always prioritize quantitative sensor measurements over visual interpretation
- RULE: Use provided task requirements to determine if environmental components are needed

SENSOR BEHAVIORAL ADAPTATION ERRORS:
- SYMPTOMS: Robot behaves identically with/without sensors (no measurable difference)
- CAUSE: Adding minimal sensor bonuses instead of behavioral switching
- FIX: Use context-aware behavioral adaptation:
```python
# âŒ WRONG: Minimal sensor impact
total = foundation + sensor_bonus * 0.1  # Robot still walks the same way

# âœ… CORRECT: Behavioral switching
if gap_detected:
    behavior = foundation + gap_crossing_adaptation()  # Different gait
elif obstacle_detected:
    behavior = foundation + obstacle_avoidance_adaptation()  # Different navigation
else:
    behavior = foundation_only()  # Normal walking
```
- RULE: Sensors must create observable behavioral differences for comparison studies

Apply these fixes systematically to prevent future runtime crashes.

**âš ï¸ PERFORMANCE ISSUES (SLOW TRAINING):**

**SYMPTOMS:** Training takes >10 minutes for 500 iterations

**COMMON CAUSES:**
1. **Per-environment loops** - Any `for i in range(env.num_envs):` 
2. **Individual tensor indexing** - `tensor[i][mask[i]]` in loops
3. **Repeated median/complex calculations** - Use mean instead
4. **Heavy sensor processing** - Minimize height scanner operations

**IMMEDIATE FIX PATTERN:**
```python
# REPLACE THIS (SLOW):
for i in range(env.num_envs):
    vals = height_data[i][mask[i]]
    if vals.numel() > 0:
        result[i] = torch.median(vals)

# WITH THIS (FAST):
valid_data = torch.where(mask, height_data, torch.zeros_like(height_data))
result = valid_data.sum(dim=1) / torch.clamp(mask.sum(dim=1), min=1.0)
```

**PERFORMANCE TARGET:** <5 minutes for 500 iterations with 3000 environments

**ğŸš¨ CRITICAL FIXES FOR COMMON REWARD ISSUES:**

**ISSUE 1: SLOW PER-ENVIRONMENT LOOPS**
```python
# âŒ WRONG (EXTREMELY SLOW):
for i in range(env.num_envs):
    vals = hm[i][valid_mask[i]]
    if vals.numel() > 0:
        dyn_baseline[i] = torch.median(vals)

# âœ… CORRECT (FAST):
valid_readings = torch.where(valid_mask, hm, torch.zeros_like(hm))
valid_counts = valid_mask.sum(dim=1)
has_valid = valid_counts > 0
mean_readings = valid_readings.sum(dim=1) / torch.clamp(valid_counts, min=1.0)
dyn_baseline = torch.where(has_valid, mean_readings, torch.full_like(mean_readings, 0.209))
```

**ISSUE 2: WRONG VELOCITY TRACKING**
```python
# âŒ WRONG (LINEAR):
vel_err = torch.norm(commands[:, :2] - vel_yaw[:, :2], dim=1)
vel_reward = torch.clamp(1.0 - vel_err, 0.0, 2.0)

# âœ… CORRECT (EXPONENTIAL):
lin_vel_error = torch.sum(torch.square(commands[:, :2] - vel_yaw[:, :2]), dim=1)
vel_reward = torch.exp(-lin_vel_error / (0.25)**2)
```

**ISSUE 3: MISSING PROPER GAIT PATTERN**
```python
# âœ… ADD MISSING in_mode_time PATTERN:
in_mode_time = torch.where(in_contact, contact_time, air_time)
gait_reward = torch.min(torch.where(single_stance.unsqueeze(-1), in_mode_time, 0.0), dim=1)[0]
gait_reward = torch.clamp(gait_reward, max=0.3)  # Cap at 300ms
```

**APPLY THESE FIXES IMMEDIATELY FOR FAST, NATURAL WALKING**

**RUNTIME ERROR AUTO-FIX (feet_slide shape mismatch):**

If you see: `RuntimeError: The size of tensor a (44) must match the size of tensor b (2) at non-singleton dimension 1`

Cause: contacts and velocities indexed with DIFFERENT foot sets (regex mismatch) or `feet_slide` called with mismatched `body_names`.

Mandatory fix for next attempt (choose ONE):

Option A (preferred inline computation):
```python
contact_sensor = env.scene.sensors["contact_forces"]
foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)

forces = contact_sensor.data.net_forces_w_history[:, :, foot_ids, :].norm(dim=-1).max(dim=1)[0]
contacts = forces > 50.0

robot = env.scene["robot"]
body_vel = robot.data.body_lin_vel_w[:, foot_ids, :2]

assert body_vel.shape[:2] == contacts.shape, (
    f"feet_slide shape mismatch: vel {{tuple(body_vel.shape)}} vs contacts {{tuple(contacts.shape)}}"
)
slide_penalty = torch.sum(body_vel.norm(dim=-1) * contacts, dim=1)
```

Option B (keep feet_slide):
```python
slide_penalty = feet_slide(
    env,
    sensor_cfg=SceneEntityCfg("contact_forces", body_names=".*_ankle_roll_link"),
    asset_cfg=SceneEntityCfg("robot",           body_names=".*_ankle_roll_link")
)
```

Do NOT mix different `body_names` across cfgs.

 #3 TRAINING CRASH: TORCH.CLAMP TYPEERROR - 


**CRITICAL TORCH.CLAMP ARGUMENT ERROR:**

**TYPEERROR: clamp() received an invalid combination of arguments - got (float, min=float)**
- CAUSE: Using torch.clamp() with scalar values instead of tensors in nested operations
- CRASH LINE EXAMPLE:
```python
# âŒ WRONG: This creates invalid argument combination
result = torch.clamp(1.0 - value / torch.clamp(tol, min=1e-3), 0.0, 1.0)
# ERROR: When tol is a float, torch.clamp(tol, min=1e-3) fails
```

**âœ… IMMEDIATE FIX PATTERNS:**

**Pattern 1: Use torch.clamp correctly with scalars**
```python
# âŒ WRONG: Nested clamp with scalar
denominator = torch.clamp(torch.tensor(tol, device=device), min=1e-3)
# OR use Python max for scalars:
denominator = max(tol, 1e-3)
```

**Pattern 2: Safe division with tensor operations**
```python
# âŒ WRONG: Mixing scalar and tensor operations
result = torch.clamp(1.0 - (a - b).abs() / torch.clamp(tol, min=1e-3), 0.0, 1.0)

# âœ… CORRECT: Ensure all operations use tensors consistently
safe_denominator = torch.clamp(torch.tensor(tol).to(device), min=1e-3)
result = torch.clamp(1.0 - (a - b).abs() / safe_denominator, 0.0, 1.0)

# âœ… ALTERNATIVE: Use Python max for scalar safety
safe_denominator = max(tol, 1e-3)
result = torch.clamp(1.0 - (a - b).abs() / safe_denominator, 0.0, 1.0)
```

**Pattern 3: Proper tensor device handling**
```python
# âœ… CORRECT: Always specify device for tensor creation
if isinstance(tolerance, (int, float)):
    tolerance = torch.tensor(tolerance, device=input_tensor.device)
safe_tolerance = torch.clamp(tolerance, min=1e-3)
```

