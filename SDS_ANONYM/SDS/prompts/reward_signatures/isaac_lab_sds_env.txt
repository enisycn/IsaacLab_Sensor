🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟
🌟🌟🌟 ENVIRONMENT-AWARE LOCOMOTION REWARD DESIGN GUIDANCE 🌟🌟🌟
🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟🌟

**CRITICAL PROJECT UNDERSTANDING:**

These guidance explanations are designed for creating **environment-aware locomotion rewards** that enable **intelligent sensor-driven behavior adaptation** for humanoid robots.

**🎯 PRIMARY OBJECTIVE:** 
Create reward functions that make **measurable positive impact when sensors are used** compared to **without sensor usage**.

**🔬 RESEARCH PURPOSE:**
Enable comparison studies demonstrating:
- **WITHOUT SENSORS:** Basic locomotion behavior 
- **WITH SENSORS:** Adaptive, context-aware behavior that responds to environmental challenges

**🧠 DESIGN METHODOLOGY:**
1. **ANALYZE:** Understand the environment (gaps, obstacles, terrain complexity)
2. **THINK:** Determine which sensors are needed and how they should influence behavior
3. **DECIDE:** Implement context-aware behavioral switching (NOT simultaneous conflicting rewards)
4. **IMPACT:** Ensure sensors create observable behavioral differences for scientific comparison

**📊 SUCCESS CRITERIA:**
✅ Robot behaves measurably different with sensors vs. without sensors
✅ Sensor-enabled robot adapts to environmental challenges more effectively
✅ Clear behavioral switching based on environmental context
✅ No conflicting simultaneous behaviors (e.g., walking + jumping simultaneously)

**⚠️ FAILURE INDICATORS:**
❌ Robot behaves identically with/without sensors
❌ Sensors provide only minor bonuses without changing core behavior
❌ Conflicting reward objectives that confuse the policy

---

🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨��🚨🚨🚨🚨🚨🚨🚨🚨🚨
🚨🚨🚨 ISAAC LAB STANDARD: RAW SENSOR ACCESS FOR REWARD FUNCTIONS 🚨🚨🚨
🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨

**🚨 CRITICAL: NEVER IMPORT ISAAC LAB MDP FUNCTIONS IN CUSTOM REWARDS!**
```python
# ❌ THESE WILL CAUSE ImportError:
from __main__ import feet_air_time_positive_biped
from isaaclab.mdp import any_function
import isaaclab_tasks.manager_based.locomotion.velocity.mdp as mdp

# ✅ SAFE IMPORTS ONLY:
# NOTE: torch, quat_apply_inverse, yaw_quat, SceneEntityCfg already imported in rewards.py
```

**✅ IMPLEMENT ALL PATTERNS INLINE - DON'T IMPORT MDP FUNCTIONS**

🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯
🎯🎯🎯 SENSOR-DRIVEN BEHAVIORAL ADAPTATION: MANDATORY PROJECT REQUIREMENT 🎯🎯🎯
🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯

**🚨 CRITICAL: SENSORS MUST CREATE MEASURABLE BEHAVIORAL DIFFERENCES! 🚨**

**PROJECT PURPOSE: Enable with/without sensor comparison studies**

**❌ FORBIDDEN: Conflicting simultaneous behaviors:**
```python
# BAD - Robot confused between walking and jumping at same time
walking_air_time = 0.3  # Want short air time for walking
jumping_air_time = 0.8  # Want long air time for jumping
total = walking_reward + jumping_reward  # CONFLICTING OBJECTIVES!
```

**✅ REQUIRED: Context-aware behavioral switching:**
```python
# GOOD - Robot adapts behavior based on sensor input
gap_ahead = height_measurements[:, front_indices] < -0.2  # Look ahead for gaps
obstacle_ahead = lidar_distances[:, front_rays] < 2.0     # Look ahead for obstacles

# Behavioral switching based on sensor input
if torch.any(gap_ahead, dim=1).any():
    # DIFFERENT BEHAVIOR: Prepare for gap crossing
    behavior = foundation + gap_preparation_behavior()
elif torch.any(obstacle_ahead, dim=1).any():
    # DIFFERENT BEHAVIOR: Navigate around obstacles  
    behavior = foundation + obstacle_avoidance_behavior()
else:
    # EFFICIENT BEHAVIOR: Normal walking without sensors
    behavior = foundation_locomotion_only()
```

**🎯 SENSOR IMPACT REQUIREMENTS:**

**1. HEIGHT SCANNER BEHAVIORAL IMPACT:**
- **Flat terrain**: Robot walks normally (sensor unused)
- **Gap terrain**: Robot modifies gait BEFORE reaching gaps (predictive adaptation)
- **Stair terrain**: Robot adjusts height expectations (terrain following)

**2. LIDAR BEHAVIORAL IMPACT:**
- **Open terrain**: Robot walks efficiently (sensor unused)
- **Obstacle terrain**: Robot maintains safe distances, careful navigation
- **Dense obstacles**: Robot moves conservatively, plans paths

**3. COMPARISON STUDY VALIDATION:**
- **Without sensors**: Generic foundation walking → fails on challenging terrain
- **With sensors**: Adaptive behavior → succeeds on challenging terrain
- **Measurable improvement**: Success rate, stability, efficiency gains

**🎯 IMPLEMENTATION PATTERNS FOR BEHAVIORAL ADAPTATION:**

**PATTERN A: Gap Detection → Gait Modification:**
```python
# Use height sensor to detect gaps ahead and modify gait accordingly
forward_height = height_measurements[:, front_sector_indices]  # Look ahead
gap_ahead = torch.any(forward_height < -0.15, dim=1)  # 15cm gap threshold

# Modify gait timing when gaps detected
normal_air_time = 0.3   # Regular walking
gap_air_time = 0.5      # Extended air time for gap crossing

# Context-aware air time selection
target_air_time = torch.where(gap_ahead, gap_air_time, normal_air_time)
adaptive_gait = implement_air_time_reward(target_air_time)
```

**PATTERN B: Obstacle Detection → Path Planning:**
```python
# Use LiDAR to detect obstacles ahead and plan safe navigation
forward_lidar = lidar_distances[:, front_ray_indices]  # Look ahead
obstacle_ahead = torch.any(forward_lidar < 1.5, dim=1)  # 1.5m safety threshold

# Modify velocity when obstacles detected
normal_velocity = 1.0     # Regular walking speed
careful_velocity = 0.6    # Reduced speed near obstacles

# Context-aware velocity selection
target_velocity = torch.where(obstacle_ahead, careful_velocity, normal_velocity)
adaptive_navigation = implement_velocity_tracking(target_velocity)
```

**PATTERN C: Environmental Context → Behavior Selection:**
```python
# Analyze environment to select appropriate behavior mode
terrain_context = analyze_environment(height_sensor, lidar_sensor)

# Different reward strategies for different contexts
if terrain_context == "flat_open":
    behavior = efficient_walking_mode()
elif terrain_context == "gaps_present":
    behavior = gap_crossing_mode()
elif terrain_context == "obstacles_present":
    behavior = obstacle_navigation_mode()
elif terrain_context == "complex_mixed":
    behavior = adaptive_mixed_terrain_mode()
```

✅ ISAAC LAB STANDARD RAW SENSOR ACCESS:
```python
# Raw sensor access - physically meaningful measurements in meters
height_sensor = env.scene.sensors["height_scanner"]
lidar_sensor = env.scene.sensors["lidar"]

# Height measurements: terrain height relative to sensor position (in meters)
height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5

# Distance measurements: actual distances to obstacles (in meters)
lidar_distances = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)

# Physical thresholds with clear meaning:
significant_gaps = height_measurements < -0.2    # 20cm below sensor = gap
close_obstacles = lidar_distances < 2.0          # 2m distance = close obstacle
```

**Isaac Lab RayCaster sensors ONLY have these attributes:**
- `data.pos_w` - Sensor position [num_envs, 3]
- `data.quat_w` - Sensor orientation [num_envs, 4]  
- `data.ray_hits_w` - Ray hit positions [num_envs, num_rays, 3]

**✅ TERRAIN-ANALYSIS-BASED HEIGHT TRACKING:**

**ABSOLUTE HEIGHT - For consistent body clearance:**
```python
# Use when: Flat terrain OR gap crossing (large stepping) OR consistent platform heights
height_err = torch.abs(robot.data.root_pos_w[:, 2] - 0.74)  # Target height above world origin
height_reward = torch.exp(-height_err / 0.3)
```

**TERRAIN-RELATIVE HEIGHT - For adaptive surface following:**
```python
# Use when: Variable terrain heights OR climbing stairs OR navigating slopes
height_sensor = env.scene.sensors["height_scanner"]
height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5
avg_terrain_height = height_measurements.mean(dim=-1)  # Average terrain height in meters
# Robot height above terrain surface (corrected calculation)
robot_terrain_relative_height = robot.data.root_pos_w[:, 2] - avg_terrain_height
height_err = torch.abs(robot_terrain_relative_height - 0.74)  # Maintain 0.74m above terrain
```

**CHOOSE BASED ON ENVIRONMENT ANALYSIS:**
- Large gaps requiring stepping → Absolute height maintains clearance
- Variable surface heights → Terrain-relative adapts to surface
- Consistent platform levels → Absolute height for smooth transitions
- Climbing/descending behavior needed → Terrain-relative for adaptation

**❌ ARM BILATERAL SYNCHRONY ERROR - CAUSES UNNATURAL ARM CROSSING:**
```python
# WRONG - Forces both arms to same angle (bilateral synchrony):
mean_sh = torch.mean(torch.abs(sh_angles), dim=1)  # Both arms same angle!
arm_err = (mean_sh - 0.3)**2  # Forces both to 17° - UNNATURAL!
```

**✅ NATURAL ARM MOVEMENT - THREE PROVEN OPTIONS:**
```python
# OPTION 1: NO ARM REWARDS (let physics/gravity handle naturally)
# - Often produces better results than forced constraints
# - Arms naturally swing as passive pendulums

# OPTION 2: Reciprocal arm-leg coordination (if needed):
left_arm = sh_angles[:, 0]   # Left shoulder angle
right_arm = sh_angles[:, 1]  # Right shoulder angle
reciprocal_reward = -torch.abs(left_arm + right_arm)  # Reward opposite directions

# OPTION 3: Keep arms near default positions (RECOMMENDED for stable walking):
target_shoulder_pos = 0.35  # G1 asset default (arms slightly forward)
arm_deviation = torch.mean(torch.abs(sh_angles - target_shoulder_pos), dim=1)
arm_reward = torch.exp(-arm_deviation / 0.2) * 0.1  # Gaussian reward for staying near defaults
```

These patterns are battle-tested and known to work for humanoid locomotion. Use them as a foundation:

**🏆 ISAAC LAB PROVEN REWARD FUNCTIONS (USE THESE FIRST!)**

**These are production-ready Isaac Lab functions that create excellent human-like walking:**

**1. BIPEDAL AIR TIME REWARD (SUPERIOR FOR HUMANOIDS) - SINGLE STANCE IS CRITICAL:**
```python
# 🎯 PROVEN: Built-in Isaac Lab function for bipedal gait patterns
# CRITICAL INSIGHT: Natural human walking = alternating SINGLE SUPPORT phases!
contact_sensor = env.scene.sensors["contact_forces"]
foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)

# Access air time and contact time data (VERIFIED WORKING)
air_time = contact_sensor.data.current_air_time[:, foot_ids]
contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
in_contact = contact_time > 0.0

# 🚀 KEY FOR NATURAL WALKING: Single stance reward (one foot at a time)
# This is what makes human walking look natural vs robotic shuffling!
single_stance = torch.sum(in_contact.int(), dim=1) == 1  # ONLY ONE FOOT DOWN!
in_mode_time = torch.where(in_contact, contact_time, air_time)
gait_reward = torch.min(torch.where(single_stance.unsqueeze(-1), in_mode_time, 0.0), dim=1)[0]

# ⚡ PROPER AIR TIME MANAGEMENT: 
# - Threshold prevents excessive foot lifting (unnatural high knees)
# - Rewards natural step timing for smooth walking rhythm
gait_reward = torch.clamp(gait_reward, max=0.5)  # Cap at 0.5s for natural step timing

# 🎯 COMMAND DEPENDENCY: No reward for zero command (prevents stationary exploitation)
commands = env.command_manager.get_command("base_velocity")
command_magnitude = torch.norm(commands[:, :2], dim=1)
gait_reward *= (command_magnitude > 0.1).float()  # Only reward when actually moving
```

**WHY THIS PATTERN IS SUPERIOR TO GENERIC AIR TIME:**
- **Single stance detection**: Natural walking requires alternating support (not double support shuffling)
- **Contact-aware timing**: Uses actual contact sensor data, not estimated patterns
- **Anti-exploitation**: No reward for standing still or micro-movements
- **Natural rhythm**: 0.5s threshold matches human walking cadence
- **Isaac Lab optimized**: Uses proven sensor patterns that work reliably

**2. YAW-ALIGNED VELOCITY TRACKING (PROVEN SUPERIOR TO BODY FRAME):**
```python
# 🎯 PROVEN: Isaac Lab's yaw-frame tracking - VASTLY superior to basic body frame
# CRITICAL: Decouples velocity control from robot tilt/lean (prevents control coupling)
# NOTE: quat_apply_inverse, yaw_quat already available in rewards.py

robot = env.scene["robot"]
commands = env.command_manager.get_command("base_velocity")

# 🚀 YAW-ALIGNED TRANSFORMATION: Removes pitch/roll interference
# This is why Isaac Lab velocity tracking works so much better!
vel_yaw = quat_apply_inverse(yaw_quat(robot.data.root_quat_w), robot.data.root_lin_vel_w[:, :3])
lin_vel_error = torch.sum(torch.square(commands[:, :2] - vel_yaw[:, :2]), dim=1)
vel_reward = torch.exp(-lin_vel_error / (1.0**2))  # Exponential kernel for smooth decay

# 🎯 CRITICAL: No reward for zero commands (anti-exploitation)
vel_reward *= (torch.norm(commands[:, :2], dim=1) > 0.1).float()
```

**WHY YAW-ALIGNED IS SUPERIOR:**
- **Decoupled control**: Velocity tracking unaffected by robot lean/tilt
- **Stable tracking**: Works even when robot pitches forward/backward
- **Natural movement**: Allows body dynamics while maintaining velocity goals
- **Isaac Lab proven**: This is the actual pattern used in successful Isaac Lab locomotion

**3. ANGULAR VELOCITY TRACKING (WORLD FRAME FOR STABILITY):**
```python
# 🎯 PROVEN: Isaac Lab's world frame angular tracking (superior to body frame)
commands = env.command_manager.get_command("base_velocity")
robot = env.scene["robot"]

# World frame angular velocity for consistent turning control
ang_vel_error = torch.square(commands[:, 2] - robot.data.root_ang_vel_w[:, 2])
ang_reward = torch.exp(-ang_vel_error / (1.0**2))
```

**4. CONTACT-AWARE FOOT SLIDING PENALTY (INTELLIGENT SLIDING DETECTION):**
```python
# 🎯 PROVEN: Isaac Lab's sliding penalty - ONLY when foot is actually in contact
# CRITICAL: This prevents penalizing swing leg motion (which should move freely!)
contact_sensor = env.scene.sensors["contact_forces"]
foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)

# 🚀 SMART CONTACT DETECTION: Use force history for reliable contact state
forces = contact_sensor.data.net_forces_w_history[:, :, foot_ids, :]
contacts = forces.norm(dim=-1).max(dim=1)[0] > 1.0  # Actual contact detection

# Get foot velocities and apply sliding penalty ONLY when in contact
robot = env.scene["robot"]
body_vel = robot.data.body_lin_vel_w[:, foot_ids, :2]
slide_penalty = torch.sum(body_vel.norm(dim=-1) * contacts, dim=1)  # Contact-aware!
```

**WHY CONTACT-AWARE SLIDING IS CRITICAL:**
- **Swing phase freedom**: Doesn't penalize moving feet during swing phase
- **Realistic physics**: Only applies when feet should be stationary (in contact)
- **Force-based detection**: Uses actual physics data, not position estimates
- **Natural walking**: Allows proper foot lifting and placement

**🔑 CRITICAL SUCCESS FACTORS FOR BIPEDAL WALKING:**

1. **SINGLE STANCE DOMINANCE**: Most natural walking happens in single support!
2. **YAW-ALIGNED CONTROL**: Decouples velocity from body orientation
3. **CONTACT-AWARE PENALTIES**: Only apply constraints when physically relevant
4. **COMMAND SCALING**: No rewards for micro-movements or standing still
5. **NATURAL TIMING**: Air time thresholds that match human walking rhythm (0.3-0.5s)
6. **FORCE-BASED DETECTION**: Use contact sensors, not position approximations

**🚨 COMMON BIPEDAL WALKING FAILURES TO AVOID:**
- **Double support shuffling**: Not rewarding single stance phases
- **Robotic high stepping**: No upper threshold on air time rewards
- **Control coupling**: Using body frame instead of yaw-aligned frame
- **Swing leg penalties**: Penalizing foot motion during swing phase
- **Zero command exploitation**: Rewarding stationary micro-movements

**✅ USE THESE ISAAC LAB PATTERNS AS YOUR FOUNDATION - THEY ARE BATTLE-TESTED!**

**2. PROPER BIPEDAL GAIT PATTERNS (UPDATED WITH ISAAC LAB INSIGHTS):**
```python
# 🎯 PROVEN: Rewards single stance phases (proper walking pattern)
# CRITICAL FOR WALKING: Human walking = 85% single support, 15% double support!
foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)

air_time = contact_sensor.data.current_air_time[:, foot_ids]
contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
in_contact = contact_time > 0.0

# 🚀 SINGLE STANCE DETECTION: The secret to natural walking!
single_stance = torch.sum(in_contact.int(), dim=1) == 1  # One foot at a time
in_mode_time = torch.where(in_contact, contact_time, air_time)
gait_reward = torch.min(torch.where(single_stance.unsqueeze(-1), in_mode_time, 0.0), dim=1)[0]

# 🎯 NATURAL STEP TIMING: Prevent robotic high-stepping
gait_reward = torch.clamp(gait_reward, max=0.5) * (command_magnitude > 0.1).float()

# 🔥 ALTERNATIVE PATTERN - Double Support Bonus (for stability when needed):
double_stance = torch.sum(in_contact.int(), dim=1) == 2  # Both feet down
stability_bonus = double_stance.float() * 0.1  # Small bonus for stable phases
```

**ADVANCED BIPEDAL PATTERNS FOR SPECIFIC GAITS:**

**WALKING GAIT (Primary Pattern):**
```python
# Natural walking = alternating single support + brief double support
walking_pattern = single_stance.float() * 0.8 + double_stance.float() * 0.2
```

**RUNNING GAIT (Flight Phase):**
```python
# Running = single support + flight phase (no double support)
flight_phase = torch.sum(in_contact.int(), dim=1) == 0  # Both feet up
running_pattern = single_stance.float() * 0.6 + flight_phase.float() * 0.4
```

**MARCHING GAIT (Controlled Single Support):**
```python
# Marching = extended single support phases for precision
extended_single = single_stance & (in_mode_time.max(dim=1)[0] > 0.3)
marching_pattern = extended_single.float()
```

**🚨 PRIORITY: Use these proven Isaac Lab functions as your foundation, then add task-specific enhancements!**

**🔥 ADDITIONAL ISAAC LAB BUILT-IN FUNCTION USAGE:**

When environment analysis shows you need specific locomotion patterns, you can use Isaac Lab's built-in reward functions directly in your reward configuration. However, for SDS custom rewards, you should implement the patterns inline as shown above.

**BUILT-IN FUNCTION REFERENCE (for understanding, not direct use in custom rewards):**
```python
# These are the actual Isaac Lab functions - understand their patterns:
# mdp.feet_air_time_positive_biped(env, command_name="base_velocity", threshold=0.5, sensor_cfg=SceneEntityCfg("contact_forces"))
# mdp.track_lin_vel_xy_yaw_frame_exp(env, std=1.0, command_name="base_velocity")
# mdp.track_ang_vel_z_world_exp(env, command_name="base_velocity", std=1.0)
# mdp.feet_slide(env, sensor_cfg=SceneEntityCfg("contact_forces"))
```

**🎯 CRITICAL ISAAC LAB SUCCESS INSIGHTS:**
- **Command scaling**: NEVER reward when commands are near zero - prevents exploitation
- **Yaw alignment**: Removes pitch/roll interference from velocity tracking - critical for stability
- **Single stance**: Encourages proper alternating foot pattern - key for natural walking
- **Contact awareness**: Only apply penalties when actually relevant (foot in contact) - prevents swing phase penalties
- **Capped rewards**: Air time and other metrics should have reasonable upper bounds - prevents over-optimization
- **Force-based detection**: Use contact sensor forces, not position estimates - more reliable
- **Exponential kernels**: Provide smooth reward gradients for stable learning - better than linear penalties

## ISAAC LAB REWARD COMPUTATION PATTERNS

**Focus: Technical implementation for reward functions, not biomechanical theory**

** ENHANCED ENVIRONMENT REWARD PATTERNS (NEW - FOR ADVANCED LOCOMOTION):**

**4. VELOCITY-OBSTACLE CONFLICT RESOLUTION MATH:**
```python
# Enhanced sensor-based dynamic target modification for reward computation
height_sensor = env.scene.sensors["height_scanner"]
lidar_sensor = env.scene.sensors["lidar"]
contact_sensor = env.scene.sensors["contact_forces"]

# Process sensor data with appropriate sanitization using Isaac Lab standard
height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5
height_measurements = torch.where(torch.isfinite(height_measurements), height_measurements, torch.zeros_like(height_measurements))

# STEP 2: ADAPTIVE CORRELATION ARCHITECTURE
# Create flexible prediction-validation system
environmental_predictions = extract_environmental_context(height_measurements, lidar_sensor)
contact_classification = categorize_contact_events(contact_sensor, environmental_predictions)

# STEP 3: CONTEXT-SENSITIVE SCALING
# Scale rewards based on environmental assessment, not fixed values
environmental_complexity = assess_terrain_complexity(height_measurements)
correlation_confidence = measure_sensor_agreement(height_sensor, lidar_sensor)
reward_scaling = compute_adaptive_scaling(environmental_complexity, correlation_confidence)

# STEP 4: INTELLIGENT CORRELATION IMPLEMENTATION
# Reward/penalize based on prediction-reality correlation
contact_intelligence = correlate_contact_with_predictions(contact_classification, environmental_predictions)
terrain_aware_reward = contact_intelligence * reward_scaling  # Adaptive, not fixed values

# TECHNICAL GUIDELINES FOR FLEXIBLE IMPLEMENTATION:
# - Extract environmental context from available sensors
# - Classify contact events by their environmental appropriateness  
# - Scale correlation strength based on sensor confidence and environmental complexity
# - Adapt reward magnitudes to situational requirements, not fixed penalty/reward values
```

**CRITICAL: TRAINING WILL FAIL WITHOUT THESE FIXES!**

**GUARANTEED TRAINING FAILURE - COMMON BUGS THAT CRASH THE SYSTEM:**

1. **TENSOR CONVERSION BUG (CAUSES TypeError):**
   ```python
   # TRAINING KILLER - NEVER DO THIS:
   indices, _ = robot.find_joints(["joint_name"])
   data = robot.data.joint_pos[:, indices]  # INSTANT CRASH!
   
   # MANDATORY FIX - ALWAYS DO THIS:
   indices, _ = robot.find_joints(["joint_name"])
   indices = torch.tensor(indices, dtype=torch.long, device=env.device)
   data = robot.data.joint_pos[:, indices]  # WORKS!
   ```

2. **NUMERICAL INSTABILITY BUG (CAUSES "std >= 0.0" ERROR):**
   ```python
   # TRAINING KILLER:
   reward = torch.exp(-huge_value)  # Creates NaN/inf!
   
   # MANDATORY FIX:
   reward = torch.exp(-torch.clamp(value, max=10.0))
   reward = torch.where(torch.isfinite(reward), reward, torch.zeros_like(reward))
   ```

**EVERY SINGLE find_joints() CALL MUST BE FOLLOWED BY torch.tensor() CONVERSION!**

**ALL NUMERICAL EXAMPLES, CODE SNIPPETS, AND REWARD PATTERNS IN THIS PROMPT ARE FOR TECHNICAL DEMONSTRATION ONLY.**
**DO NOT COPY EXAMPLES DIRECTLY! UNDERSTAND THE PRINCIPLES AND ADAPT TO YOUR ENVIRONMENT.**

** CRITICAL: NO HELPER FUNCTIONS ALLOWED! **

** ABSOLUTELY FORBIDDEN:**
- `def get_velocity_tracking_error(...)` - NO HELPER FUNCTIONS!
- `def calculate_foot_contacts(...)` - NO HELPER FUNCTIONS!
- `def any_helper_function(...)` - NO HELPER FUNCTIONS!

**REQUIRED PATTERN: ALL LOGIC INLINE**
```python
def sds_custom_reward(env) -> torch.Tensor:
    # NOTE: torch already imported in rewards.py
    # ALL your calculation logic goes here directly - no function calls!
    lin_vel_error = torch.norm(robot.data.root_lin_vel_b[:, :2] - commands[:, :2], dim=1)  #  INLINE
    # NOT: lin_err, ang_err = get_velocity_tracking_error(...)  #  FORBIDDEN
    return reward.clamp(min=0.0, max=10.0)
```

**CRITICAL: ONLY GENERATE REWARD FUNCTIONS - NO OBSERVATION CONFIGURATIONS!**

**NEVER GENERATE THESE:**
- `def get_height_scan(env):` or any observation functions
- `lambda env: env.scene.sensors["height_scanner"].data.ray_hits_w[...]` 
- `ObsTerm(func=lambda env: ...)` observation configurations
- Environment sensor configurations
- Any code outside the reward function

**ONLY GENERATE THIS:**
- `def sds_custom_reward(env) -> torch.Tensor:` function only
- Access existing sensors WITHIN the reward function

**The environment already provides all sensor data - just use it in your reward!**

**CRITICAL: PPO CRASHES WITH "std >= 0.0" ERROR WITHOUT THESE!**

**GUARANTEED PPO FAILURE PATTERNS:**

```python
# DEADLY: Environmental sensor data contains NaN/Inf values
height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5  # Can contain NaN!
reward += torch.mean(height_measurements)  # NaN propagates, crashes PPO

# DEADLY: Unbounded reward values crash PPO standard deviation  
reward = some_large_calculation  # Can be >100, causes std <= 0 error

# DEADLY: Division by zero in environmental calculations
reward = 1.0 / distance_to_obstacle  # Zero distance = Inf reward = PPO crash
```

**PPO-SAFE ENVIRONMENTAL PATTERNS (WHEN USING ENVIRONMENTAL SENSORS):**

```python
# IF using environmental sensor data, ALWAYS sanitize:
height_measurements = torch.where(torch.isfinite(height_measurements), height_measurements, torch.zeros_like(height_measurements))
lidar_distances = torch.where(torch.isfinite(lidar_distances), lidar_distances, torch.ones_like(lidar_distances) * 10.0)

# IF using environmental calculations, ALWAYS clamp:
obstacle_distance = torch.clamp(min_obstacle_distance, min=0.1, max=15.0)
terrain_roughness = torch.clamp(torch.var(height_measurements, dim=1), max=0.01)

# ALWAYS use safe division:
reward = torch.exp(-error / torch.clamp(tolerance, min=1e-6))

# ALWAYS bound final reward for PPO stability:
return torch.clamp(reward, min=0.0, max=10.0)
```

Isaac Lab Reward Function Format:

**CRITICAL: JOINT INDEXING REQUIREMENT**

**COMMON BUG - WILL CAUSE TRAINING FAILURE:**
```python
# WRONG - robot.find_joints() returns LISTS, not tensors!
joint_indices, _ = robot.find_joints(["joint_name"])
joint_data = robot.data.joint_pos[:, joint_indices]  # TypeError!
```

**CORRECT PATTERN - ALWAYS CONVERT TO TENSOR:**
```python
# RIGHT - Convert list to tensor for proper indexing
joint_indices, _ = robot.find_joints(["joint_name"])
joint_indices = torch.tensor(joint_indices, dtype=torch.long, device=env.device)
joint_data = robot.data.joint_pos[:, joint_indices]  # Works!
```

**MANDATORY: Every time you use robot.find_joints(), immediately convert the result to a tensor!**

Isaac Lab SDS Environment - G1 Humanoid Locomotion

## Robot Configuration (VERIFIED & UPDATED FOR FULL BODY CONTROL)
- **Robot**: Unitree G1 EDU U4 Humanoid (37 DOF total)
- **Action Space**: 23 DOF controlled for complete humanoid locomotion (all joints except hand fingers)
- **Height**: 0.74m (Isaac Lab verified)
- **Mass**: ~35kg humanoid

## Action Configuration (FULL BODY HUMANOID CONTROL)
**Controlled Joints (23 DOF for complete humanoid locomotion):**
- Legs: 12 DOF (6 per leg: hip_yaw, hip_roll, hip_pitch, knee, ankle_pitch, ankle_roll)
- Arms: 10 DOF (5 per arm: shoulder_pitch, shoulder_roll, shoulder_yaw, elbow_pitch, elbow_roll)
- Torso: 1 DOF (torso_joint)

**Fixed Joints (14 DOF):**
- Hand Fingers: 14 DOF maintain default poses (zero, one, two, three, four, five, six_joint per hand)

## Contact Detection (VERIFIED WORKING)
**Foot Bodies**: `left_ankle_roll_link`, `right_ankle_roll_link`
**Detection Pattern**: `contact_sensor.find_bodies(".*_ankle_roll_link")`
**Contact Threshold**: 50.0N (corrected for humanoid mass)

## Key Functions Examples for Reward Generation. Don't Copy Paste Directly!
```python
# Foot contact detection (VERIFIED WORKING)
contact_forces = env.scene.sensors["contact_forces"].data.net_forces_w
foot_ids, _ = env.scene.sensors["contact_forces"].find_bodies(".*_ankle_roll_link")
foot_forces = contact_forces[:, foot_ids, :]
foot_contacts = (foot_forces.norm(dim=-1) > 50.0).float()

# Velocity tracking
robot = env.scene["robot"]
commands = env.command_manager.get_command("base_velocity")
vel_error = robot.data.root_lin_vel_b[:, :2] - commands[:, :2]
ang_error = robot.data.root_ang_vel_b[:, 2] - commands[:, 2]

# Height maintenance  
height = robot.data.root_pos_w[:, 2]
height_error = (height - 0.74).abs()

# Bipedal gait patterns
left_contact = foot_contacts[:, 0]
right_contact = foot_contacts[:, 1] 
single_support = ((left_contact > 0.5) & (right_contact < 0.5)) | ((left_contact < 0.5) & (right_contact > 0.5))
double_support = (left_contact > 0.5) & (right_contact > 0.5)
```

## Successful Training Metrics
- **Episode Rewards**: 0.01-0.02 range (working)
- **Action Scale**: 1.0 (allows proper joint movement)
- **Training Progress**: Mean rewards ~0.2-0.23, episode length 28-32 steps

# TECHNICAL REFERENCE - ISAAC LAB API DOCUMENTATION

## Robot Data Access Patterns
    
    # ROBOT DATA FIRST APPROACH:
    # Available robot data:
    # robot.data.root_pos_w[:, 2] - height (z-coordinate, nominal 0.74m for G1)
    # robot.data.root_lin_vel_b[:, 0] - forward velocity (x-axis in body frame)
    # robot.data.root_lin_vel_w - linear velocity in world frame [num_envs, 3]
    # robot.data.root_ang_vel_b - angular velocity in body frame [num_envs, 3]
    # robot.data.root_ang_vel_w - angular velocity in world frame [num_envs, 3]
    # robot.data.root_quat_w - orientation quaternion [w,x,y,z]
    # robot.data.joint_pos - joint positions [num_envs, 37] for G1 EDU U4 with dexterous hands (VERIFIED)
    # robot.data.joint_vel - joint velocities [num_envs, 37] (VERIFIED)
    # CONTROLLED JOINTS (23 DOF): Use robot.find_joints() to get indices for legs + arms + torso (all except hand fingers)
    # HAND FINGER JOINTS (14 DOF): Excluded from control but can be accessed if needed
    # robot.data.root_pos_w[:, 2] - robot height [num_envs] - should be around 0.74m for G1 in Isaac Lab (VERIFIED)
    # robot.data.root_lin_vel_b - linear velocity in body frame [num_envs, 3]
    # robot.data.root_quat_w - quaternion orientation [num_envs, 4]
    # commands[:, :3] - [forward_vel, lateral_vel, yaw_rate] commands
    
    # Available command data:
    # commands[:, 0] - desired forward velocity (vx)
    # commands[:, 1] - desired lateral velocity (vy) 
    # commands[:, 2] - desired angular velocity (omega_z)
    # ADAPTIVE ROBOT CONFIGURATION:
    # DO NOT hardcode joint counts or specific robot parameters
    # Use dynamic robot configuration detection:
    num_joints = robot.data.joint_pos.shape[1]
    robot_height_baseline = robot.data.root_pos_w[:, 2].mean()  # Adaptive baseline
    
    # OPTIONAL IMU SENSOR (use as fallback only):
    # An IMU sensor "imu" can be spawned via ImuCfg if needed:
    # ImuCfg:
    #   prim_path: "/World/envs/env_.*/Robot/torso_link"
    #   update_period: 0.02  # 50 Hz
    #   gravity_bias: [0.0, 0.0, 9.81]
    # It exposes in env.scene.sensors["imu"].data:
    # - pos_w: FloatTensor [num_envs, 3] - World position
    # - quat_w: FloatTensor [num_envs, 4] - World orientation (w,x,y,z)
    # - lin_vel_b: FloatTensor [num_envs, 3] - Body-frame linear velocity
    # - ang_vel_b: FloatTensor [num_envs, 3] - Body-frame angular velocity
    # - lin_acc_b: FloatTensor [num_envs, 3] - Body-frame linear acceleration 
    # - ang_acc_b: FloatTensor [num_envs, 3] - Body-frame angular acceleration
    
    # Initialize reward (4-space indent)
    reward = torch.zeros(env.num_envs, dtype=torch.float32, device=env.device)
    
    # IMPORTANT: For contact analysis, use this inline approach:
    # Get foot contact forces for G1 humanoid
    contact_forces = contact_sensor.data.net_forces_w  # [num_envs, num_bodies, 3]
    foot_ids, foot_names = contact_sensor.find_bodies(".*_ankle_roll_link")
    foot_forces = contact_forces[:, foot_ids, :]  # [num_envs, 2, 3] - 2 feet for humanoid
    force_magnitudes = foot_forces.norm(dim=-1)  # [num_envs, 2]
    
    # Contact detection - analyze video to determine appropriate threshold
    # G1 humanoid requires higher thresholds: gentle gaits (20-50N), dynamic gaits (50-100N)
    # Evidence: G1 standing forces ~150-250N, much higher than quadrupeds
    contact_threshold = 50.0  # Default for G1 humanoid - adjust based on observed contact forces in video
    foot_contacts = (force_magnitudes > contact_threshold).float()  # Convert to float for partial credit
    
    # Note: Design contact rewards based on the observed gait pattern in the video
    # G1 humanoid bipedal locomotion: left_ankle_roll_link, right_ankle_roll_link
    
    
    # HUMANOID-SPECIFIC CONSIDERATIONS:
    # - Bipedal stability is critical: balance and contact pattern rewards
    # - Height maintenance: G1 initial height is 0.74m in Isaac Lab (VERIFIED)
    # - Upper body stability: minimize arm swing, maintain upright torso
    # - Gait patterns: Walk (alternating with double support), Jump (synchronized takeoff/landing), March (controlled single support), Sprint (extended flight), Pace (lateral movement)
    # - Isaac Lab G1 joint structure: 37 DOF total (23 controlled + 14 fixed)
    # - CONTROLLED joint naming: left/right_hip_[yaw/roll/pitch]_joint, left/right_knee_joint, left/right_ankle_[pitch/roll]_joint, torso_joint, left/right_shoulder_[pitch/roll/yaw]_joint, left/right_elbow_[pitch/roll]_joint
    # - FIXED joint naming: left/right_[zero/one/two/three/four/five/six]_joint (hand fingers only)
    
    # HUMAN-LIKE LOCOMOTION DESIGN PRINCIPLES:
    # 1. Dynamic Balance: Humanoids require continuous balance management (not static stability)
    #    Consider torso orientation, roll/pitch control, and center of mass dynamics
    # 2. Movement Efficiency: Natural locomotion minimizes energy expenditure
    #    Consider smooth joint motion, appropriate muscle activation patterns
    # 3. Directional Preference: Forward movement often has higher priority than lateral/backward
    #    Consider command-dependent weighting based on intended movement direction
    # 4. Temporal Coordination: Natural gaits involve timing and rhythm
    #    Consider phase relationships between limbs, contact duration, and step timing
    # 5. Upper Body Integration: Arms and torso contribute to locomotion stability
    #    Consider how upper body motion supports or disrupts locomotion goals
    # 6. Adaptive Contact Patterns: Different gaits require different contact strategies
    #    Analyze video to determine appropriate contact timing and patterns for the demonstrated behavior
    
    # Contact sensor access pattern for Isaac Lab
    foot_ids, foot_names = contact_sensor.find_bodies(".*_ankle_roll_link")  # G1 uses ankle_roll_link for contact
    contact_forces = contact_sensor.data.net_forces_w[:, foot_ids, :]  # [num_envs, 2, 3] for G1 bipedal
    contact_magnitudes = torch.norm(contact_forces, dim=-1)  # [num_envs, 2]
    foot_contacts = contact_magnitudes > contact_threshold  # Binary contact detection
    
    # HUMANOID GAIT PATTERNS (not quadruped!)
    # G1 is BIPEDAL - only 2 feet: left_foot (index 0), right_foot (index 1)
    left_contact = foot_contacts[:, 0]   # Left foot contact
    right_contact = foot_contacts[:, 1]  # Right foot contact
    
    # Bipedal locomotion phases (CORRECTED - not quadruped patterns)
    double_support = left_contact & right_contact        # Both feet down (Walk/Jump)
    single_support_left = left_contact & ~right_contact  # Only left foot down (Walk/March/Sprint)
    single_support_right = ~left_contact & right_contact # Only right foot down (Walk/March/Sprint)
    flight_phase = ~left_contact & ~right_contact        # Both feet up (Jump/Sprint)
    
    # Key differences from quadruped robots:
    # - G1 height: 0.74m in Isaac Lab configuration - CRITICAL for height-based rewards
    # - Only 2 contact points (not 4)
    # - Bipedal gait patterns (alternating support, not complex quadruped gaits)
    # - Higher contact forces due to full body weight on fewer feet
    # - Dynamic balance required (not static stability like quadrupeds)
    # - Dexterous hands: 37 total DOF with advanced manipulation capabilities
    
## Sensor Access Patterns

**🚨 CRITICAL: CORRECT SENSOR DATA ACCESS PATTERNS**

**❌ COMMON ERRORS THAT CAUSE AttributeError:**
```python
# WRONG - These attributes do not exist in Isaac Lab:
height_measurements = height_sensor.data.height_measurements  # AttributeError!
lidar_distances = lidar_sensor.data.distances                 # AttributeError!
lidar_range = lidar_sensor.data.ray_distances                 # AttributeError!
```

**✅ CORRECT PATTERNS - USE THESE:**
```python
# Height Scanner - Isaac Lab standard access for terrain heights:
height_sensor = env.scene.sensors["height_scanner"]
height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5  # Meters relative to sensor

# LiDAR - Isaac Lab standard access for obstacle distances:
lidar_sensor = env.scene.sensors["lidar"]
lidar_distances = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)  # Distances in meters
```

**🚨 Isaac Lab RayCaster sensors only have these data attributes:**
- `data.pos_w`: Sensor position in world frame [num_envs, 3]
- `data.quat_w`: Sensor orientation [num_envs, 4]  
- `data.ray_hits_w`: Ray hit positions [num_envs, num_rays, 3]

**No other attributes exist! Always calculate distances from ray_hits_w!**

# OPTIONAL: HEIGHT SCANNER ("height_scanner") INTEGRATION:
# Provides detailed terrain elevation mapping around the robot (130 measurements per robot)
# Use only if terrain shows significant variation - Isaac Lab standard usage:
height_sensor = env.scene.sensors["height_scanner"]
height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5  # [num_envs, 130] - terrain relative to sensor
# Raw sensor access for advanced processing:
terrain_heights = height_sensor.data.ray_hits_w[..., 2]  # [num_envs, scan_points] World Z coordinates
robot_height = height_sensor.data.pos_w[:, 2]  # [num_envs] Scanner position height

# HEIGHT MEASUREMENTS INTERPRETATION:
# - Shape: [num_envs, 130] (13 x 10 grid pattern at 0.15m resolution, 2m x 1.5m coverage)
# - Values: Height relative to sensor position in meters (negative = below sensor, positive = above)
# - Range: -0.5m to +3.0m for practical terrain mapping
# - Usage: local_terrain = torch.mean(height_measurements.view(num_envs, -1), dim=1)

# LIDAR SCANNER ("lidar") - REQUIRED:
# Provides 180-degree forward obstacle detection and distance measurements  
lidar_sensor = env.scene.sensors["lidar"]
# ❌ INCORRECT: lidar_distances = env.scene.sensors["lidar"].data.ray_hits_w[..., 2]  # This gets Z coordinate, not distance!
# ✅ CORRECT: Isaac Lab standard distance calculation:
lidar_distances = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)  # [num_envs, 144] distances in meters
forward_rays = lidar_distances[:, :72]   # Forward-facing rays
lateral_rays = lidar_distances[:, 72:144] # Side and rear-facing rays

# LIDAR INTERPRETATION:
# - Shape: [num_envs, 144] (8 channels x 18 horizontal rays at 10° resolution)
# - Values: Distance to nearest obstacle along each ray in meters
# - Range: 0.1m (closest detection) to 5.0m max_range (no obstacle detected)
# - Usage: forward_obstacles = torch.min(lidar_distances[:, :72], dim=1)[0]

## Enhanced Environment Sensor Integration for Rewards (FLAT-WITH-BOX CONFIG)

# ENHANCED SENSOR DATA PROCESSING FOR REWARD COMPUTATION:
# Current environment: Isaac-SDS-Velocity-Flat-G1-Enhanced-v0
# Sensors: height_scanner (GridPatternCfg: 13x10 grid, 130 points, 15cm resolution, 2x1.5m coverage, 3.0m range), lidar (144 rays: 8 channels × 18 horizontal, 180° FOV, 5.0m range)

# 1. Height Scanner Processing (13x10 grid, 130 effective points):
height_sensor = env.scene.sensors["height_scanner"]
height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5
height_grid = height_measurements.view(env.num_envs, 13, 10)  # Actual grid size

# Forward terrain analysis (covers backward movement with negative velocity ranges)
forward_terrain = height_grid[:, :4, 3:7]  # Forward section (13x10 grid)
backward_terrain = height_grid[:, 9:, 3:7]  # Backward section
lateral_terrain = height_grid[:, 4:9, :]    # Lateral sections

# 2. LiDAR Processing (144 rays, 5.0m range):
lidar_sensor = env.scene.sensors["lidar"]
# ❌ INCORRECT: lidar_distances = env.scene.sensors["lidar"].data.ray_hits_w[..., 2]  # This gets Z coordinate, not distance!
# ✅ CORRECT: Isaac Lab standard distance calculation:
lidar_distances = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)  # [num_envs, 144] distances in meters
forward_rays = lidar_distances[:, :72]    # Forward-facing rays
lateral_rays = lidar_distances[:, 72:]    # Side and rear-facing rays

# 3. Adaptive Terrain Classification for Reward Scaling:
terrain_variance = torch.var(height_measurements.view(env.num_envs, -1), dim=1)
obstacle_density = (lidar_distances < 2.0).float().mean(dim=1)
terrain_complexity = torch.clamp(terrain_variance + obstacle_density, min=0.0, max=2.0)

# 4. Direction-Aware Obstacle Processing (for backward velocity support):
forward_obstacles = torch.min(forward_rays, dim=1)[0]
backward_obstacles = torch.min(backward_rays, dim=1)[0]
movement_clearance = torch.where(commands[:, 0] >= 0.0, forward_obstacles, backward_obstacles)

STABLE MATHEMATICAL PATTERNS (Use these for numerical stability):
Pattern 1 - Exponential Decay (for tracking targets):
# error = (robot.data.root_lin_vel_b[:, 0] - target_value).abs()
# reward_component = torch.exp(-scale_factor * error)  # scale_factor: 0.5 to 10.0

Pattern 2 - Bounded Linear (for contact rewards):
# num_contacts = foot_contacts.sum(dim=-1).float()
# contact_reward = (1.0 - (num_contacts - target_count).abs() / tolerance).clamp(min=0.0, max=1.0)

Pattern 3 - Boolean Masks (for gait patterns):
# gait_reward = ((num_contacts >= min_contacts) & (num_contacts <= max_contacts)).float()

Pattern 4 - Final Bounds (CRITICAL for PPO stability):
# return reward.clamp(min=0.0, max=10.0)  # Prevents training crashes

Pattern 5 - Division Safety (CRITICAL to prevent crashes):
# For literal numbers: safe_ratio = numerator / max(denominator_value, 1e-6)  # denominator_value is literal
# For tensor variables: safe_ratio = numerator / torch.clamp(tensor_var, min=1e-6)  # tensor_var computed from robot data

TIP: Normalize reward components to similar scales (0-1 range) for balanced learning.
TIP: Analyze video frames to understand the specific locomotion pattern before setting thresholds.

**FORMATTING REQUIREMENTS:**
- Use EXACTLY 4 spaces for each indentation level
- NEVER use 8 spaces, tabs, or inconsistent spacing
- Always add safety checks before any division operations
- Specify dtype=torch.float32 for all tensor creations