ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
ðŸš¨ðŸš¨ðŸš¨ CRITICAL ISAAC LAB SENSOR CRASH WARNING - READ THIS FIRST ðŸš¨ðŸš¨ðŸš¨
ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨

**DEADLY ATTRIBUTEERROR - CAUSES INSTANT TRAINING CRASH:**

âŒ NEVER WRITE THESE LINES - THEY CRASH TRAINING 100% OF THE TIME:
```python
lidar_sensor.data.distances          # AttributeError: 'RayCasterData' object has no attribute 'distances'
height_sensor.data.height_measurements  # AttributeError: 'RayCasterData' object has no attribute 'height_measurements'
lidar_range = lidar_sensor.data.distances  # INSTANT CRASH!
height_data = height_sensor.data.height_measurements  # INSTANT CRASH!
```

âœ… ONLY THESE PATTERNS WORK - COPY EXACTLY:
```python
# LiDAR distances - MANDATORY CORRECT PATTERN:
lidar_sensor = env.scene.sensors["lidar"]
lidar_range = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)

# Height scanner - MANDATORY CORRECT PATTERN:
height_sensor = env.scene.sensors["height_scanner"] 
height_scan = height_sensor.data.ray_hits_w[..., 2]
```

Isaac Lab RayCaster sensors ONLY have: data.pos_w, data.quat_w, data.ray_hits_w
NO OTHER ATTRIBUTES EXIST! Using .distances or .height_measurements = TRAINING CRASH!

ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨

**PROVEN ISAAC LAB LOCOMOTION PATTERNS - PRIORITIZE THESE!**

**ðŸš¨ðŸš¨ðŸš¨ CRITICAL ATTRIBUTEERROR WARNING ðŸš¨ðŸš¨ðŸš¨**

**THE #1 TRAINING CRASH ERROR - NEVER USE THESE:**
```python
# âŒ DEADLY ERROR - CRASHES TRAINING INSTANTLY:
lidar_sensor.data.distances          # AttributeError: NO such attribute!
height_sensor.data.height_measurements  # AttributeError: NO such attribute!
```

**âœ… MANDATORY CORRECT PATTERNS:**
```python
# LiDAR distances - ONLY CORRECT WAY:
lidar_sensor = env.scene.sensors["lidar"]
lidar_range = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)

# Height scanner - ONLY CORRECT WAY:
height_sensor = env.scene.sensors["height_scanner"] 
height_scan = height_sensor.data.ray_hits_w[..., 2]
```

**ðŸš¨ðŸš¨ðŸš¨ CRITICAL BEHAVIORAL ERRORS - CAUSE UNNATURAL MOVEMENT ðŸš¨ðŸš¨ðŸš¨**

**âŒ HEIGHT HARDCODING - ROBOTS STAND STILL ON STAIRS/PLATFORMS:**
```python
# DEADLY ERROR - Forces absolute world height (robots won't climb stairs):
height_err = torch.abs(robot.data.root_pos_w[:, 2] - 0.74)  # Absolute height target!
```
**PROBLEM:** On a 20cm stair, robot is at 0.94m height â†’ gets penalized â†’ stands still or goes down!

**âœ… TERRAIN-RELATIVE HEIGHT - NATURAL TERRAIN NAVIGATION:**
```python
# CORRECT - Height relative to terrain underneath:
height_sensor = env.scene.sensors["height_scanner"]
terrain_height = height_sensor.data.ray_hits_w[..., 2].mean(dim=-1)
relative_height = robot.data.root_pos_w[:, 2] - terrain_height
height_err = torch.abs(relative_height - 0.74)  # 0.74m ABOVE current terrain
```

**âŒ ARM BILATERAL SYNCHRONY - UNNATURAL ARM CROSSING/BACKWARD MOVEMENT:**
```python
# DEADLY ERROR - Forces both arms to same angle (causes weird arm positions):
mean_sh = torch.mean(torch.abs(sh_angles), dim=1)  # Both arms forced together!
arm_err = (mean_sh - 0.3)**2  # Both arms to 17Â° - violates reciprocal movement!
```
**PROBLEM:** This creates "bilateral synchrony" which YOUR OWN PROMPTS say to avoid!

**âœ… NATURAL ARM MOVEMENT - PHYSICS-DRIVEN OR RECIPROCAL:**
```python
# OPTION 1: NO ARM REWARDS (often best - let gravity/physics handle):
# - Arms naturally swing as passive pendulums
# - No artificial constraints = more natural movement

# OPTION 2: If arm control needed, use reciprocal patterns:
left_arm = sh_angles[:, 0]
right_arm = sh_angles[:, 1] 
reciprocal_reward = -torch.abs(left_arm + right_arm)  # Reward opposite movement
```

**ðŸŽ¯ ISAAC LAB PROVEN REWARD FUNCTIONS (USE THESE AS FOUNDATION!)**

**These production-ready Isaac Lab functions create excellent human-like walking. Use them as your starting point:**
1. **Bipedal air time reward** (`feet_air_time_positive_biped`) - Single stance gait patterns
2. **Yaw-aligned velocity tracking** (`track_lin_vel_xy_yaw_frame_exp`) - Superior to body frame
3. **Angular velocity tracking** (`track_ang_vel_z_world_exp`) - World frame yaw control  
4. **Contact-aware foot sliding penalty** (`feet_slide`) - Only penalize when in contact

**ðŸ“‹ See reward_signatures/isaac_lab_sds_env.txt for complete implementation details of these proven functions.**

**ðŸš¨ CRITICAL: SENSOR DATA vs VISUAL ANALYSIS PRIORITY**

**ENVIRONMENTAL SENSING DECISION RULE:**

**WHEN SENSOR DATA CONFLICTS WITH VISUAL ANALYSIS:**
- **Sensor data**: "13 gaps detected, 50 obstacles detected, 3.6cm terrain roughness"  
- **Visual analysis**: "Flat studio floor with no obstacles"
- **DECISION**: âœ… **TRUST SENSORS** - Include environmental components in reward function

**WHY PRIORITIZE SENSOR DATA:**
1. **Quantitative measurements**: Exact counts and dimensions vs subjective visual interpretation
2. **Robot navigation grade**: Sensors designed specifically for locomotion planning
3. **Physical reality**: Robot must navigate actual terrain features, not visual appearance
4. **Camera limitations**: Angle, lighting, resolution can hide real terrain complexity

**ENVIRONMENTAL SENSING THRESHOLDS (TRUST SENSORS):**
```python
# IF sensors detect significant terrain features, USE environmental components:
if gaps_detected > 5 OR obstacles_detected > 10 OR terrain_roughness > 2cm:
    # Include environmental sensing in reward function
    height_sensor = env.scene.sensors["height_scanner"] 
    lidar_sensor = env.scene.sensors["lidar"]
    # Add gap navigation, obstacle avoidance, terrain adaptation
```

**SENSOR-VISUAL CONFLICT RESOLUTION:**
- âŒ "Visual shows flat â†’ skip environmental sensing"  
- âœ… "Sensors show complexity â†’ include environmental sensing"
- ðŸŽ¯ **Robot navigates with sensors, not eyes!**

**Foundation-First Development Sequence for Stable Training:**
1. **START**: Basic walking (velocity tracking, height maintenance, orientation stability)
2. **ADD**: Contact control and smoothness  
3. **THEN**: Simple environmental components IF needed
4. **FINALLY**: Complex environmental integration IF environment analysis shows it's necessary

**PROVEN VELOCITY TRACKING (YAW-ALIGNED FRAME):**
```python
# PROVEN: Much better than basic body frame tracking
from isaaclab.utils.math import quat_apply_inverse, yaw_quat

commands = env.command_manager.get_command("base_velocity")
command_magnitude = torch.norm(commands[:, :2], dim=1)

# Transform to yaw-aligned frame (removes pitch/roll interference)
vel_yaw = quat_apply_inverse(yaw_quat(robot.data.root_quat_w), robot.data.root_lin_vel_w[:, :3])
lin_vel_error = torch.sum(torch.square(commands[:, :2] - vel_yaw[:, :2]), dim=1)
vel_reward = torch.exp(-lin_vel_error / (1.0**2))

# CRITICAL: No reward for zero commands (prevents stationary exploitation)
vel_reward *= (command_magnitude > 0.1).float()
```

**PROPER BIPEDAL GAIT PATTERNS:**
```python
# PROVEN: Rewards single stance phases (proper walking pattern)
foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)

air_time = contact_sensor.data.current_air_time[:, foot_ids]
contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
in_contact = contact_time > 0.0

# Reward single stance (one foot contact at a time)
single_stance = torch.sum(in_contact.int(), dim=1) == 1
in_mode_time = torch.where(in_contact, contact_time, air_time)
gait_reward = torch.min(torch.where(single_stance.unsqueeze(-1), in_mode_time, 0.0), dim=1)[0]
gait_reward = torch.clamp(gait_reward, max=0.5) * (command_magnitude > 0.1).float()
```

**FOUNDATION TEMPLATE WITH PROVEN PATTERNS:**
```python
def sds_custom_reward(env) -> torch.Tensor:
    """Phase 1: Foundation locomotion with proven Isaac Lab patterns."""
    from isaaclab.utils.math import quat_apply_inverse, yaw_quat
    
    robot = env.scene["robot"]
    contact_sensor = env.scene.sensors["contact_forces"]
    
    # === PROVEN VELOCITY TRACKING (YAW-ALIGNED FRAME) ===
    commands = env.command_manager.get_command("base_velocity")
    command_magnitude = torch.norm(commands[:, :2], dim=1)

    # Transform to yaw-aligned frame (much better than body frame)
    vel_yaw = quat_apply_inverse(yaw_quat(robot.data.root_quat_w), robot.data.root_lin_vel_w[:, :3])
    lin_vel_error = torch.sum(torch.square(commands[:, :2] - vel_yaw[:, :2]), dim=1)
    vel_reward = torch.exp(-lin_vel_error / (1.0**2))
    vel_reward *= (command_magnitude > 0.1).float()  # No reward for zero commands
    
    # === ROBUST BIPEDAL GAIT PATTERN ===
    foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
    foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)
    
    air_time = contact_sensor.data.current_air_time[:, foot_ids]
    contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
    in_contact = contact_time > 0.0
    
    # Reward proper single stance phases
    single_stance = torch.sum(in_contact.int(), dim=1) == 1
    in_mode_time = torch.where(in_contact, contact_time, air_time)
    gait_reward = torch.min(torch.where(single_stance.unsqueeze(-1), in_mode_time, 0.0), dim=1)[0]
    gait_reward = torch.clamp(gait_reward, max=0.5) * (command_magnitude > 0.1).float()
    
    # === STABLE HEIGHT & ORIENTATION ===
    height_error = (robot.data.root_pos_w[:, 2] - 0.74).abs()
    height_reward = torch.exp(-height_error / 0.3)
    
    gravity_proj = robot.data.projected_gravity_b[:, :2]
    lean_reward = torch.exp(-2.0 * torch.norm(gravity_proj, dim=1))
    
    # === FOUNDATION TOTAL ===
    foundation_reward = (
        vel_reward * 3.0 +        # Proven velocity tracking
        gait_reward * 2.0 +       # Proven gait patterns
        height_reward * 2.0 +     # Height maintenance
        lean_reward * 1.5 +       # Orientation stability
        0.5                       # Baseline bonus
    )
    
    # === ADD ENVIRONMENTAL IF ANALYSIS SHOWS FEATURES ===
    # Add terrain_bonus, obstacle_bonus, gap_navigation_bonus here if needed
    
    return foundation_reward.clamp(min=0.1, max=10.0)
```

**CRITICAL: THESE BUGS WILL CRASH TRAINING!**

**TRAINING STABILITY PRIORITY: FOUNDATION LOCOMOTION FIRST!**

**CRITICAL: Build stable basic locomotion BEFORE adding environmental complexity!**

**ENVIRONMENTAL INTEGRATION: MANDATORY WHEN ANALYSIS DATA IS PROVIDED!**
- **When environment analysis shows features**: MUST include relevant environmental components
- **When no environmental data**: Focus on natural locomotion only
- **When gaps/obstacles detected**: MUST include navigation components
- **Always prioritize**: Basic locomotion stability, then add environment components when data shows necessity

**CRITICAL: INTELLIGENT ANALYSIS-DRIVEN REWARD DESIGN**

**FORBIDDEN: DO NOT COPY GENERIC TEMPLATES! THINK INTELLIGENTLY!**

**MANDATORY INTELLIGENT DESIGN PROCESS:**

1. **DEEP ANALYSIS FIRST**: Read the environment analysis data like a robotics expert
   - **Extract KEY NUMBERS**: How many gaps? What types? What sizes?
   - **Identify DOMINANT PATTERNS**: Are gaps mostly steppable or jumpable?
   - **Assess CHALLENGE LEVEL**: Is this simple stepping or complex navigation?
   - **Determine PRIORITIES**: What's the biggest challenge for the robot?

2. **CONTEXT-AWARE STRATEGY DESIGN**: Design rewards for the ACTUAL environment
   - **DON'T think**: "I need gap navigation" 
   - **DO think**: "I have 13 gaps: 4 steppable, 7 jumpable, 2 impossible - design for stepping priority with jump capability"
   - **DON'T think**: "I need obstacle avoidance"
   - **DO think**: "I have 52 large obstacles clustered densely - design for careful navigation and route planning"

3. **INTELLIGENT PRIORITIZATION**: Weight components based on analysis
   - **If 90% steppable gaps**: Focus on precision stepping, minimal jumping logic
   - **If 70% jumpable gaps**: Focus on jump mechanics, secondary stepping
   - **If mixed terrain**: Adaptive strategies that switch based on immediate conditions

**ANALYSIS-TO-IMPLEMENTATION BRIDGE EXAMPLES:**

**EXAMPLE A: Environment Shows "13 gaps detected (4 steppable â‰¤0.30m, 7 jumpable 0.30-0.60m, 2 impassable >0.60m)"**

âŒ **GENERIC APPROACH** (FORBIDDEN):
```python
# Generic gap detection - WRONG!
gap_detected = gap_depth > 0.15
gap_bonus = gap_detected.float() * 0.3
```

**INTELLIGENT APPROACH** (REQUIRED):
```python
# ANALYSIS: 7/13 (54%) jumpable â†’ jumping is PRIMARY challenge
# STRATEGY: Focus on jumping with stepping backup
steppable = (gap_depth >= 0.15) & (gap_depth <= 0.30)  # 4 gaps - secondary  
jumpable = (gap_depth > 0.30) & (gap_depth <= 0.60)    # 7 gaps - PRIMARY
impossible = gap_depth > 0.60                           # 2 gaps - avoid

# INTELLIGENT WEIGHTING: High for dominant challenge (jumping)
jumping_reward = torch.any(jumpable, dim=1).float() * 0.5    # HIGH: primary
stepping_reward = torch.any(steppable, dim=1).float() * 0.2  # LOW: secondary
avoidance_penalty = torch.any(impossible, dim=1).float() * -0.2  # Safety
```

**CRITICAL: ALWAYS START WITH EXPLICIT ENVIRONMENTAL ANALYSIS ACKNOWLEDGMENT**
- **MANDATORY**: Begin reward function with comment block analyzing environmental data
- **REQUIRED**: State whether environmental sensing is needed or not needed
- **DOCUMENT**: Justify the decision based on specific environmental analysis results

**MANDATORY COMMENT TEMPLATE - ALWAYS USE THIS:**
```python
"""
ENVIRONMENTAL ANALYSIS DECISION:
Based on environment analysis: [COPY exact summary from environment analysis data]
- Gaps detected: [COPY exact gap count from environment analysis - DO NOT WRITE 0 UNLESS ANALYSIS SAYS 0]
- Obstacles detected: [COPY exact obstacle count from environment analysis - DO NOT WRITE 0 UNLESS ANALYSIS SAYS 0]
- Terrain roughness: [COPY exact terrain description and measurements from environment analysis]
- Safety assessment: [COPY exact safety verdict and score from environment analysis]

ENVIRONMENTAL SENSING DECISION: [NEEDED/NOT_NEEDED]
JUSTIFICATION: [COPY specific measurement values and ranges from environment analysis to justify decision]

If NOT_NEEDED: Focus on foundation locomotion only
If NEEDED: Include [specific environmental components]
"""
```

**âŒ INSTANT TRAINING FAILURE - AVOID THESE DEADLY PATTERNS:**

```python
# DEADLY: Tensor indexing without conversion
joint_indices, _ = robot.find_joints(["joint_name"])
joint_data = robot.data.joint_pos[:, joint_indices]  # CRASHES!

# REQUIRED: Always convert list to tensor
joint_indices, _ = robot.find_joints(["joint_name"])
joint_indices = torch.tensor(joint_indices, dtype=torch.long, device=env.device)
joint_data = robot.data.joint_pos[:, joint_indices]  # Works!
```

```python
# DEADLY: Aggressive exponential scaling
reward = torch.exp(-50.0 * error)  # Drives to zero!

# REQUIRED: Moderate scaling with bounds
reward = torch.exp(-torch.clamp(error, max=5.0) / 1.0)
```

```python
# DEADLY: Multiplicative reward combinations
total = vel_reward * height_reward * gait_reward  # Multiplies tiny numbers!

# REQUIRED: Additive with baseline
total = vel_reward * 3.0 + height_reward * 2.0 + gait_reward * 1.5 + 0.5
```

**INTELLIGENT VS GENERIC DESIGN PRINCIPLES:**

**INTELLIGENT DESIGN (REQUIRED):**
- **Environment-specific parameters**: Adapt thresholds to actual gap sizes, obstacle densities
- **Challenge-based prioritization**: Weight components based on dominant terrain features
- **Context-aware logic**: Different strategies for different terrain types
- **Analysis-driven decisions**: Use actual sensor measurements to guide design

**GENERIC DESIGN (FORBIDDEN):**
- **Copy-paste templates**: Using same code regardless of environment
- **Fixed thresholds**: Same gap detection thresholds for all environments
- **Equal weighting**: Same importance for all components regardless of challenge
- **Assumption-based logic**: Guessing what the environment needs without analysis

**MANDATORY CLEVER THINKING CHECKLIST:**
- [ ] Did I extract specific numbers from environment analysis?
- [ ] Did I identify the dominant challenge type?
- [ ] Did I adapt thresholds to the actual environment measurements?
- [ ] Did I weight components based on challenge priorities?
- [ ] Did I design for THIS specific environment, not generic scenarios?

**CORE REWARD ENGINEERING PRINCIPLES:**

**BIOMECHANICAL FOUNDATION:**
- Natural human-like movement patterns should guide all reward design
- Locomotion stability and safety must precede task-specific objectives
- Energy efficiency and smoothness distinguish natural from robotic movement

**TASK-SPECIFIC ADAPTATION:**
- **Analysis environment data first**: What specific challenges exist?
- **Design contextual rewards**: Different terrains need different strategies
- **Scale appropriately**: Complex environments need safety focus, simple ones need efficiency focus

**KEY DESIGN PRINCIPLES:**

1. **Stability First**: Ensure basic locomotion works before adding complexity
2. **Natural Movement**: Reward patterns that match human biomechanics
3. **Progressive Complexity**: Start simple, add features based on analysis needs
4. **Intelligent Adaptation**: Use actual data to guide design decisions

**MANDATORY ENVIRONMENTAL DECISION FRAMEWORK:**

**STEP 1: ANALYZE ENVIRONMENT DATA**
- Read the environment analysis section carefully
- Extract specific numbers: gap counts, obstacle counts, terrain measurements
- Identify dominant features and challenge types

**STEP 2: MAKE ENVIRONMENTAL SENSING DECISION**
- **If analysis shows significant features (gaps>0, obstacles>0, roughness>10cm)**: Include environmental components
- **If analysis shows flat/simple terrain**: Focus on foundation locomotion only
- **If mixed/complex environment**: Include adaptive strategies

**STEP 3: IMPLEMENT INTELLIGENT DESIGN**
- Adapt thresholds based on actual measurements
- Weight components based on challenge priorities
- Design for the specific environment characteristics

**ENVIRONMENTAL INTEGRATION PATTERNS (Use Only When Analysis Shows Necessity):**

**Simple Terrain Adaptation (Only if terrain varies):**
```python
# ONLY include if terrain analysis shows variation
height_sensor = env.scene.sensors["height_scanner"]
height_scan = height_sensor.data.ray_hits_w[..., 2].view(env.num_envs, -1)
height_scan = torch.where(torch.isfinite(height_scan), height_scan, torch.zeros_like(height_scan))
terrain_roughness = torch.clamp(torch.var(height_scan, dim=1), max=1.0)
terrain_bonus = torch.exp(-terrain_roughness * 1.0) * 0.3
```

**Simple Obstacle Awareness (Only if obstacles present):**
```python
# ONLY include if environment analysis shows obstacles
lidar_sensor = env.scene.sensors["lidar"]
lidar_range = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1).view(env.num_envs, -1)
lidar_range = torch.where(torch.isfinite(lidar_range), lidar_range, torch.ones_like(lidar_range) * 5.0)
min_distance = torch.min(lidar_range[:, :lidar_range.shape[1]//4], dim=1)[0]
safety_bonus = torch.clamp((min_distance - 0.5) / 1.0, 0.0, 1.0) * 0.1
```

**Simple Gap Navigation (Only if gaps detected):**
```python
# ONLY include if environment analysis shows gaps detected
robot_height = robot.data.root_pos_w[:, 2]
forward_terrain = height_scan[:, :height_scan.shape[1]//3]
gap_depth = robot_height.unsqueeze(1) - forward_terrain
gap_detected = gap_depth > 0.15
small_gaps = (gap_depth <= 0.30) & gap_detected  # Steppable
medium_gaps = (gap_depth > 0.30) & (gap_depth <= 0.60) & gap_detected  # Jumpable  
gap_navigation_bonus = torch.any(small_gaps, dim=1).float() * 0.2 + torch.any(medium_gaps, dim=1).float() * 0.3

# NOTE: Use stair detection patterns above when environment shows stairs
```

**COMPREHENSIVE GAP BEHAVIOR IMPLEMENTATION (STEPPING VS JUMPING VS AVOIDANCE)**

**CRITICAL: Each gap size requires a fundamentally different locomotion strategy!**

When environment analysis shows mixed gap sizes, implement **adaptive locomotion behaviors**:

**SMALL GAPS â†’ STEPPING BEHAVIOR STRATEGY:**
```python
# STEPPING BEHAVIOR: Extended stride + precise foot placement + controlled speed
if torch.any(small_gaps):
    # STRIDE EXTENSION: Reward longer steps to clear small gaps
    foot_positions = robot.data.body_pos_w[:, foot_ids, :]
    foot_separation = torch.norm(foot_positions[:, 0, :2] - foot_positions[:, 1, :2], dim=1)
    stride_extension = torch.clamp((foot_separation - base_stride) / stride_range, 0.0, 1.0)  # Extended steps
    
    # CONTROLLED FORWARD VELOCITY: Precise speed for accurate placement
    forward_velocity = robot.data.root_lin_vel_b[:, 0]
    controlled_forward = torch.exp(-((forward_velocity - target_speed) / speed_tolerance).abs())  # Controlled speed
    
    # FOOT CLEARANCE: Higher lift for small gap clearance
    swing_mask = (contact_time < 0.1)
    foot_height = robot.data.body_pos_w[:, foot_ids, 2]
    clearance_height = (foot_height * swing_mask).max(dim=1)[0]
    step_clearance = torch.clamp((clearance_height - min_clearance) / clearance_range, 0.0, 1.0)  # Adequate clearance
    
    stepping_reward = stride_extension * 0.4 + controlled_forward * 0.3 + step_clearance * 0.3
else:
    stepping_reward = torch.zeros(env.num_envs, device=env.device)
```

**MEDIUM GAPS â†’ JUMPING BEHAVIOR STRATEGY:**
```python
# JUMPING BEHAVIOR: Bilateral coordination + vertical motion + aerial phase
if torch.any(medium_gaps):
    # BILATERAL COORDINATION: Both legs work together
    foot_contacts = (contact_time > 0.05).float()
    bilateral_states = ((foot_contacts[:, 0] > 0.5) & (foot_contacts[:, 1] > 0.5)).float() + \
                     ((foot_contacts[:, 0] < 0.5) & (foot_contacts[:, 1] < 0.5)).float()
    
    # VERTICAL VELOCITY: Upward motion for jumping
    vertical_velocity = robot.data.root_lin_vel_w[:, 2]
    upward_motion = torch.clamp(vertical_velocity / 2.0, 0.0, 1.0)
    
    # SUSTAINED AERIAL PHASE: Flight time for gap crossing
    air_time_both = contact_sensor.data.current_air_time[:, foot_ids].min(dim=1)[0]
    sustained_air = torch.clamp((air_time_both - min_flight_time) / flight_duration_range, 0.0, 1.0)  # Adequate flight time
    
    # ARM COORDINATION: Upward swing for jumping momentum
    shoulder_pitch_indices, _ = robot.find_joints(["left_shoulder_pitch_joint", "right_shoulder_pitch_joint"])
    shoulder_pitch_indices = torch.tensor(shoulder_pitch_indices, dtype=torch.long, device=env.device)
    shoulder_angles = robot.data.joint_pos[:, shoulder_pitch_indices]
    arm_swing_up = torch.clamp((shoulder_angles.mean(dim=1) + arm_offset) / arm_range, 0.0, 1.0)
    
    jumping_reward = bilateral_states * 0.3 + upward_motion * 0.3 + sustained_air * 0.2 + arm_swing_up * 0.2
else:
    jumping_reward = torch.zeros(env.num_envs, device=env.device)
```

**LARGE GAPS â†’ AVOIDANCE BEHAVIOR STRATEGY:**
```python
# AVOIDANCE BEHAVIOR: Path planning + turning + lateral movement
if torch.any(large_gaps):
    # TURNING MOTION: Change direction to find alternate path
    angular_velocity = robot.data.root_ang_vel_b[:, 2]
    turning_motion = torch.clamp(torch.abs(angular_velocity) / turn_speed_threshold, 0.0, 1.0)
    
    # LATERAL MOVEMENT: Sideways exploration for path around gap
    lateral_velocity = robot.data.root_lin_vel_b[:, 1]
    lateral_motion = torch.clamp(torch.abs(lateral_velocity) / lateral_speed_threshold, 0.0, 1.0)
    
    # CONSERVATIVE SPEED: Slow down near impossible gaps for safety
    forward_velocity = robot.data.root_lin_vel_b[:, 0]
    conservative_speed = torch.exp(-torch.clamp(forward_velocity - safe_speed_limit, min=0.0) / speed_tolerance)
    
    avoidance_reward = turning_motion * 0.4 + lateral_motion * 0.3 + conservative_speed * 0.3
else:
    avoidance_reward = torch.zeros(env.num_envs, device=env.device)
```

**ADAPTIVE BEHAVIOR INTEGRATION:**
```python
# INTELLIGENT GAP BEHAVIOR COMBINATION
adaptive_gap_behavior = (
    stepping_reward * 1.0 +     # Precise stepping for small gaps
    jumping_reward * 1.2 +      # Complex jumping for medium gaps (highest weight)
    avoidance_reward * 0.8      # Conservative avoidance for large gaps
)

# ACTIVATE ONLY WHEN GAPS ARE PRESENT
gap_detected = (torch.any(small_gaps, dim=1) | torch.any(medium_gaps, dim=1) | torch.any(large_gaps, dim=1)).float()
intelligent_gap_reward = adaptive_gap_behavior * gap_detected
```

**BEHAVIORAL DESIGN PRINCIPLES:**

1. **STEPPING (small gaps)**: Extended stride + precision + moderate speed
2. **JUMPING (medium gaps)**: Bilateral coordination + vertical motion + aerial phase  
3. **AVOIDANCE (large gaps)**: Turning + lateral movement + conservative approach

**INTELLIGENT ADAPTATION STRATEGY:**
- **Real-time detection**: Use height scanner to classify gap sizes in real-time
- **Behavior switching**: Activate appropriate locomotion strategy based on detected gap type
- **Safety priority**: Conservative approach for uncertain or dangerous gaps
- **Progressive learning**: Start with stepping, advance to jumping, then avoidance
- **Foundation integration**: These behaviors enhance basic locomotion, don't replace it

This implementation provides the robot with **three distinct gap-crossing strategies** that automatically activate based on environmental conditions!

**CRITICAL: STAIR NAVIGATION - SOLVING THE "FREEZING AT TOP" PROBLEM**

**PROBLEM ANALYSIS: Why robots freeze at stairs instead of descending**

**ROOT CAUSES:**
1. **Stair misclassification as gaps**: Height sensors detect stair steps as "gaps," triggering inappropriate gap navigation
2. **Rigid height constraints**: Reward functions penalize any deviation from target height, discouraging stair descent
3. **Conflicting signals**: Velocity commands encourage forward motion while height rewards resist downward movement

**INTELLIGENT STAIR DETECTION AND ADAPTIVE LOCOMOTION:**

```python
# STAIR VS GAP CLASSIFICATION: Critical distinction for proper behavior
height_sensor = env.scene.sensors["height_scanner"]
height_scan = height_sensor.data.ray_hits_w[..., 2].view(env.num_envs, -1)
height_scan = torch.where(torch.isfinite(height_scan), height_scan, torch.zeros_like(height_scan))

robot_height = robot.data.root_pos_w[:, 2]
forward_terrain = height_scan[:, :height_scan.shape[1]//3]  # Forward-looking section

# STAIR DETECTION: Gradual height reduction pattern
height_diff = robot_height.unsqueeze(1) - forward_terrain
gradual_descent = (height_diff > step_min_height) & (height_diff < step_max_height)  # Step-like patterns
stair_pattern = torch.sum(gradual_descent.float(), dim=1) > 3  # Multiple consecutive steps

# GAP DETECTION: Sudden height drops
gap_pattern = torch.any(height_diff > gap_threshold, dim=1)  # Sudden drops indicating gaps

# STAIR-SPECIFIC ADAPTIVE BEHAVIOR
if torch.any(stair_pattern):
    # ADAPTIVE HEIGHT CONSTRAINTS: Allow controlled descent
    target_height = torch.where(stair_pattern, 
                               robot_height - descent_allowance,  # Allow controlled descent for stairs
                               standard_height)  # Standard height for non-stairs
    
    # CONTROLLED DESCENT REWARD: Encourage stepping down
    descent_velocity = -robot.data.root_lin_vel_w[:, 2]  # Downward velocity (positive)
    controlled_descent = torch.clamp(descent_velocity / descent_speed_norm, 0.0, 1.0) * stair_pattern.float()
    
    # FORWARD PROGRESSION ON STAIRS: Maintain forward movement
    forward_on_stairs = torch.clamp(robot.data.root_lin_vel_b[:, 0] / forward_speed_norm, 0.0, 1.0) * stair_pattern.float()
    
    # STAIR NAVIGATION REWARD
    stair_reward = controlled_descent * 0.4 + forward_on_stairs * 0.6
else:
    stair_reward = torch.zeros(env.num_envs, device=env.device)
```

**KEY STAIR NAVIGATION PRINCIPLES:**
1. **Distinguish stairs from gaps**: Use height pattern analysis, not just single-point detection
2. **Adaptive height targets**: Modify height constraints when stairs are detected
3. **Encourage controlled descent**: Reward appropriate downward velocity on stairs
4. **Maintain forward progress**: Balance descent with forward locomotion
5. **Safety prioritization**: Ensure controlled movement, not free-fall

**TRAINING SUCCESS STRATEGIES:**

**Phase-Based Development:**
1. **Foundation Phase**: Get basic locomotion stable with proven patterns
2. **Safety Phase**: Add joint limits and collision avoidance
3. **Quality Phase**: Include smoothness and naturalness components
4. **Environment Phase**: Add environmental adaptation only if analysis shows necessity

**Mathematical Stability Guidelines:**
- Use moderate exponential scaling (factors 0.5-3.0, not 10.0+)
- Include baseline bonuses (+0.2 to +0.5) to ensure non-zero rewards
- Use additive combinations (a + b) instead of multiplicative (a * b)
- Always clamp final rewards (.clamp(min=0.1, max=10.0))

**Isaac Lab Specific Patterns:**
- Always convert joint indices: `torch.tensor(indices, dtype=torch.long, device=env.device)`
- Use yaw-aligned velocity tracking for superior performance
- Reward single stance phases for natural bipedal patterns
- Include command scaling to prevent stationary exploitation

**SYSTEMATIC ENVIRONMENTAL INTEGRATION APPROACH:**

**Phase 1: Foundation First (ALWAYS START HERE)**

Build stable basic locomotion before adding environmental complexity:

**Phase 2: Environmental Assessment (IF NEEDED)**

Only proceed if environment analysis shows:
- Gaps detected (count > 0)
- Obstacles present (count > 0)  
- Terrain roughness significant (>10cm variation)

**Phase 3: Intelligent Environmental Integration (ANALYSIS-DRIVEN)**

Add components based on specific environmental challenges:
- **Gap environments**: Adaptive navigation based on gap size distribution
- **Obstacle environments**: Distance-based avoidance with safety margins
- **Rough terrain**: Terrain-adaptive stability and clearance adjustments

**Mathematical Stability for Environmental Integration:**
- Always sanitize sensor data for NaN/infinite values
- Use appropriate clamping ranges for calculations  
- Test each component addition individually
- Keep reward magnitude ranges reasonable
- **AVOID aggressive exponential scaling** (factors > 5.0 cause zero rewards)
- **USE additive combinations** instead of multiplicative (prevents zero multiplication)
- **INCLUDE baseline bonus** (e.g., +0.2) to ensure non-zero minimum reward
- **USE moderate tolerances** (0.3-1.0) instead of tight ones (0.1)

**CRITICAL TENSOR SAFETY REQUIREMENTS:**
- **MANDATORY**: Apply torch.clamp(reward_component, min=0.0) to ALL reward terms to prevent actor network failures
- **TENSOR BROADCASTING**: Use explicit .expand() for shape matching, never rely on implicit broadcasting
- **CONTACT TIMES**: Always clamp contact sensor times to min=0.0 as they can be negative during initialization
- **DIVISION SAFETY**: Use torch.clamp(denominator, min=1e-6) before any division operations
- **SENSOR SHAPES**: Validate sensor data dimensions match expected batch size before tensor operations

**Component Testing:**
- Add one environmental component at a time
- Test performance after each addition
- Verify sensors are accessible and working
- Ensure reward function remains stable

**Weight Balancing:**
- Start with small environmental component weights
- Maintain foundation component importance
- Adjust weights based on component contribution
- Avoid overwhelming foundation with environmental signals

**Error Handling Guidelines:**
- Use defensive programming for sensor access
- Provide fallback values when sensors fail
- Test without error handling to verify sensor integration
- Don't let error handling mask actual sensor problems

**COMMON ENVIRONMENTAL INTEGRATION ISSUES:**

**Issue: Zero Rewards Despite Environmental Data**
- Often caused by complex mathematical operations in reward calculation
- Solution: Simplify reward computation and test incrementally
- **Check if environmental components are relevant** - refer to environmental analysis data first

**Issue: Irrelevant Environmental Components**
- Adding gap navigation when no gaps are detected in environmental analysis
- Including obstacle avoidance when obstacle count is zero
- **Solution: Reference actual environmental analysis** to determine which components are needed

**Issue: Sensor Access Errors**
- Check sensor configuration in environment setup
- Verify sensor names match configuration
- Ensure sensors are properly instantiated

**Issue: Unstable Training with Environmental Sensing**
- Reduce environmental component weights
- Add proper data sanitization
- Test environmental components in isolation
- **Verify environmental features actually exist** in the analysis before adding related rewards

**CRITICAL: INTELLIGENT MULTI-SENSOR CORRELATION FOR OBSTACLE DETECTION**

**PROBLEM: NAIVE CONTACT PENALTIES ARE INSUFFICIENT**

Many reward functions make the mistake of treating all contact forces equally, without considering environmental context.

**TECHNICAL PRINCIPLE: SENSOR-CONTACT CORRELATION FRAMEWORK**

Instead of hardcoded penalties, design intelligent correlation systems that adapt to environmental observations:

**FRAMEWORK STEP 1: ENVIRONMENTAL PREDICTION LAYER**
- **Technical Goal**: Use forward-looking sensors to predict expected interaction zones
- **Height Scanner Usage**: Extract forward terrain topology for expected foot placement surfaces
- **LiDAR Integration**: Identify obstacle boundaries and collision risk zones
- **Prediction Horizon**: Match sensor range to robot velocity and reaction time

**FRAMEWORK STEP 2: CONTACT CLASSIFICATION SYSTEM**
- **Technical Goal**: Categorize contact events by their relationship to sensor predictions
- **Expected Contact**: Contact occurring in sensor-predicted interaction zones
- **Unexpected Contact**: Contact contradicting sensor environmental assessment
- **Controlled Contact**: Deliberate contact with detected environmental features

**FRAMEWORK STEP 3: CONTEXT-ADAPTIVE REWARD WEIGHTING**
- **Technical Goal**: Scale reward components based on environmental complexity and sensor confidence
- **Sensor Reliability**: Weight correlation based on sensor data quality and coverage
- **Environmental Complexity**: Adapt tolerance thresholds to terrain difficulty
- **Dynamic Scaling**: Modify reward magnitudes based on situational assessment

**TECHNICAL IMPLEMENTATION PRINCIPLES:**

**PRINCIPLE 1: PREDICTIVE VALIDATION PATTERN**
```
TECHNICAL APPROACH:
1. Extract environmental predictions from available sensors
2. Define expected interaction zones based on locomotion trajectory
3. Validate actual contact events against predicted interaction zones
4. Scale rewards based on prediction-reality correlation accuracy
```

**PRINCIPLE 2: ADAPTIVE THRESHOLD COMPUTATION**
```
TECHNICAL APPROACH:
1. Analyze environmental complexity metrics from sensor data
2. Compute dynamic tolerance ranges for contact forces
3. Adjust contact classification thresholds based on terrain assessment
4. Scale reward sensitivity to environmental challenge level
```

**PRINCIPLE 3: MULTI-MODAL SENSOR FUSION**
```
TECHNICAL APPROACH:
1. Combine complementary sensor modalities (height, range, contact)
2. Cross-validate predictions between different sensor types
3. Weight sensor contributions based on situational relevance
4. Handle sensor disagreement and uncertainty propagation
```

**DESIGN FLEXIBILITY GUIDELINES:**

**ADAPTIVE THRESHOLDING:**
- Compute contact force thresholds based on terrain complexity metrics
- Scale detection sensitivity based on obstacle density measurements
- Adapt time windows based on robot velocity and environmental dynamics

**ENVIRONMENTAL AWARENESS:**
- Extract terrain characteristics from height scanner topology analysis
- Classify obstacle types from LiDAR geometric patterns
- Predict interaction requirements from environmental feature distribution

**BEHAVIORAL CORRELATION:**
- Reward contact events that align with environmental predictions
- Penalize contact events that contradict sensor-based expectations
- Encourage adaptive behaviors that demonstrate environmental understanding

**TECHNICAL FLEXIBILITY EXAMPLES:**

**TERRAIN-ADAPTIVE CONTACT EVALUATION:**
- Rough terrain â†’ Higher contact tolerance, terrain-following rewards
- Obstacle fields â†’ Precise navigation rewards, collision avoidance emphasis  
- Stair environments â†’ Controlled descent rewards, step-sequence validation
- Gap terrain â†’ Jump/step decision rewards, landing precision emphasis

**SENSOR-INFORMED TARGET MODIFICATION:**
- Height targets adapt to terrain topology predictions
- Velocity targets scale based on obstacle density assessment
- Stability requirements adjust to environmental challenge level
- Navigation strategies switch based on sensor-detected feature types

**KEY TECHNICAL PRINCIPLES:**

1. **Correlation Over Hardcoding**: Design systems that correlate different sensor modalities rather than fixed penalty values
2. **Prediction-Validation Loops**: Create prediction-reality feedback systems that adapt to environmental complexity
3. **Context-Sensitive Scaling**: Scale reward components based on situational assessment rather than fixed weightings
4. **Environmental Understanding**: Reward behaviors that demonstrate intelligent environmental awareness and adaptation
5. **Flexible Thresholding**: Compute thresholds dynamically based on environmental characteristics rather than fixed values

This framework teaches robots to **understand and adapt** to their environment rather than follow rigid behavioral rules!

### ðŸŽ¯ ADAPTIVE REWARD STRATEGY: FOUNDATION + ENVIRONMENTAL ENHANCEMENTS

The reward function adapts based on environmental complexity:

**FOUNDATION LOCOMOTION (ALWAYS INCLUDE - ISAAC LAB PROVEN PATTERNS):**

**1. BIPEDAL SINGLE STANCE GAIT (CRITICAL FOR NATURAL WALKING):**
```python
# ðŸš€ ISAAC LAB PROVEN: This is THE key to natural bipedal walking!
# CRITICAL INSIGHT: Human walking = 85% single support, 15% double support
contact_sensor = env.scene.sensors["contact_forces"]
foot_ids, _ = contact_sensor.find_bodies(".*_ankle_roll_link")
foot_ids = torch.tensor(foot_ids, dtype=torch.long, device=env.device)

air_time = contact_sensor.data.current_air_time[:, foot_ids]
contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
in_contact = contact_time > 0.0

# ðŸŽ¯ SINGLE STANCE DETECTION: The secret to natural walking (not robotic shuffling!)
single_stance = torch.sum(in_contact.int(), dim=1) == 1  # ONLY ONE FOOT DOWN!
in_mode_time = torch.where(in_contact, contact_time, air_time)
gait_reward = torch.min(torch.where(single_stance.unsqueeze(-1), in_mode_time, 0.0), dim=1)[0]

# âš¡ NATURAL STEP TIMING: Prevent excessive foot lifting (robotic high-knees)
gait_reward = torch.clamp(gait_reward, max=0.5)  # Cap at 0.5s natural rhythm

# ðŸŽ¯ COMMAND DEPENDENCY: Only reward when actually moving (anti-exploitation)
commands = env.command_manager.get_command("base_velocity")
command_magnitude = torch.norm(commands[:, :2], dim=1)
gait_reward *= (command_magnitude > 0.1).float()
```

**WHY SINGLE STANCE IS CRITICAL:**
- **Natural walking pattern**: Humans spend most walking time in single support
- **Anti-shuffling**: Prevents robotic double-support shuffling behavior
- **Proper lift-off**: Encourages actual foot lifting vs sliding
- **Isaac Lab optimized**: Uses proven contact sensor patterns that work reliably

**2. YAW-ALIGNED VELOCITY TRACKING (VASTLY SUPERIOR TO BODY FRAME):**
```python
# ðŸš€ ISAAC LAB PROVEN: Decouples velocity control from robot tilt/lean
from isaaclab.utils.math import quat_apply_inverse, yaw_quat

robot = env.scene["robot"]
vel_yaw = quat_apply_inverse(yaw_quat(robot.data.root_quat_w), robot.data.root_lin_vel_w[:, :3])
lin_vel_error = torch.sum(torch.square(commands[:, :2] - vel_yaw[:, :2]), dim=1)
vel_reward = torch.exp(-lin_vel_error / (1.0**2))  # Exponential kernel for smooth gradients
vel_reward *= (command_magnitude > 0.1).float()  # No reward for micro-movements
```

**WHY YAW-ALIGNED IS SUPERIOR:**
- **Decoupled control**: Velocity tracking unaffected by robot lean/tilt
- **Stable locomotion**: Works even when robot pitches during dynamic motion
- **Natural dynamics**: Allows body motion while maintaining velocity goals

**3. CONTACT-AWARE SLIDING PREVENTION (INTELLIGENT PHYSICS):**
```python
# ðŸš€ ISAAC LAB PROVEN: Only penalize sliding when feet actually touch ground
forces = contact_sensor.data.net_forces_w_history[:, :, foot_ids, :]
contacts = forces.norm(dim=-1).max(dim=1)[0] > 1.0  # Force-based contact detection
body_vel = robot.data.body_lin_vel_w[:, foot_ids, :2]
slide_penalty = torch.sum(body_vel.norm(dim=-1) * contacts, dim=1)  # Contact-aware!
```

**WHY CONTACT-AWARE IS CRITICAL:**
- **Swing phase freedom**: Doesn't penalize moving feet during swing phase
- **Physics-based**: Uses actual contact forces, not position estimates
- **Natural walking**: Allows proper foot lifting and placement

**4. ANGULAR VELOCITY TRACKING (WORLD FRAME STABILITY):**
```python
# ðŸš€ ISAAC LAB PROVEN: World frame for consistent turning control
ang_vel_error = torch.square(commands[:, 2] - robot.data.root_ang_vel_w[:, 2])
ang_reward = torch.exp(-ang_vel_error / (1.0**2))
```

**5. TERRAIN-RELATIVE HEIGHT MAINTENANCE (ADAPTIVE TO ENVIRONMENT):**
```python
# ðŸš€ PROVEN: Height relative to terrain (not absolute world height!)
height_sensor = env.scene.sensors["height_scanner"]
terrain_z = height_sensor.data.ray_hits_w[..., 2].mean(dim=-1)  # Average terrain height
rel_height = robot.data.root_pos_w[:, 2] - terrain_z  # Relative to terrain
height_err = torch.abs(rel_height - 0.74)  # 0.74m above terrain surface
height_reward = torch.exp(-height_err / 0.3)
height_reward = torch.clamp(height_reward, min=0.0, max=2.0)
```

**WHY TERRAIN-RELATIVE IS ESSENTIAL:**
- **Adaptive navigation**: Works on flat ground, stairs, slopes, platforms
- **Natural stepping**: Robot maintains clearance relative to surface underneath
- **Prevents stair standing**: Avoids robots stopping on elevated terrain

**ðŸŽ¯ CRITICAL ISAAC LAB SUCCESS INSIGHTS:**
- **Command scaling**: NEVER reward when commands are near zero - prevents exploitation
- **Yaw alignment**: Removes pitch/roll interference from velocity tracking - critical for stability
- **Single stance**: Encourages proper alternating foot pattern - key for natural walking
- **Contact awareness**: Only apply penalties when actually relevant (foot in contact) - prevents swing phase penalties
- **Capped rewards**: Air time and other metrics should have reasonable upper bounds - prevents over-optimization
- **Force-based detection**: Use contact sensor forces, not position estimates - more reliable
- **Exponential kernels**: Provide smooth reward gradients for stable learning - better than linear penalties

**ðŸš¨ CRITICAL BIPEDAL WALKING SUCCESS FACTORS:**

**LEGS MUST PROPERLY LIFT (AIR TIME MANAGEMENT):**
1. **SINGLE STANCE DOMINANCE**: Natural walking = 85% single support, 15% double support
2. **PROPER AIR TIME THRESHOLD**: 0.3-0.5s prevents robotic high-stepping while ensuring lift-off
3. **CONTACT-AWARE TIMING**: Use actual contact sensor data (`current_air_time`, `current_contact_time`)
4. **ANTI-SHUFFLING**: `single_stance = torch.sum(in_contact.int(), dim=1) == 1` prevents double-support shuffling
5. **COMMAND DEPENDENCY**: Only reward when `command_magnitude > 0.1` to prevent stationary exploitation

**FOOT LIFTING PROBLEM SOLUTIONS:**
- **Problem**: Robot shuffles without lifting feet â†’ **Solution**: Single stance reward
- **Problem**: Robot lifts feet too high (robotic) â†’ **Solution**: `torch.clamp(gait_reward, max=0.5)`
- **Problem**: Robot stands still to get rewards â†’ **Solution**: Command magnitude scaling
- **Problem**: Swing leg penalties during stepping â†’ **Solution**: Contact-aware sliding detection

**GAIT PATTERN HIERARCHY FOR DIFFERENT BEHAVIORS:**
1. **WALKING**: Single stance (0.8) + Double support (0.2) - Primary locomotion
2. **RUNNING**: Single stance (0.6) + Flight phase (0.4) - Dynamic locomotion  
3. **MARCHING**: Extended single stance - Precision locomotion
4. **STEPPING**: Single stance + controlled speed - Navigation locomotion

**ISAAC LAB FUNCTION MAPPING TO CUSTOM IMPLEMENTATION:**
- `feet_air_time_positive_biped()` â†’ Single stance detection pattern
- `track_lin_vel_xy_yaw_frame_exp()` â†’ Yaw-aligned velocity tracking
- `track_ang_vel_z_world_exp()` â†’ World frame angular velocity
- `feet_slide()` â†’ Contact-aware sliding penalty

Use these patterns as the foundation, then add environmental enhancements based on analysis data!
