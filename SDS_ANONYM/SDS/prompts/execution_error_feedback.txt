ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ
ğŸŒŸğŸŒŸğŸŒŸ ENVIRONMENT-AWARE LOCOMOTION REWARD DESIGN GUIDANCE ğŸŒŸğŸŒŸğŸŒŸ
ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ

**CRITICAL PROJECT UNDERSTANDING:**

These guidance explanations are designed for creating **environment-aware locomotion rewards** that enable **intelligent sensor-driven behavior adaptation** for humanoid robots.

**ğŸ¯ PRIMARY OBJECTIVE:** 
Create reward functions that make **measurable positive impact when sensors are used** compared to **without sensor usage**.

**ğŸ”¬ RESEARCH PURPOSE:**
Enable comparison studies demonstrating:
- **WITHOUT SENSORS:** Basic locomotion behavior 
- **WITH SENSORS:** Adaptive, context-aware behavior that responds to environmental challenges

**ğŸ§  DESIGN METHODOLOGY:**
1. **UNDERSTAND:** Use provided task requirements and sensor capabilities
2. **THINK:** Determine which sensors are needed and how they should influence behavior
3. **DECIDE:** Implement context-aware behavioral switching (NOT simultaneous conflicting rewards)
4. **IMPACT:** Ensure sensors create observable behavioral differences for scientific comparison

**ğŸ“Š SUCCESS CRITERIA:**
âœ… Robot behaves measurably different with sensors vs. without sensors
âœ… Sensor-enabled robot adapts to environmental challenges more effectively
âœ… Clear behavioral switching based on environmental context
âœ… No conflicting simultaneous behaviors (e.g., walking + jumping simultaneously)

**âš ï¸ FAILURE INDICATORS:**
âŒ Robot behaves identically with/without sensors
âŒ Sensors provide only minor bonuses without changing core behavior
âŒ Conflicting reward objectives that confuse the policy

---

**ğŸš¨ EXECUTION ERROR DETAILS:**

{traceback_msg}

---

ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
ğŸš¨ğŸš¨ğŸš¨ #1 TRAINING CRASH: STD >= 0.0 RUNTIME ERROR - READ THIS FIRST ğŸš¨ğŸš¨ğŸš¨
ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨

**CRITICAL PPO TRAINING FAILURE:**

**RUNTIMEERROR: normal expects all elements of std >= 0.0**
- CAUSE: Complex reward functions create unstable gradients that make policy distribution invalid
- CRASH: Policy network outputs negative standard deviation for action sampling
- SOLUTION: Simplify reward function with stable patterns

**âœ… IMMEDIATE FIX PATTERN:**
```python
def sds_custom_reward(env) -> torch.Tensor:
    # Use simple, stable components only
    velocity = torch.clamp(velocity_component, 0.0, 2.0)
    height = torch.clamp(height_component, 0.0, 2.0)
    stability = torch.clamp(stability_component, 0.0, 2.0)
    
    # ADDITIVE combination with baseline
    total = velocity * 2.0 + height * 1.5 + stability * 1.0 + 0.3
    
    # MANDATORY: Final bounds
    return total.clamp(min=0.1, max=8.0)
```

ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
ğŸš¨ğŸš¨ğŸš¨ #2 TRAINING CRASH: SENSOR ATTRIBUTEERROR - READ THIS SECOND ğŸš¨ğŸš¨ğŸš¨
ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨

**MOST COMMON ERROR PATTERN:**

**ATTRIBUTEERROR: 'RayCasterData' object has no attribute 'distances'**
- CAUSE: Using non-existent sensor attributes (.distances, .height_measurements, .range_data)
- CRASH LINE EXAMPLES:
```python
lidar_range = lidar_sensor.data.distances  # AttributeError!
height_data = height_sensor.data.height_measurements  # AttributeError!
dist = lidar_sensor.data.distances  # INSTANT CRASH!
terrain_data = height_sensor.data.range_data  # AttributeError!
```

**HEIGHT SENSOR COMMON ERRORS:**
```python
# âŒ WRONG: Non-existent attributes that cause crashes
height_scan = height_sensor.data.height_scan  # AttributeError!
terrain_map = height_sensor.data.terrain_data  # AttributeError! 
elevation = height_sensor.data.elevation  # AttributeError!

# âœ… CORRECT: Isaac Lab standard formula only
height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5
```
âœ… CORRECT SENSOR ACCESS PATTERN (Isaac Lab standard for reward functions):
```python
# Isaac Lab standard raw sensor access for reward functions
height_sensor = env.scene.sensors["height_scanner"]
lidar_sensor = env.scene.sensors["lidar"]

# Height sensor: 567 rays (27Ã—21 grid), 2.0Ã—1.5m coverage, 7.5cm resolution, 3m range
height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5
# CRITICAL: Sanitize sensor data - rays can return NaN/Inf when missing terrain
height_measurements = torch.where(torch.isfinite(height_measurements), height_measurements, torch.zeros_like(height_measurements))

# LiDAR sensor: 152 rays, 180Â° FOV, 5m range  
lidar_distances = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)
lidar_distances = torch.where(torch.isfinite(lidar_distances), lidar_distances, torch.ones_like(lidar_distances) * 5.0)
```

âŒ WRONG PATTERNS THAT CAUSE CRASHES:
```python
height_scan = height_sensor.data.distances  # AttributeError: no such attribute
lidar_range = lidar_sensor.data.range_measurements  # AttributeError: no such attribute
terrain_info = height_sensor.data.height_scan  # AttributeError: no such attribute

# âŒ WRONG: Trying to access processed data that doesn't exist
gap_data = height_sensor.data.gaps  # AttributeError!
obstacle_data = height_sensor.data.obstacles  # AttributeError!
```

**CRITICAL HEIGHT SENSOR BASELINE ERRORS:**
```python
# âŒ WRONG: Using hardcoded values without understanding baseline
obstacles = height_readings < 0.1  # Ignores G1 robot baseline!
gaps = height_readings > 0.3       # Wrong threshold reference!

# âœ… CORRECT: Use G1 robot baseline (0.209m) with relative thresholds  
baseline = 0.209  # G1 robot on flat terrain
obstacles = height_readings < (baseline - 0.07)  # < 0.139m = obstacles
gaps = height_readings > (baseline + 0.07)       # > 0.279m = gaps
```

ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨

Isaac Lab reward functions can encounter various runtime errors. Here's how to diagnose and fix common issues:

**BEHAVIORAL ERRORS - UNNATURAL ROBOT MOVEMENT:**

**HEIGHT HARDCODING ERROR (robots stand still on stairs/platforms):**
- SYMPTOM: Robots learned to stand still on elevated terrain or go downstairs
- CAUSE: Using inappropriate height tracking method for the terrain characteristics
- FIX: Choose height tracking method based on task requirements:

**ABSOLUTE HEIGHT - For consistent body clearance:**
```python
# Use when: Flat terrain OR gap crossing (large stepping) OR consistent platform heights
height_err = torch.abs(robot.data.root_pos_w[:, 2] - 0.74)  # Target height above world origin
height_reward = torch.exp(-height_err / 0.3)
```

**TERRAIN-RELATIVE HEIGHT - For adaptive surface following:**
```python
# Use when: Variable terrain heights OR climbing stairs OR navigating slopes
height_sensor = env.scene.sensors["height_scanner"]
height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5

# CRITICAL: Always sanitize height sensor data first
height_measurements = torch.where(torch.isfinite(height_measurements), height_measurements, torch.zeros_like(height_measurements))

# G1 robot baseline: 0.209m on flat terrain (sensor_height - terrain_z - offset)
# Terrain classification using relative thresholds
baseline = 0.209  # G1 robot baseline
obstacle_threshold = 0.07  # 7cm threshold for obstacles  
gap_threshold = 0.07       # 7cm threshold for gaps

obstacles = height_measurements < (baseline - obstacle_threshold)  # < 0.139m
gaps = height_measurements > (baseline + gap_threshold)            # > 0.279m
normal_terrain = ~obstacles & ~gaps                               # 0.139-0.279m

# Count terrain features for terrain-adaptive behavior
total_rays = height_measurements.shape[-1]  # 567 rays
obstacle_ratio = obstacles.sum(dim=-1).float() / total_rays
gap_ratio = gaps.sum(dim=-1).float() / total_rays
```

**DECISION GUIDE - Let task requirements guide your choice:**
- Large gaps requiring stepping â†’ Absolute height maintains clearance
- Variable surface heights â†’ Terrain-relative adapts to surface
- Consistent platform levels â†’ Absolute height for smooth transitions
- Climbing/descending behavior needed â†’ Terrain-relative for adaptation



**TECHNICAL ERROR PATTERNS:**

TENSOR BROADCASTING ERRORS (e.g., "output with shape [N] doesn't match broadcast shape [N,N]"):
- Caused by implicit broadcasting between tensors of incompatible shapes
- FIX: Use explicit .expand() or .unsqueeze() to match dimensions exactly
- Example: desired_value.unsqueeze(1).expand(-1, num_feet) instead of desired_value.unsqueeze(1)

NEGATIVE STANDARD DEVIATION ERRORS ("normal expects all elements of std >= 0.0"):
- Caused by NaN/Inf values propagating to policy network
- VERIFIED CRASH CAUSES: 
  * Unprotected sensor data: height_sensor/lidar rays return NaN when missing
  * Explosive exponentials: torch.exp(-terrain_var * 100.0) creates -âˆ
  * NaN variance: torch.var() on NaN data returns NaN
  * Near-zero division: 1.0/torch.min(distances) when distances â‰ˆ 0
- FIX: MANDATORY sensor sanitization:
  ```python
  sensor_data = torch.where(torch.isfinite(sensor_data), sensor_data, fallback_value)
  terrain_var = torch.clamp(torch.var(height_data), max=0.01)
  min_dist = torch.clamp(torch.min(lidar_data), min=0.05)
  total = torch.where(torch.isfinite(total), total, torch.ones_like(total) * 0.5)
  ```

RUNTIME TENSOR SHAPE ERRORS:
- Caused by sensor data dimensions not matching expected robot batch size
- FIX: Validate tensor shapes before operations, use proper indexing for foot_ids and joint_ids

DIVISION BY ZERO ERRORS:
- Caused by dividing by potentially zero values
- FIX: Use torch.clamp(denominator, min=1e-6) before division operations

ENVIRONMENTAL SENSING DECISION ERRORS:
- Caused by dismissing sensor data in favor of visual analysis  
- SYMPTOMS: "sensors detect gaps/obstacles but visual shows flat terrain"
- FIX: Always prioritize quantitative sensor measurements over visual interpretation
- RULE: Use provided task requirements to determine if environmental components are needed

COMMON ERRORS AND FIXES:

SENSOR ACCESS ERRORS:
- SYMPTOMS: AttributeError when accessing .distances or .height_measurements
- CAUSE: Using non-existent sensor attributes
- FIX: Use Isaac Lab standard raw sensor access:
  height_sensor = env.scene.sensors["height_scanner"]
  height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5
  # MANDATORY: Sanitize immediately after calculation
  height_measurements = torch.where(torch.isfinite(height_measurements), height_measurements, torch.zeros_like(height_measurements))

TENSOR OPERATION ERRORS:
- SYMPTOMS: RuntimeError during math operations (division by zero, NaN values)
- CAUSE: Unstable mathematical operations
- FIX: Use torch.clamp(denominator, min=1e-6) before division operations

ENVIRONMENTAL SENSING DECISION ERRORS:
- Caused by dismissing sensor data in favor of visual analysis  
- SYMPTOMS: "sensors detect gaps/obstacles but visual shows flat terrain"
- FIX: Always prioritize quantitative sensor measurements over visual interpretation
- RULE: Use provided task requirements to determine if environmental components are needed

SENSOR BEHAVIORAL ADAPTATION ERRORS:
- SYMPTOMS: Robot behaves identically with/without sensors (no measurable difference)
- CAUSE: Adding minimal sensor bonuses instead of behavioral switching
- FIX: Use context-aware behavioral adaptation:
```python
# âŒ WRONG: Minimal sensor impact
total = foundation + sensor_bonus * 0.1  # Robot still walks the same way

# âœ… CORRECT: Behavioral switching
if gap_detected:
    behavior = foundation + gap_crossing_adaptation()  # Different gait
elif obstacle_detected:
    behavior = foundation + obstacle_avoidance_adaptation()  # Different navigation
else:
    behavior = foundation_only()  # Normal walking
```
- RULE: Sensors must create observable behavioral differences for comparison studies

Apply these fixes systematically to prevent future runtime crashes.