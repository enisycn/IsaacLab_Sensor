Please carefully analyze the policy feedback and provide a new, improved reward function that can better solve the task.

Project context (terrain-aware guidance, not prescriptions):
- If the SUS includes `TERRAIN_CLASS: [0|1|2|3]`, use it to decide which families of components to consider (0 SIMPLE foundation-only; 1 GAP height-based handling; 2 OBSTACLES LiDAR safety; 3 STAIRS relaxed-height). Choose the specific components/thresholds/weights based on the analysis rather than hardcoding.
- Keep rewards additive + clamped; use direct sensor access; avoid multiplicative chains.
- Prefer deriving thresholds/weights from feedback and environment metrics, not fixed constants.

**üö® MANDATORY: YOUR RESPONSE MUST INCLUDE COMPREHENSIVE FEEDBACK ANALYSIS! üö®**

**üé• CRITICAL: TRAINING FOOTAGE BEHAVIOR ANALYSIS**

**üîç FRAME-BY-FRAME ANALYSIS PROTOCOL:**
1. **Motion Detection**: Compare robot position across frames - is it moving forward?
2. **Gait Analysis**: Check if legs show alternating contact patterns
3. **Environmental Interaction**: Does robot navigate terrain features or avoid them?
4. **Stability Assessment**: Is robot maintaining balance while moving?
5. **Task Progress**: Is robot making progress toward locomotion goals?


**CRITICAL: Generate the sds_custom_reward function with detailed analysis in the docstring, following the format:**
- üîç FEEDBACK ANALYSIS (analyze the policy performance issues)
- üìä PERFORMANCE METRICS (actual training metrics and observations)
- üéØ SOLUTION STRATEGY (reasoning for the improvement approach)
- üìã IMPLEMENTATION COMPONENTS (with weight priorities)

**üéØ TERRAIN STRATEGY FLEXIBILITY & SENSOR EFFECTIVENESS:**

**NAVIGATION STRATEGY AUTONOMY:**
- **Gap Terrain**: Robot can traverse through gaps, step over them, go around, adjust stride length, etc. - GPT decides based on environmental analysis
- **Obstacle Terrain**: Navigate around obstacles, step over small ones, find optimal paths, maintain clearance, etc.
- **Stair Terrain**: Climb stairs, descend carefully, use step-following behavior, adapt height control, etc.
- **Simple Terrain**: Focus on locomotion efficiency, speed optimization, energy conservation, etc.

**ENVIRONMENT-AWARE SENSOR IMPACT:**
- **Height Scanner Usage**: Should provide measurable improvements in terrain navigation, gap detection, step planning, etc.
- **LiDAR Integration**: Should enable better obstacle avoidance, path planning, collision prevention, etc.
- **Sensor Combination**: Multi-modal sensing should result in significantly better performance metrics vs. foundation-only
- **Measurable Benefits**: 15-30% improvement in success rates, safety metrics, task completion, etc.

**STRATEGY SELECTION GUIDANCE:**
- Use sensor data to inform behavioral choices, not rigid rules
- Adapt strategy based on detected environmental characteristics
- Let environmental analysis guide optimal navigation approach
- No strict behavioral prescriptions - allow contextual decision making

**üìä ENHANCED METRICS ANALYSIS GUIDANCE:**

You now receive 15+ comprehensive training metrics (instead of just 2 basic ones). Use these for detailed analysis:

**üöÄ CRITICAL: SINGLE-SKILL FOCUS FOR TEACHER-STUDENT LEARNING**

Each training session develops **ONE SPECIALIZED LOCOMOTION SKILL** for teacher-student learning:

**‚ö†Ô∏è AVOID CONFLICTING BEHAVIORS:**
- ‚ùå NO simultaneous walking + jumping in same reward function
- ‚ùå NO mixed locomotion strategies that confuse policy learning
- ‚ùå NO hardcoded behaviors - develop strategy from environmental analysis

**‚úÖ FOCUS ON SKILL SPECIALIZATION:**
- **Analysis-Driven:** Base locomotion strategy on environmental sensor data
- **Single-Skill:** Each training produces one specialized teacher policy
- **Environment-Responsive:** Let challenges determine appropriate locomotion approach
- **Teacher Quality:** Clean, focused skills make better teacher policies for student learning

**üéØ SENSOR UTILIZATION FOR COMPARATIVE ANALYSIS:**
- **MANDATORY in Environment-Aware Mode:** Actively use height scanner and LiDAR data in rewards
- **Performance Target:** Achieve 15-30% improvement over foundation-only mode
- **Measurable Benefits:** Lower termination rates, better navigation, higher rewards
- **Foundation-Only Mode:** Use only proprioceptive data - NO sensor access
- **Critical Contrast:** Sensor vs. non-sensor performance demonstrates environmental sensing value

**Core Performance Metrics:**
- `reward`: Total episode reward progression
- `episode length`: Episode duration trends
- `reward_sds_custom`: Your custom reward component breakdown

**Task Performance Indicators:**
- `velocity_error_xy`, `velocity_error_yaw`: Locomotion accuracy
- `termination_base_contact`: Fall/contact failure rates
- `termination_timeout`: Task completion rates

**Training Stability Diagnostics:**
- `value_function_loss`, `surrogate_loss`: Neural network training health
- `action_noise_std`: Exploration level progression
- `entropy_loss`: Policy exploration behavior

**System Performance Monitoring:**
- `computation_steps_per_sec`: Training efficiency
- `collection_time`, `learning_time`: Performance bottlenecks

**‚úÖ NEW: Environmental Sensing & Robot Stability:**
- `terrain_height_variance`: Terrain roughness under robot (higher = more challenging terrain)
- `terrain_complexity_score`: Obstacle/gap/cliff percentage (higher = more dangerous terrain)
- `robot_height_baseline`: Height baseline above terrain (stability indicator)
- `body_orientation_deviation`: Roll/pitch from upright (lower = better posture control)
- `height_tracking_error`: Target height maintenance accuracy

**Analysis Tips:**
‚Ä¢ Flat or identical values ‚Üí Component not optimizing (adjust scale/rewrite)
‚Ä¢ Large magnitude differences ‚Üí Re-scale components to proper ranges
‚Ä¢ Increasing error rates ‚Üí Check reward conflicts or mathematical instability
‚Ä¢ Decreasing reward components ‚Üí Verify component is actually beneficial
‚Ä¢ ‚úÖ NEW: High terrain variance + high orientation deviation ‚Üí Robot struggling with rough terrain
‚Ä¢ ‚úÖ NEW: High height tracking error ‚Üí Reward function may not be encouraging proper height control
‚Ä¢ ‚úÖ NEW: Inconsistent robot height above terrain ‚Üí Check terrain adaptation and balance

Some helpful tips for analyzing the policy feedback:

    **üö® FIRST PRIORITY: CHECK FOR TRAINING CRASH (std >= 0.0 error) üö®**
    (-1) **Critical Training Failure:** If getting "normal expects all elements of std >= 0.0" error:
        (a) **VERIFIED CRASH CAUSES**: 
            * Unprotected sensor data: height_sensor.data.ray_hits_w[..., 2] ‚Üí NaN when rays miss
            * Explosive exponentials: torch.exp(-terrain_var * 100.0) ‚Üí -‚àû values
            * NaN variance propagation: torch.var(sensor_data_with_nan) ‚Üí NaN
            * Near-zero division: 1.0/torch.min(distances) when distances ‚âà 0.001
            * **torch.clamp() argument errors**: torch.clamp(scalar, min=value) ‚Üí TypeError
        (b) **MANDATORY SENSOR SAFETY**: 
            ```python
            sensor_data = torch.where(torch.isfinite(sensor_data), sensor_data, fallback)
            terrain_var = torch.clamp(torch.var(height_data), max=0.01)
            total = torch.where(torch.isfinite(total), total, torch.ones_like(total) * 0.5)
            ```
        (c) **CRITICAL: TORCH.CLAMP() TYPEERROR FIX**:
            ```python
            # ‚ùå WRONG: Nested clamp with scalar causes TypeError
            result = torch.clamp(1.0 - value / torch.clamp(tol, min=1e-3), 0.0, 1.0)
            
            # ‚úÖ CORRECT: Use Python max() for scalar safety
            safe_denominator = max(tol, 1e-3)
            result = torch.clamp(1.0 - value / safe_denominator, 0.0, 1.0)
            
            # ‚úÖ ALTERNATIVE: Convert scalar to tensor first
            safe_denom = torch.clamp(torch.tensor(tol, device=device), min=1e-3)
            result = torch.clamp(1.0 - value / safe_denom, 0.0, 1.0)
            ```
        (d) **Stability Requirements**: Each component should be [0, 2] range, final reward [0.0, 5.0]
        (e) **Safe Pattern**: `total = a*2.0 + b*1.5 + c*1.0 + baseline`
        (f) **Never Use**: Multiplicative chains, division without protection, aggressive exponentials, **nested torch.clamp with scalars**

    **üö® SECOND PRIORITY: CHECK FOR SPARSE REWARD PROBLEMS üö®**
    (0) **Sparse Reward Analysis:** If mean reward is consistently low or zero:
        (a) **Zero Reward Problem**: Check if most robots get zero reward most of the time
        (b) **Learning Progression Missing**: Verify robot gets reward for basic movement attempts, not just perfect execution
        (c) **Foundation-First Fix**: Start with simple movement rewards (upward velocity for jumping, forward motion for walking)
        (d) **Progressive Complexity**: Add coordination and perfection rewards only after foundation works
        (e) **Dense Feedback Rule**: Robot should get SOME reward for ANY progress toward goal
        (f) **Example Fix**: Replace `flight_time_perfect_reward` with `upward_movement_reward + height_gain_reward + flight_bonus`

    **üö® THIRD PRIORITY: CHECK FOR TRAPPING PROBLEMS üö®**
    (0.5) **Anti-Trapping Analysis:** If robot converges to unwanted behavior (standing still, minimal effort):
        (a) **Baseline Check**: Verify baseline reward is minimal - just enough to prevent zeros
        (b) **Movement Incentive**: Ensure task attempts are substantially more rewarding than inactivity  
        (c) **Activity Requirement**: Primary rewards should require target behavior, not passive existence
        (d) **Quick Test**: Calculate "doing nothing" vs "attempting task" rewards - attempts should be much better

    (1) If the values for a certain reward component are near identical throughout, then this means RL is not able to optimize this component as it is written. You may consider
        (a) Changing its scale or the value of its temperature parameter
        (b) Re-writing the reward components
        (c) Discarding the reward components
        (d) Add new reward components
    (2) If some reward components' magnitude is significantly larger, then you must re-scale its value to a proper range

    (4) **Foundation-First Stability Analysis:** If training is crashing or mean reward is zero:
        (a) **FIRST**: Check if basic locomotion foundation is stable (height, velocity, orientation)
        (b) **SECOND**: Verify mathematical stability (no aggressive exponentials, tight tolerances, or multiplicative chains)
        (c) **THIRD**: Check if environmental components are conflicting with basic locomotion
        (d) **SOLUTION**: Start with foundation-only reward, then add complexity incrementally
    (5) **Enhanced Environment Reward Issues (example config):** If mean reward is consistently zero with sensors available:
        (a) **Sensor Access Verification**: Test height_scanner and lidar data access separately
        (b) **Backward Movement Integration**: Check if negative velocity commands (-0.1 to 0.4 range) work with terrain analysis
        (c) **Velocity-Obstacle Conflict Debugging**: If robot stops near obstacles, check adaptive velocity scaling math
        (d) **Stair vs Gap Classification Issues**: If robot freezes at terrain changes, verify height gradient detection
        (e) **Gap Behavior Switching Problems**: If inconsistent behaviors, check gap size classification thresholds
        (f) **Sensor Mathematical Stability**: Replace complex terrain processing with simple forward/backward clearance checks
        (g) **Foundation First**: Always test basic locomotion (velocity, height, orientation) before adding sensor components
        (h) **Progressive Integration**: Add sensor components incrementally: velocity tracking ‚Üí basic contact ‚Üí terrain adaptation
        (i) **Reward Balance Check**: Ensure sensor-based rewards don't overwhelm core locomotion components (max 30% weight)
        (j) **Flat-with-Box Specifics**: Use 27x21 height scanner (567 points, 7.5cm resolution, 2x1.5m coverage) and 152-ray lidar (8 channels √ó 19 horizontal rays, 180¬∞ FOV, 5.0m max range), not generic sensor assumptions
    (6) **Mathematical Stability Check:** If rewards are consistently zero or training crashes:
        (a) Replace aggressive exponentials (factors >3.0) with moderate ones (0.5-2.0)
        (b) Use relaxed tolerances (0.3-1.0) instead of tight ones (<=0.1)
        (c) Switch from multiplicative to additive reward combinations
        (d) Add small baseline bonus (~+0.1) to ensure non-zero minimum
        (e) Use safe division patterns and clamp final rewards to min=0.1
    (7) **Advanced Locomotion Pattern Debugging (NEW):** For enhanced environment failures:
        (a) **Velocity-Obstacle Conflicts**: If robot stops at obstacles but velocity commands are non-zero:
            - Check adaptive velocity scaling: `obstacle_factor = torch.clamp((terrain_clearance + 0.2) / 0.4, min=0.1, max=1.0)`
            - Verify terrain clearance calculation from height scanner forward strips
            - Ensure velocity rewards use adapted commands, not original commands
        (b) **Stair Behavior Issues**: If robot behavior is incorrect for STAIRS scenario:
            - Use task requirements for stair locomotion
            - Apply stair-specific locomotion rewards based on task needs
        (c) **Gap Behavior Issues**: If behavior is incorrect for GAP scenario:
            - Use environmental analysis to develop appropriate navigation strategy  
            - Apply gap-specific thresholds based on task specifications
            - Ensure conditional rewards use proper masking: `torch.where(gap_condition.unsqueeze(-1), reward_value, zeros)`
        (d) **Backward Movement Problems**: If negative velocity commands cause issues:
            - Check bidirectional velocity tracking: separate forward/backward error calculation
            - Verify sensor coverage: height scanner and lidar cover backward movement areas
            - Ensure yaw-aligned velocity transformation works with negative commands
        (e) **Sensor Integration Crashes**: If adding height_scanner or lidar causes failures:
            - **CRITICAL**: Use direct sensor data access from Isaac Lab APIs
            - **Verified Failure**: Raw sensor data contains NaN/Inf when rays miss targets
            - Test sensor data shapes: height_measurements should be [num_envs, 567], lidar_distances should be [num_envs, 152] 
            - Check grid access: use height_measurements directly (already flattened from 27x21 grid)
            - Verify ray indexing: forward_rays = lidar_distances[:, :76], backward_rays = lidar_distances[:, 76:]
            - **LiDAR Max Range**: 5.0m - infinite values mean no obstacles beyond range (handle with torch.where)
            - **Production Fix**: Use numerical stability with clamping: `torch.clamp(total, min=reasonable_min, max=reasonable_max)`
        (f) **Enhanced Environment Specific**: For Isaac-SDS-Velocity-Flat-G1-Enhanced-v0:
            - Use exact sensor configurations: GridPatternCfg (2.0x1.5m, 0.075m resolution), LidarPatternCfg (8 channels, 180¬∞ FoV, 10¬∞ resolution)
            - Account for velocity ranges: x: (-0.1, 0.4), y: (-0.15, 0.15), z: (-0.3, 0.3)
            - Focus on legs + torso coordination for locomotion (exclude arm-specific requirements)
Please analyze each existing reward component in the suggested manner above first, and then write the reward function code. 

**Cadence and Air-Time Control (Walking and Jumping):**

- **Walking (alternating gait) targets:**
  - Swing duration: 0.35‚Äì0.55 s; Stance duration: 0.25‚Äì0.45 s
  - Contact ratio: 70‚Äì80%; Double support: ~10‚Äì20% of cycle
  - Penalize ultra-short phases: add small hinge penalties for swing/stance <0.15‚Äì0.20 s
  - Use `feet_air_time_positive_biped(threshold=0.55‚Äì0.65)` and clamp the component to prevent high-knee tapping
  - Optional: add a small step-frequency limiter (penalty ‚àù transitions per second) to discourage rapid micro‚Äësteps

- **Jumping (bilateral flight) targets:**
  - Reward synchronized takeoff/landing using bilateral measures (e.g., `torch.min(air_time, dim=1)[0]`), not single-foot maxima
  - Basic vertical jumps: bilateral flight 0.20‚Äì0.40 s; walking tasks should avoid prolonged flight
  - Gate jumping components so they are only active in jump skills; do not mix with walking cadence terms in the same reward

- **Command gating (both):**
  - Gate gait/velocity rewards by non-trivial commands (e.g., `||cmd_xy||>0.1`, `|cmd_yaw|>0.05`) to avoid rewarding inactivity

- **Monitoring (both):**
  - Track distributions of `current_air_time`, `current_contact_time`, step transitions per second, `contact_ratio`, and (for jumping) bilateral flight fraction; retune thresholds/weights if you observe contact_ratio >0.9 (shuffling) or swing/stance <0.15 s (rapid stepping)

**üîß DEBUGGING: GAP NAVIGATION BEHAVIORAL ISSUES**

**SYMPTOM: Robot turns in place, doesn't move forward**
**DIAGNOSIS:**
- Environmental penalties (-1.0 to -2.0) overwhelm velocity rewards (+3.0)
- Fixed baseline misclassifies normal terrain as gaps
- Robot optimizes by staying stationary to avoid penalties

**FIXES:**
1. **Reduce environmental impact:** Use 60/40 foundation/environmental split
2. **Dynamic baseline:** Calculate from actual terrain, not hardcoded 0.209
3. **Positive structure:** Replace penalties with progressive bonuses

**SYMPTOM: Robot walks around gaps instead of through them**
**DIAGNOSIS:**
- Environment-based navigation: Develop strategy based on environmental analysis
- Robot learns path planning to avoid gap areas entirely
- Missing gap traversal incentives

**FIXES:**
1. **Gap traversal bonus:** `forward_vel * gap_ratio * weight`
2. **Safety limits:** Progressive penalties only for excessive gap density
3. **Navigation rewards:** Bonus for successful forward movement through challenges

**CRITICAL WEIGHT STRUCTURE:**
```python
# Velocity must dominate to prevent stationary optimization
foundation = vel * 3.0 + gait * 2.0 + height * 2.0 + lean * 1.5 + baseline
environmental = gap_traversal + safety_penalty + terrain_bonus
total = foundation * 0.6 + environmental * 0.4  # Foundation dominates
``` 