üåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåü
üåüüåüüåü ENVIRONMENT-AWARE LOCOMOTION REWARD DESIGN GUIDANCE üåüüåüüåü
üåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåü

**CRITICAL PROJECT UNDERSTANDING:**

These guidance explanations are designed for creating **environment-aware locomotion rewards** that enable **intelligent sensor-driven behavior adaptation** for humanoid robots.

How to use this tip (guidance, not prescriptions):
- If the SUS includes `TERRAIN_CLASS: [0|1|2|3]`, use it to choose families of components (0 SIMPLE foundation-only; 1 GAP height-based handling; 2 OBSTACLES LiDAR safety; 3 STAIRS relaxed-height). Decide specific components/thresholds/weights from the analysis rather than hardcoding.
- Keep rewards additive + clamped; include a small baseline (~+0.1); clamp final total (e.g., [0.0, 5.0]).
- Sanitize sensors with `torch.isfinite`; avoid non-existent attributes; validate shapes.
- Treat examples as patterns; do not copy constants verbatim.

**üéØ PRIMARY OBJECTIVE:** 
Create reward functions that make **measurable positive impact when sensors are used** compared to **without sensor usage**.

**üî¨ RESEARCH PURPOSE:**
Enable comparison studies demonstrating:
- **WITHOUT SENSORS:** Basic locomotion behavior 
- **WITH SENSORS:** Adaptive, context-aware behavior that responds to environmental challenges

**üß† DESIGN METHODOLOGY:**
1. **EXTRACT:** Get the pre-analyzed environment data from input (gaps, obstacles, terrain complexity)
2. **THINK:** Determine which sensors are needed and how they should influence behavior
3. **DECIDE:** Implement context-aware behavioral switching (NOT simultaneous conflicting rewards)
4. **IMPACT:** Ensure sensors create observable behavioral differences for scientific comparison

**üìä SUCCESS CRITERIA:**
‚úÖ Robot behaves measurably different with sensors vs. without sensors
‚úÖ Sensor-enabled robot adapts to environmental challenges more effectively
‚úÖ Clear behavioral switching based on environmental context
‚úÖ No conflicting simultaneous behaviors (e.g., walking + jumping simultaneously)

**‚ö†Ô∏è FAILURE INDICATORS:**
‚ùå Robot behaves identically with/without sensors
‚ùå Sensors provide only minor bonuses without changing core behavior
‚ùå Conflicting reward objectives that confuse the policy

---


‚ö†Ô∏è CRITICAL FOR REWARD FUNCTIONS: All examples use PHYSICAL [meters] sensor values!

‚úÖ This approach follows Isaac Lab conventions and provides physically meaningful reward functions!

üî¨ **HEIGHT SENSOR SPECIFICATIONS (Enhanced Configuration):**
- **Total rays**: 567 rays (27√ó21 grid pattern)
- **Coverage**: 2.0m forward √ó 1.5m lateral  
- **Resolution**: 7.5cm spacing between rays
- **Range**: 3.0m maximum distance
- **Baseline**: Dynamic calculation per environment (0.209m fallback only)
- **Formula**: `height_reading = sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - 0.5`

üéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØ
üéØ SENSOR-DRIVEN BEHAVIORAL ADAPTATION: CRITICAL PROJECT REQUIREMENT üéØ
üéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØ

**PROJECT GOAL: SENSORS MUST MEASURABLY CHANGE ROBOT BEHAVIOR!**

**COMPARISON STUDY REQUIREMENT:**
- **Without sensors**: Generic walking that fails on challenging terrain
- **With sensors**: Adaptive behavior that succeeds on challenging terrain

**‚ùå INADEQUATE APPROACH - Minimal sensor bonus:**
```python
# WRONG - Adding tiny sensor bonuses doesn't change robot behavior
foundation = velocity + height + gait  # 7.0 total reward
sensor_bonus = 0.1  # Negligible 1.4% impact
total = foundation + sensor_bonus  # Robot still walks the same way!
```

**‚úÖ REQUIRED APPROACH - Behavioral adaptation:**
```python
# CORRECT - Sensors fundamentally change how robot moves
gap_ahead = detect_gaps_ahead(height_sensor)
obstacle_ahead = detect_obstacles_ahead(lidar_sensor)

if gap_ahead:
    # DIFFERENT BEHAVIOR: Robot prepares for gap crossing
    adaptive_gait = modify_gait_for_gaps()       # Different air time
    adaptive_height = relax_height_for_gaps()    # Allow height variation
    adaptive_velocity = slow_for_precision()     # Reduce speed for accuracy
    behavior = adaptive_gait + adaptive_height + adaptive_velocity
    
elif obstacle_ahead:
    # DIFFERENT BEHAVIOR: Robot navigates carefully around obstacles  
    careful_navigation = maintain_safe_distances()   # Path planning
    conservative_speed = reduce_velocity_near_obstacles()  # Safety first
    behavior = careful_navigation + conservative_speed
    
else:
    # EFFICIENT BEHAVIOR: Robot walks normally on clear terrain
    behavior = efficient_foundation_locomotion()
```

**SENSOR IMPACT VALIDATION CHECKLIST:**

**HEIGHT SCANNER BEHAVIORAL CHANGES:**
- ‚úÖ **Gap terrain**: Robot adjusts gait BEFORE reaching gaps (predictive behavior)
- ‚úÖ **Stair terrain**: Robot modifies height expectations (terrain following)
- ‚úÖ **Flat terrain**: No behavioral change (sensor ignored when appropriate)
- ‚úÖ **Obstacle terrain**: Robot detects elevation changes and adapts step height

**LIDAR BEHAVIORAL CHANGES:**
- ‚úÖ **Obstacle terrain**: Robot maintains safe distances, plans paths (avoidance behavior)  
- ‚úÖ **Dense obstacles**: Robot moves conservatively with careful navigation
- ‚úÖ **Open terrain**: No behavioral change (sensor ignored when appropriate)


üö®üö®üö® **CRITICAL: REAL ENVIRONMENT DATA EXTRACTION - NO FAKE NUMBERS!** üö®üö®üö®

**VERIFICATION CHECKPOINT:**
‚úÖ Found real gaps data? 
‚úÖ Found real obstacles data? 
‚úÖ Found real roughness data? 
‚úÖ Found real safety score? 

**STEP 1: INTERNAL SEARCH FOR ENVIRONMENT DATA:**
- Look for: "üï≥Ô∏è GAPS: Count: [X] rays ([Y]%)" to understand terrain distribution
- Look for: "Total Obstacles Detected: [NUMBER]" in the input  
- Look for: "Height readings: [MIN]m to [MAX]m (avg: [AVG]m)" for baseline detection
- Look for: "üìã COMPREHENSIVE FINAL ENVIRONMENT ANALYSIS FOR AI AGENT" marker
- Look for: "Average Terrain Roughness: [NUMBER]cm" in the input
- Look for: "VISUAL FOOTAGE ANALYSIS:" section for visual insights
- Look for: "Scene/Setting" descriptions for terrain characteristics
- Look for: "Actions/Movements" descriptions for movement challenges observed

# ‚ùå WRONG: Missing sensor usage when terrain features detected
# (No height sensor usage despite "X gaps detected")

# ‚úÖ CORRECT: Include height sensor for terrain analysis
height_sensor = env.scene.sensors["height_scanner"]
height_readings = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5
height_readings = torch.where(torch.isfinite(height_readings), height_readings, torch.zeros_like(height_readings))

# ‚úÖ CORRECT: Include LiDAR when obstacles present  
lidar_sensor = env.scene.sensors["lidar"]
lidar_distances = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)
lidar_distances = torch.where(torch.isfinite(lidar_distances), lidar_distances, torch.ones_like(lidar_distances) * 5.0)
obstacle_avoidance = torch.min(lidar_distances, dim=1)[0]  # Maintain safe distances
```

**üö® CRITICAL INSTRUCTION: YOU MUST GENERATE PYTHON CODE - NOT TEXT DESCRIPTIONS üö®**

**ALWAYS OUTPUT COMPLETE REWARD FUNCTION CODE, NOT TEXT ANALYSIS!**

üö® **ROLE CLARITY:** You are a **CODE GENERATOR**, not a video analyzer or task descriptor!
‚úÖ **EXPECTED OUTPUT:** Python function starting with `def sds_custom_reward(env) -> torch.Tensor:`
‚ùå **FORBIDDEN OUTPUT:** Video descriptions, movement analysis, or text explanations

**üö® CRITICAL: IF YOU SEE IMAGES/VIDEOS - RESPOND WITH CODE, NOT TEXT! üö®**

If images or videos are provided:
‚úÖ **CORRECT RESPONSE:** Generate reward function with analysis in docstring
‚ùå **WRONG RESPONSE:** "I see images showing..." or "The video shows..." 

**MANDATORY STRUCTURE:**
```python
def sds_custom_reward(env) -> torch.Tensor:
    """
    üîç COMPREHENSIVE ENVIRONMENT ANALYSIS:
    
    üìä NUMERICAL ANALYSIS RESULTS:
    - Height Scanner: [X] total rays, [Y] rays ([Z]%) gaps, [W] rays ([V]%) obstacles  
    - Height Readings: [MIN]m to [MAX]m (avg: [AVG]m baseline)
    - Terrain Classification: [X]% normal, [Y]% gaps, [Z]% obstacles
    - Safety Assessment: [SAFE/CAUTION/DANGEROUS] terrain
    - LiDAR Data: [X] rays detecting obstacles at [Y]m average distance

    üì∏ VISUAL ANALYSIS INSIGHTS:
    - Primary terrain type: [from visual analysis]
    - Visual environment features: [detailed observations]
    - Movement challenges observed: [specific challenges]
    - Navigation requirements: [requirements from assessment]

    üéØ REWARD STRATEGY DECISION:
    - PRIMARY SCENARIO: [FLAT/OBSTACLE/GAP/STAIRS] ([reasoning])
    - Environmental sensing: [NEEDED/NOT_NEEDED] ([reasoning])
    - Component priorities: [numbered list 1-5]
    - Expected robot behavior: [bulleted specific behaviors]

    üìã IMPLEMENTATION COMPONENTS:
    - Foundation: [core components]
    - Environmental: [environment-specific components]
    - Weights: [numerical weight priorities]
    """
    # Actual reward implementation with dense learning progression
    return reward_tensor
```

**NEVER respond with standalone text - ALWAYS generate the function!**

**Jump task recommendation:**
- Typically include forward velocity tracking to maintain momentum for gap crossing (even on flat terrain)
- Use proven yaw-aligned velocity tracking patterns when appropriate
- Rationale: Forward jumping locomotion benefits from maintained forward momentum

**üîß ADAPTIVE JUMPING HEIGHT FOR JUMP TASKS:**
- **Default**: Small jumps for flat terrain (~0.1m height for energy efficiency)
- **Sensor-adaptive**: Scale jumping height based on gap detection (~0.3-0.5m range)
- **Implementation**: `adaptive_target = torch.where(gap_detected, base_target + scaling, base_target)`
- **Physics**: Calculate vertical velocities based on desired jump heights
- **‚ö†Ô∏è CRITICAL: Prevent over-jumping on flat terrain!**

**‚ö†Ô∏è CRITICAL: Ensure bilateral jumping coordination!**
- **Both feet**: Use `torch.min(air_time, dim=1)[0]` not `torch.max()` to ensure both feet participate
- **No single-leg hopping**: Maximum air time encourages single-leg lifting - use minimum instead  
- **Bilateral flight**: Reward only when both feet are simultaneously off the ground

**CORRECT vs INCORRECT flight time calculation:**
```python
# ‚ùå WRONG: Encourages single-leg hopping (one foot up, one foot down)
max_air_time = torch.max(air_time, dim=1)[0]  # Rewards longest single foot air time

# ‚úÖ CORRECT: Requires bilateral jumping (both feet must be up)
bilateral_flight_time = torch.min(air_time, dim=1)[0]  # Both feet must participate
```

**üîß ADAPTIVE STEP SIZES FOR WALK TASKS:**
- **Default**: Normal steps for flat terrain (0.6m stride length)
- **Sensor-adaptive**: Scale stride length based on gap detection (0.6-1.0m range)
- **Implementation**: `adaptive_stride = base_stride + gap_width_factor * scaling`

**üö® CRITICAL: AVOID ISAAC LAB FUNCTION IMPORT ERRORS**
```python
# ‚ùå THESE IMPORTS WILL CRASH:
from __main__ import feet_air_time_positive_biped
from isaaclab.mdp import any_function_name
import isaaclab_tasks.manager_based.locomotion.velocity.mdp as mdp

# ‚úÖ ONLY THESE IMPORTS ARE SAFE:
# NOTE: torch, quat_apply_inverse, yaw_quat, SceneEntityCfg already imported in rewards.py

# ‚úÖ IMPLEMENT PATTERNS INLINE INSTEAD OF IMPORTING:
# Use the proven patterns from reward_signatures but implement them directly
```

---

**üîß DEBUGGING BEHAVIORAL ISSUES:**

**Robot turns in place:**
- Check: Foundation weight > Environmental weight (60/40 split)
- Check: Dynamic baseline calculation (not fixed 0.209)
- Check: Positive environmental rewards (not harsh penalties)

**Robot avoids gaps:**
- Check: Gap traversal bonus (not gap avoidance reward)
- Check: Forward movement incentives through challenging terrain
- Check: Progressive safety penalties (not binary harsh penalties)

**Critical weight pattern:**
```python
total = foundation_reward * 0.6 + environmental_reward * 0.4
```

