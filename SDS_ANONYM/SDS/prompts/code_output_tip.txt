ðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸ
ðŸŒŸðŸŒŸðŸŒŸ ENVIRONMENT-AWARE LOCOMOTION REWARD DESIGN GUIDANCE ðŸŒŸðŸŒŸðŸŒŸ
ðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸ

**CRITICAL PROJECT UNDERSTANDING:**

These guidance explanations are designed for creating **environment-aware locomotion rewards** that enable **intelligent sensor-driven behavior adaptation** for humanoid robots.

How to use this tip (guidance, not prescriptions):
- If the SUS includes `TERRAIN_CLASS: [0|1|2|3]`, use it to choose families of components (0 SIMPLE foundation-only; 1 GAP height-based handling; 2 OBSTACLES LiDAR safety; 3 STAIRS relaxed-height). Decide specific components/thresholds/weights from the analysis rather than hardcoding.
- Keep rewards additive + clamped; include a small baseline (~+0.1); clamp final total (e.g., [0.0, 5.0]).
- Sanitize sensors with `torch.isfinite`; avoid non-existent attributes; validate shapes.
- Treat examples as patterns; do not copy constants verbatim.

**ðŸŽ¯ PRIMARY OBJECTIVE:** 
Create reward functions that make **measurable positive impact when sensors are used** compared to **without sensor usage**.

**ðŸ”¬ RESEARCH PURPOSE:**
Enable comparison studies demonstrating:
- **WITHOUT SENSORS:** Basic locomotion behavior 
- **WITH SENSORS:** Adaptive, context-aware behavior that responds to environmental challenges

**ðŸ§  DESIGN METHODOLOGY:**
1. **EXTRACT:** Get the pre-analyzed environment data from input (gaps, obstacles, terrain complexity)
2. **THINK:** Determine which sensors are needed and how they should influence behavior
3. **DECIDE:** Implement context-aware behavioral switching (NOT simultaneous conflicting rewards)
4. **IMPACT:** Ensure sensors create observable behavioral differences for scientific comparison

**ðŸ“Š SUCCESS CRITERIA:**
âœ… Robot behaves measurably different with sensors vs. without sensors
âœ… Sensor-enabled robot adapts to environmental challenges more effectively
âœ… Clear behavioral switching based on environmental context
âœ… No conflicting simultaneous behaviors (e.g., walking + jumping simultaneously)

**âš ï¸ FAILURE INDICATORS:**
âŒ Robot behaves identically with/without sensors
âŒ Sensors provide only minor bonuses without changing core behavior
âŒ Conflicting reward objectives that confuse the policy

---


âš ï¸ CRITICAL FOR REWARD FUNCTIONS: All examples use PHYSICAL [meters] sensor values!

âœ… This approach follows Isaac Lab conventions and provides physically meaningful reward functions!

ðŸ”¬ **HEIGHT SENSOR SPECIFICATIONS (Enhanced Configuration):**
- **Total rays**: 567 rays (27Ã—21 grid pattern)
- **Coverage**: 2.0m forward Ã— 1.5m lateral  
- **Resolution**: 7.5cm spacing between rays
- **Range**: 3.0m maximum distance
- **Baseline**: Dynamic calculation per environment (0.209m fallback only)
- **Formula**: `height_reading = sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - 0.5`

ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯
ðŸŽ¯ SENSOR-DRIVEN BEHAVIORAL ADAPTATION: CRITICAL PROJECT REQUIREMENT ðŸŽ¯
ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯ðŸŽ¯

**PROJECT GOAL: SENSORS MUST MEASURABLY CHANGE ROBOT BEHAVIOR!**

**COMPARISON STUDY REQUIREMENT:**
- **Without sensors**: Generic walking that fails on challenging terrain
- **With sensors**: Adaptive behavior that succeeds on challenging terrain

**âŒ INADEQUATE APPROACH - Minimal sensor bonus:**
```python
# WRONG - Adding tiny sensor bonuses doesn't change robot behavior
foundation = velocity + height + gait  # 7.0 total reward
sensor_bonus = 0.1  # Negligible 1.4% impact
total = foundation + sensor_bonus  # Robot still walks the same way!
```

**âœ… REQUIRED APPROACH - Behavioral adaptation:**
```python
# CORRECT - Sensors fundamentally change how robot moves
gap_ahead = detect_gaps_ahead(height_sensor)
obstacle_ahead = detect_obstacles_ahead(lidar_sensor)

if gap_ahead:
    # DIFFERENT BEHAVIOR: Robot prepares for gap crossing
    adaptive_gait = modify_gait_for_gaps()       # Different air time
    adaptive_height = relax_height_for_gaps()    # Allow height variation
    adaptive_velocity = slow_for_precision()     # Reduce speed for accuracy
    behavior = adaptive_gait + adaptive_height + adaptive_velocity
    
elif obstacle_ahead:
    # DIFFERENT BEHAVIOR: Robot navigates carefully around obstacles  
    careful_navigation = maintain_safe_distances()   # Path planning
    conservative_speed = reduce_velocity_near_obstacles()  # Safety first
    behavior = careful_navigation + conservative_speed
    
else:
    # EFFICIENT BEHAVIOR: Robot walks normally on clear terrain
    behavior = efficient_foundation_locomotion()
```

**SENSOR IMPACT VALIDATION CHECKLIST:**

**HEIGHT SCANNER BEHAVIORAL CHANGES:**
- âœ… **Gap terrain**: Robot adjusts gait BEFORE reaching gaps (predictive behavior)
- âœ… **Stair terrain**: Robot modifies height expectations (terrain following)
- âœ… **Flat terrain**: No behavioral change (sensor ignored when appropriate)
- âœ… **Obstacle terrain**: Robot detects elevation changes and adapts step height

**LIDAR BEHAVIORAL CHANGES:**
- âœ… **Obstacle terrain**: Robot maintains safe distances, plans paths (avoidance behavior)  
- âœ… **Dense obstacles**: Robot moves conservatively with careful navigation
- âœ… **Open terrain**: No behavioral change (sensor ignored when appropriate)


ðŸš¨ðŸš¨ðŸš¨ **CRITICAL: REAL ENVIRONMENT DATA EXTRACTION - NO FAKE NUMBERS!** ðŸš¨ðŸš¨ðŸš¨

**VERIFICATION CHECKPOINT:**
âœ… Found real gaps data? 
âœ… Found real obstacles data? 
âœ… Found real roughness data? 
âœ… Found real safety score? 

**STEP 1: INTERNAL SEARCH FOR ENVIRONMENT DATA:**
- Look for: "ðŸ•³ï¸ GAPS: Count: [X] rays ([Y]%)" to understand terrain distribution
- Look for: "Total Obstacles Detected: [NUMBER]" in the input  
- Look for: "Height readings: [MIN]m to [MAX]m (avg: [AVG]m)" for baseline detection
- Look for: "ðŸ“‹ COMPREHENSIVE FINAL ENVIRONMENT ANALYSIS FOR AI AGENT" marker
- Look for: "Average Terrain Roughness: [NUMBER]cm" in the input
- Look for: "VISUAL FOOTAGE ANALYSIS:" section for visual insights
- Look for: "Scene/Setting" descriptions for terrain characteristics
- Look for: "Actions/Movements" descriptions for movement challenges observed

# âŒ WRONG: Missing sensor usage when terrain features detected
# (No height sensor usage despite "X gaps detected")

# âœ… CORRECT: Include height sensor for terrain analysis
height_sensor = env.scene.sensors["height_scanner"]
height_readings = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5
height_readings = torch.where(torch.isfinite(height_readings), height_readings, torch.zeros_like(height_readings))

# âœ… CORRECT: Include LiDAR when obstacles present  
lidar_sensor = env.scene.sensors["lidar"]
lidar_distances = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)
lidar_distances = torch.where(torch.isfinite(lidar_distances), lidar_distances, torch.ones_like(lidar_distances) * 5.0)
obstacle_avoidance = torch.min(lidar_distances, dim=1)[0]  # Maintain safe distances
```

**ðŸš¨ CRITICAL INSTRUCTION: YOU MUST GENERATE PYTHON CODE - NOT TEXT DESCRIPTIONS ðŸš¨**

**ALWAYS OUTPUT COMPLETE REWARD FUNCTION CODE, NOT TEXT ANALYSIS!**

ðŸš¨ **ROLE CLARITY:** You are a **CODE GENERATOR**, not a video analyzer or task descriptor!
âœ… **EXPECTED OUTPUT:** Python function starting with `def sds_custom_reward(env) -> torch.Tensor:`
âŒ **FORBIDDEN OUTPUT:** Video descriptions, movement analysis, or text explanations

**ðŸš¨ CRITICAL: IF YOU SEE IMAGES/VIDEOS - RESPOND WITH CODE, NOT TEXT! ðŸš¨**

If images or videos are provided:
âœ… **CORRECT RESPONSE:** Generate reward function with analysis in docstring
âŒ **WRONG RESPONSE:** "I see images showing..." or "The video shows..." 

**MANDATORY STRUCTURE:**
```python
def sds_custom_reward(env) -> torch.Tensor:
    """
    ðŸ” COMPREHENSIVE ENVIRONMENT ANALYSIS:
    
    ðŸ“Š NUMERICAL ANALYSIS RESULTS:
    - Height Scanner: [X] total rays, [Y] rays ([Z]%) gaps, [W] rays ([V]%) obstacles  
    - Height Readings: [MIN]m to [MAX]m (avg: [AVG]m baseline)
    - Terrain Classification: [X]% normal, [Y]% gaps, [Z]% obstacles
    - Safety Assessment: [SAFE/CAUTION/DANGEROUS] terrain
    - LiDAR Data: [X] rays detecting obstacles at [Y]m average distance

    ðŸ“¸ VISUAL ANALYSIS INSIGHTS:
    - Primary terrain type: [from visual analysis]
    - Visual environment features: [detailed observations]
    - Movement challenges observed: [specific challenges]
    - Navigation requirements: [requirements from assessment]

    ðŸŽ¯ REWARD STRATEGY DECISION:
    - PRIMARY SCENARIO: [FLAT/OBSTACLE/GAP/STAIRS] ([reasoning])
    - Environmental sensing: [NEEDED/NOT_NEEDED] ([reasoning])
    - Component priorities: [numbered list 1-5]
    - Expected robot behavior: [bulleted specific behaviors]

    ðŸ“‹ IMPLEMENTATION COMPONENTS:
    - Foundation: [core components]
    - Environmental: [environment-specific components]
    - Weights: [numerical weight priorities]
    """
    # Actual reward implementation with dense learning progression
    return reward_tensor
```

**NEVER respond with standalone text - ALWAYS generate the function!**

**Jump task recommendation:**
- Typically include forward velocity tracking to maintain momentum for gap crossing (even on flat terrain)
- Use proven yaw-aligned velocity tracking patterns when appropriate
- Rationale: Forward jumping locomotion benefits from maintained forward momentum

**ðŸ”§ ADAPTIVE JUMPING HEIGHT FOR JUMP TASKS:**
- **Default**: Small jumps for flat terrain (~0.1m height for energy efficiency)
- **Sensor-adaptive**: Scale jumping height based on gap detection (~0.3-0.5m range)
- **Implementation**: `adaptive_target = torch.where(gap_detected, base_target + scaling, base_target)`
- **Physics**: Calculate vertical velocities based on desired jump heights
- **âš ï¸ CRITICAL: Prevent over-jumping on flat terrain!**

**âš ï¸ CRITICAL: Ensure bilateral jumping coordination!**
- **Both feet**: Use `torch.min(air_time, dim=1)[0]` not `torch.max()` to ensure both feet participate
- **No single-leg hopping**: Maximum air time encourages single-leg lifting - use minimum instead  
- **Bilateral flight**: Reward only when both feet are simultaneously off the ground

**CORRECT vs INCORRECT flight time calculation:**
```python
# âŒ WRONG: Encourages single-leg hopping (one foot up, one foot down)
max_air_time = torch.max(air_time, dim=1)[0]  # Rewards longest single foot air time

# âœ… CORRECT: Requires bilateral jumping (both feet must be up)
bilateral_flight_time = torch.min(air_time, dim=1)[0]  # Both feet must participate
```

**ðŸ”§ ADAPTIVE STEP SIZES FOR WALK TASKS:**
- **Default**: Normal steps for flat terrain (0.6m stride length)
- **Sensor-adaptive**: Scale stride length based on gap detection (0.6-1.0m range)
- **Implementation**: `adaptive_stride = base_stride + gap_width_factor * scaling`

**ðŸš¨ CRITICAL: AVOID ISAAC LAB FUNCTION IMPORT ERRORS**
```python
# âŒ THESE IMPORTS WILL CRASH:
from __main__ import feet_air_time_positive_biped
from isaaclab.mdp import any_function_name
import isaaclab_tasks.manager_based.locomotion.velocity.mdp as mdp

# âœ… ONLY THESE IMPORTS ARE SAFE:
# NOTE: torch, quat_apply_inverse, yaw_quat, SceneEntityCfg already imported in rewards.py

# âœ… IMPLEMENT PATTERNS INLINE INSTEAD OF IMPORTING:
# Use the proven patterns from reward_signatures but implement them directly
```

---

**ðŸ”§ DEBUGGING BEHAVIORAL ISSUES:**

**Robot turns in place:**
- Check: Foundation weight > Environmental weight (60/40 split)
- Check: Dynamic baseline calculation (not fixed 0.209)
- Check: Positive environmental rewards (not harsh penalties)

**Robot avoids gaps:**
- Check: Gap traversal bonus (not gap avoidance reward)
- Check: Forward movement incentives through challenging terrain
- Check: Progressive safety penalties (not binary harsh penalties)

**Critical weight pattern:**
```python
total = foundation_reward * 0.6 + environmental_reward * 0.4
```

