üåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåü
üåüüåüüåü ENVIRONMENT-AWARE LOCOMOTION REWARD DESIGN GUIDANCE üåüüåüüåü
üåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåü

**CRITICAL PROJECT UNDERSTANDING:**

These guidance explanations are designed for creating **environment-aware locomotion rewards** that enable **intelligent sensor-driven behavior adaptation** for humanoid robots.

How to use this tip (guidance, not prescriptions):
- **CRITICAL: INCLUDE TERRAIN CLASSIFICATION**: Always extract and include the `TERRAIN_CLASS: [0|1|2|3]` from environment image analysis in your reward function docstring
- If the SUS includes `TERRAIN_CLASS: [0|1|2|3]`, use it to choose families of components (0 SIMPLE foundation-only; 1 GAP height-based handling; 2 OBSTACLES LiDAR safety; 3 STAIRS relaxed-height). Decide specific components/thresholds/weights from the analysis rather than hardcoding.
- **MANDATORY**: Include terrain classification reasoning based on environment image visual analysis
- Keep rewards additive + clamped; include a small baseline (~+0.1); clamp final total (e.g., [0.0, 5.0]). For SIMPLE terrain you MAY use bounded exponential kernels for velocity/yaw tracking (exp-shaped but clamped to [0,1]) to improve natural gait formation.
- Use direct sensor access; avoid non-existent attributes; validate shapes.
- Treat examples as patterns; do not copy constants verbatim.

**üéØ TERRAIN NAVIGATION STRATEGY FLEXIBILITY:**

**GPT DECISION AUTONOMY:**
- **Gap Terrain**: Robot can go through gaps, step over them, avoid them entirely, etc. - GPT decides optimal strategy
- **Obstacle Terrain**: Robot can navigate around obstacles, step over small ones, find alternative paths, etc.
- **Stair Terrain**: Robot can climb up, descend carefully, use handrails if available, etc.
- **Simple Terrain**: Robot can focus on speed, efficiency, energy conservation, etc.

**STRATEGY SELECTION PRINCIPLES:**
- Use environmental sensor data to inform strategy choice (height scanner for gap assessment, LiDAR for obstacle detection, etc.)
- Consider robot capabilities and safety constraints
- Adapt behavior based on detected terrain characteristics
- No strict rules - let environmental analysis guide decisions

**ENVIRONMENT-AWARE SENSOR UTILIZATION:**
- **CRITICAL**: Sensor usage must create measurable performance improvements vs. foundation-only mode
- Use height scanner for terrain analysis, gap detection, surface assessment, etc.
- Use LiDAR for obstacle avoidance, path planning, safety assessment, etc.
- Combine sensor data for intelligent decision making, adaptive behavior, contextual responses, etc.
- **Goal**: 15-30% better performance metrics when sensors are actively utilized vs. disabled

**ADAPTIVE BEHAVIOR EXAMPLES:**
- Gap terrain: Sensor-guided step length adjustment, clearance assessment, landing prediction, etc.
- Obstacle terrain: Dynamic path planning, safe distance maintenance, collision avoidance, etc.
- Variable terrain: Surface adaptation, height adjustment, stability optimization, etc.

**üéØ PRIMARY OBJECTIVE:** 
Create reward functions that make **measurable positive impact when sensors are used** compared to **without sensor usage**.

**üî¨ RESEARCH PURPOSE:**
Enable comparison studies demonstrating:
- **WITHOUT SENSORS:** Basic locomotion behavior 
- **WITH SENSORS:** Adaptive, context-aware behavior that responds to environmental challenges

**üß† DESIGN METHODOLOGY:**
1. **EXTRACT:** Get the pre-analyzed environment data from input (gaps, obstacles, terrain complexity)
2. **THINK:** Determine which sensors are needed and how they should influence behavior
3. **DECIDE:** Implement context-aware behavioral switching (NOT simultaneous conflicting rewards)
4. **IMPACT:** Ensure sensors create observable behavioral differences for scientific comparison

**üìä SUCCESS CRITERIA:**
‚úÖ Robot behaves measurably different with sensors vs. without sensors
‚úÖ Sensor-enabled robot adapts to environmental challenges more effectively
‚úÖ Clear behavioral switching based on environmental context
‚úÖ No conflicting simultaneous behaviors (e.g., walking + jumping simultaneously)

**‚ö†Ô∏è FAILURE INDICATORS:**
‚ùå Robot behaves identically with/without sensors
‚ùå Sensors provide only minor bonuses without changing core behavior
‚ùå Conflicting reward objectives that confuse the policy

---


‚ö†Ô∏è CRITICAL FOR REWARD FUNCTIONS: All examples use PHYSICAL [meters] sensor values!

‚úÖ This approach follows Isaac Lab conventions and provides physically meaningful reward functions!

üî¨ **HEIGHT SENSOR SPECIFICATIONS (Enhanced Configuration):**
- **Total rays**: 567 rays (27√ó21 grid pattern)
- **Coverage**: 2.0m forward √ó 1.5m lateral  
- **Resolution**: 7.5cm spacing between rays
- **Range**: 3.0m maximum distance
- **Baseline**: Dynamic calculation per environment (0.209m fallback only)
- **Formula**: `height_reading = sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - 0.5`

üîß **CONTACT SENSOR DEFAULT (G1):**
- **‚ùå CRITICAL: ONLY foot bodies available**: `.*_ankle_roll_link`
- **Contact detection threshold**: 50.0 N (default; align with contact plotting/play scripts)
- **‚ö†Ô∏è DO NOT USE**: `torso_link`, `base_link`, `trunk_link` with contact sensor (WILL CRASH!)
- Example code:
```python
forces = env.scene.sensors["contact_forces"].data.net_forces_w[:, foot_ids, :]
force_mag = forces.norm(dim=-1)
contacts_mask = force_mag > 50.0
```

üöß **COLLISION SENSOR FOR UPPER BODY OBSTACLE DETECTION:**
- **Available sensor**: `collision_sensor` (comprehensive) OR `torso_contact` (limited)
- **Upper body collision threshold**: 300.0 N (high-force impacts only)
- **‚úÖ G1 UPPER BODY COLLISION MONITORING**: Monitor ONLY upper body parts for obstacle/terrain collisions
- **G1 Upper Body Parts (Exact Names)**:
  ```python
  # G1 TORSO COLLISIONS - Body hitting obstacles/terrain
  "pelvis"                     # Main pelvis contact
  "torso_link"                 # Main torso contact
  "pelvis_contour_link"        # Pelvis contour contact
  
  # G1 ARM COLLISIONS - Arms hitting obstacles/terrain  
  "left_shoulder_pitch_link"   # Left shoulder collisions
  "right_shoulder_pitch_link"  # Right shoulder collisions
  "left_shoulder_roll_link"    # Left shoulder roll collisions
  "right_shoulder_roll_link"   # Right shoulder roll collisions
  "left_shoulder_yaw_link"     # Left shoulder yaw collisions
  "right_shoulder_yaw_link"    # Right shoulder yaw collisions
  "left_elbow_pitch_link"      # Left elbow collisions
  "right_elbow_pitch_link"     # Right elbow collisions
  "left_elbow_roll_link"       # Left elbow roll collisions
  "right_elbow_roll_link"      # Right elbow roll collisions
  
  # G1 HAND COLLISIONS - Hands hitting obstacles/terrain
  "left_palm_link"             # Left palm collisions
  "right_palm_link"            # Right palm collisions
  ```
- **‚ùå EXCLUDE LEG LINKS**: DO NOT monitor legs for obstacle collisions (normal locomotion contact)
- **Usage Pattern**:
  ```python
  # Access collision sensor (prioritize collision_sensor over torso_contact)
  collision_sensor = env.scene.sensors.get("collision_sensor") or env.scene.sensors.get("torso_contact")
  
  # Get upper body force data with history for peak detection
  peak_forces = collision_sensor.data.net_forces_w_history.norm(dim=-1).max(dim=1)[0]
  
  # Find upper body collision body IDs using exact G1 names
  upper_body_parts = ["pelvis", "torso_link", "left_shoulder_pitch_link", 
                     "right_shoulder_pitch_link", "left_shoulder_roll_link",
                     "right_shoulder_roll_link", "left_shoulder_yaw_link", 
                     "right_shoulder_yaw_link", "left_elbow_pitch_link",
                     "right_elbow_pitch_link", "left_elbow_roll_link",
                     "right_elbow_roll_link", "left_palm_link", "right_palm_link"]
  collision_body_ids = []
  for body_name in upper_body_parts:
      if body_name in robot.body_names:
          collision_body_ids.append(robot.body_names.index(body_name))
  
  # Detect high-force collisions (300N threshold for significant impacts)
  collision_forces = peak_forces[:, collision_body_ids]
  collision_mask = collision_forces > 300.0
  collision_count = torch.sum(collision_mask, dim=1)
  
  # Apply collision penalty (obstacle/terrain impacts)
  collision_penalty = -collision_count.float() * 0.2  # Penalty for body collisions
  ```

üéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØ
üéØ SENSOR-DRIVEN BEHAVIORAL ADAPTATION: CRITICAL PROJECT REQUIREMENT üéØ
üéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØ

**PROJECT GOAL: SENSORS MUST MEASURABLY CHANGE ROBOT BEHAVIOR!**

**COMPARISON STUDY REQUIREMENT:**
- **Without sensors**: Generic walking that fails on challenging terrain
- **With sensors**: Adaptive behavior that succeeds on challenging terrain

**‚ùå INADEQUATE APPROACH - Minimal sensor bonus:**
```python
# WRONG - Adding tiny sensor bonuses doesn't change robot behavior
foundation = velocity + height + gait  # 7.0 total reward
sensor_bonus = 0.1  # Negligible 1.4% impact
total = foundation + sensor_bonus  # Robot still walks the same way!
```

**‚úÖ REQUIRED APPROACH - Behavioral adaptation:**
```python
# CORRECT - Sensors fundamentally change how robot moves
gap_ahead = detect_gaps_ahead(height_sensor)
obstacle_ahead = detect_obstacles_ahead(lidar_sensor)

if gap_ahead:
    # DIFFERENT BEHAVIOR: Robot prepares for gap crossing
    adaptive_gait = modify_gait_for_gaps()       # Different air time
    adaptive_height = relax_height_for_gaps()    # Allow height variation
    adaptive_velocity = slow_for_precision()     # Reduce speed for accuracy
    behavior = adaptive_gait + adaptive_height + adaptive_velocity
    
elif obstacle_ahead:
    # DIFFERENT BEHAVIOR: Robot navigates carefully around obstacles  
    careful_navigation = maintain_safe_distances()   # Path planning
    conservative_speed = reduce_velocity_near_obstacles()  # Safety first
    behavior = careful_navigation + conservative_speed
    
else:
    # EFFICIENT BEHAVIOR: Robot walks normally on clear terrain
    behavior = efficient_foundation_locomotion()
```

**SENSOR IMPACT VALIDATION CHECKLIST:**

**HEIGHT SCANNER BEHAVIORAL CHANGES:**
- ‚úÖ **Gap terrain**: Robot adjusts gait BEFORE reaching gaps (predictive behavior)
- ‚úÖ **Stair terrain**: Robot modifies height expectations (terrain following)
- ‚úÖ **Flat terrain**: No behavioral change (sensor ignored when appropriate)
- ‚úÖ **Obstacle terrain**: Robot detects elevation changes and adapts step height

**LIDAR BEHAVIORAL CHANGES:**
- ‚úÖ **Obstacle terrain**: Robot maintains safe distances, plans paths (avoidance behavior)  
- ‚úÖ **Dense obstacles**: Robot moves conservatively with careful navigation
- ‚úÖ **Open terrain**: No behavioral change (sensor ignored when appropriate)
- üîß **Max Range**: 5.0m - infinite readings indicate obstacle-free space beyond range


üö®üö®üö® **CRITICAL: REAL ENVIRONMENT DATA EXTRACTION - NO FAKE NUMBERS!** üö®üö®üö®

**VERIFICATION CHECKPOINT:**
‚úÖ Found real gaps data? 
‚úÖ Found real obstacles data? 
‚úÖ Found real roughness data? 
‚úÖ Found real safety score? 

**STEP 1: INTERNAL SEARCH FOR ENVIRONMENT DATA:**
- Look for: "üï≥Ô∏è GAPS: Count: [X] rays ([Y]%)" to understand terrain distribution
- Look for: "Total Obstacles Detected: [NUMBER]" in the input  
- Look for: "Height readings: [MIN]m to [MAX]m (avg: [AVG]m)" for baseline detection
- Look for: "üìã COMPREHENSIVE FINAL ENVIRONMENT ANALYSIS FOR AI AGENT" marker
- Look for: "Average Terrain Roughness: [NUMBER]cm" in the input
- Look for: "VISUAL FOOTAGE ANALYSIS:" section for visual insights
- Look for: "Scene/Setting" descriptions for terrain characteristics
- Look for: "Actions/Movements" descriptions for movement challenges observed

# ‚ùå WRONG: Missing sensor usage when terrain features detected
# (No height sensor usage despite "X gaps detected")

# ‚úÖ CORRECT: Include height sensor for terrain analysis
height_sensor = env.scene.sensors["height_scanner"]
height_readings = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5

# ‚úÖ CORRECT: Include LiDAR when obstacles present  
lidar_sensor = env.scene.sensors["lidar"]
lidar_distances = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)
# CRITICAL: Handle 5.0m max range - infinite values mean no obstacles detected
lidar_distances = torch.where(torch.isfinite(lidar_distances), lidar_distances, torch.tensor(5.0, device=lidar_distances.device))
obstacle_avoidance = torch.min(lidar_distances, dim=1)[0]  # Maintain safe distances
```

**üö® CRITICAL INSTRUCTION: YOU MUST GENERATE PYTHON CODE - NOT TEXT DESCRIPTIONS üö®**

**ALWAYS OUTPUT COMPLETE REWARD FUNCTION CODE, NOT TEXT ANALYSIS!**

üö® **ROLE CLARITY:** You are a **CODE GENERATOR**, not a video analyzer or task descriptor!
‚úÖ **EXPECTED OUTPUT:** Python function starting with `def sds_custom_reward(env) -> torch.Tensor:`
‚ùå **FORBIDDEN OUTPUT:** Video descriptions, movement analysis, or text explanations

**üö® CRITICAL: IF YOU SEE IMAGES/VIDEOS - RESPOND WITH CODE, NOT TEXT! üö®**

If images or videos are provided:
‚úÖ **CORRECT RESPONSE:** Generate reward function with analysis in docstring
‚ùå **WRONG RESPONSE:** "I see images showing..." or "The video shows..." 

**MANDATORY STRUCTURE:**
```python
def sds_custom_reward(env) -> torch.Tensor:
    """
    üîç COMPREHENSIVE ENVIRONMENT ANALYSIS:
    
    üèûÔ∏è TERRAIN CLASSIFICATION:
    - TERRAIN_CLASS: [0|1|2|3] ([SIMPLE/GAP/OBSTACLES/STAIRS])
    - Classification reasoning: [brief justification from environment image analysis]
    - Sensor data confidence: [HIGH/MEDIUM/LOW] ([reasoning])
    
    üìä NUMERICAL ANALYSIS RESULTS:
    - Height Scanner: [X] total rays, [Y] rays ([Z]%) gaps, [W] rays ([V]%) obstacles  
    - Height Readings: [MIN]m to [MAX]m (avg: [AVG]m baseline)
    - Terrain Classification: [X]% normal, [Y]% gaps, [Z]% obstacles
    - Safety Assessment: [SAFE/CAUTION/DANGEROUS] terrain
    - LiDAR Data: [X] rays detecting obstacles at [Y]m average distance

    üì∏ VISUAL ANALYSIS INSIGHTS:
    - Primary terrain type: [from visual analysis]
    - Visual environment features: [detailed observations]
    - Movement challenges observed: [specific challenges]
    - Navigation requirements: [requirements from assessment]

    üéØ REWARD STRATEGY DECISION:
    - PRIMARY SCENARIO: [FLAT/OBSTACLE/GAP/STAIRS] ([reasoning])
    - Environmental sensing: [NEEDED/NOT_NEEDED] ([reasoning])
    - Component priorities: [numbered list 1-5]
    - Expected robot behavior: [bulleted specific behaviors]

    üìã IMPLEMENTATION COMPONENTS:
    - Foundation: [core components]
    - Environmental: [environment-specific components]
    - Weights: [numerical weight priorities]
    """
    # Actual reward implementation with dense learning progression
    return reward_tensor
```

**NEVER respond with standalone text - ALWAYS generate the function!**

**Jump task recommendation:**
- Typically include forward velocity tracking to maintain momentum for gap crossing (even on flat terrain)
- Use proven yaw-aligned velocity tracking patterns when appropriate
- Rationale: Forward jumping locomotion benefits from maintained forward momentum

**üîß ADAPTIVE JUMPING HEIGHT FOR JUMP TASKS:**
- **Default**: Small jumps for flat terrain (energy efficient baseline)
- **Sensor-adaptive**: Scale jumping height based on detected gap sizes from environmental analysis
- **Implementation**: `adaptive_target = torch.where(gap_detected, base_target + gap_based_scaling, base_target)`
- **Physics**: Calculate vertical velocities based on desired jump heights from sensor data
- **‚ö†Ô∏è CRITICAL: Prevent over-jumping on flat terrain!**

**‚ö†Ô∏è CRITICAL: Ensure bilateral jumping coordination!**
- **Both feet**: Use `torch.min(air_time, dim=1)[0]` not `torch.max()` to ensure both feet participate
- **No single-leg hopping**: Maximum air time encourages single-leg lifting - use minimum instead  
- **Bilateral flight**: Reward only when both feet are simultaneously off the ground

**CORRECT vs INCORRECT flight time calculation:**
```python
# ‚ùå WRONG: Encourages single-leg hopping (one foot up, one foot down)
max_air_time = torch.max(air_time, dim=1)[0]  # Rewards longest single foot air time

# ‚úÖ CORRECT: Requires bilateral jumping (both feet must be up)
bilateral_flight_time = torch.min(air_time, dim=1)[0]  # Both feet must participate
```

**üîß ADAPTIVE STEP SIZES FOR WALK TASKS:**
- **Default**: Normal steps for flat terrain (nominal stride length)
- **Sensor-adaptive**: Scale stride length based on detected gap sizes from environmental analysis
- **Implementation**: `adaptive_stride = base_stride + detected_gap_scaling_factor`

**üö® CRITICAL: AVOID ISAAC LAB FUNCTION IMPORT ERRORS**
```python
# ‚ùå THESE IMPORTS WILL CRASH:
from __main__ import feet_air_time_positive_biped
from isaaclab.mdp import any_function_name
import isaaclab_tasks.manager_based.locomotion.velocity.mdp as mdp

# ‚úÖ ONLY THESE IMPORTS ARE SAFE:
# NOTE: torch, quat_apply_inverse, yaw_quat, SceneEntityCfg already imported in rewards.py

# ‚úÖ IMPLEMENT PATTERNS INLINE INSTEAD OF IMPORTING:
# Use the proven patterns from reward_signatures but implement them directly
```

---

**üöÄ PERFORMANCE OPTIMIZATION REQUIREMENTS:**

**MANDATORY: Use vectorized operations for 3000+ environments**
- ‚ùå `for i in range(env.num_envs):` ‚Üí EXTREMELY SLOW
- ‚úÖ `torch.where()`, `torch.clamp()`, `tensor.mean()` ‚Üí FAST

**EFFICIENT HEIGHT SCANNER PROCESSING:**
```python
# ‚úÖ FAST: Vectorized processing
valid_mask = torch.isfinite(height_measurements)
baseline = torch.where(valid_mask, height_measurements, torch.zeros_like(height_measurements)).mean(dim=1)

# ‚ùå SLOW: Per-environment loops  
for i in range(env.num_envs):
    vals = height_measurements[i][valid_mask[i]]  # Don't do this!
```

**EFFICIENT CONTACT PROCESSING:**
```python
# ‚úÖ FAST: Batch operations
contact_forces = contact_sensor.data.net_forces_w_history[:, :, foot_ids, :].norm(dim=-1).max(dim=1)[0]

# ‚ùå SLOW: Individual processing
for foot in foot_ids: 
    forces = contact_sensor.data.net_forces_w_history[:, :, foot, :]  # Don't do this!
```

**TARGET: <5 minutes for 500 training iterations**

**üö® CRITICAL: TORCH.CLAMP() SAFETY PATTERNS:**

**COMMON TYPEERROR: clamp() received invalid combination of arguments**
```python
# ‚ùå WRONG: Nested clamp with scalar causes TypeError
result = torch.clamp(1.0 - value / torch.clamp(tolerance, min=1e-3), 0.0, 1.0)
# ERROR: When tolerance is a float, this fails

# ‚úÖ CORRECT: Use Python max() for scalar safety
safe_tolerance = max(tolerance, 1e-3)
result = torch.clamp(1.0 - value / safe_tolerance, 0.0, 1.0)

# ‚úÖ ALTERNATIVE: Convert scalar to tensor first
if isinstance(tolerance, (int, float)):
    tolerance = torch.tensor(tolerance, device=value.device)
safe_tolerance = torch.clamp(tolerance, min=1e-3)
result = torch.clamp(1.0 - value / safe_tolerance, 0.0, 1.0)
```

**SAFE CLAMPING PATTERNS:**
```python
# ‚úÖ Always use torch.clamp for tensors, max/min for scalars
denominator = max(scalar_tolerance, 1e-6)  # Scalar safety
tensor_result = torch.clamp(tensor_input, min=0.0, max=2.0)  # Tensor safety

# ‚úÖ Safe division with clamping
safe_denom = torch.clamp(tensor_denom, min=1e-6)
result = numerator / safe_denom
```

**üîß DEBUGGING BEHAVIORAL ISSUES:**

**Robot turns in place:**
- Check: Foundation weight > Environmental weight (60/40 split)
- Check: Dynamic baseline calculation (not fixed 0.209)
- Check: Positive environmental rewards (not harsh penalties)

**Robot avoids gaps:**
- Check: Gap traversal bonus (not gap avoidance reward)
- Check: Forward movement incentives through challenging terrain
- Check: Progressive safety penalties (not binary harsh penalties)

**Critical weight pattern:**
```python
total = foundation_reward * 0.6 + environmental_reward * 0.4
```

**üèÜ USE PROVEN ISAAC LAB PATTERNS FOR NATURAL WALKING:**

**1. EXPONENTIAL VELOCITY TRACKING:**
```python
# Use exponential kernels instead of linear clamping
vel_yaw = quat_apply_inverse(yaw_quat(robot.data.root_quat_w), robot.data.root_lin_vel_w[:, :3])
lin_vel_error = torch.sum(torch.square(commands[:, :2] - vel_yaw[:, :2]), dim=1)
vel_reward = torch.exp(-lin_vel_error / (0.25)**2)  # Exponential is smoother than clamp
```

**2. PROPER BIPED AIR TIME:**
```python
# Use the proven biped pattern for natural gait
air_time = contact_sensor.data.current_air_time[:, foot_ids]
contact_time = contact_sensor.data.current_contact_time[:, foot_ids]
in_contact = contact_time > 0.0
in_mode_time = torch.where(in_contact, contact_time, air_time)
single_stance = torch.sum(in_contact.int(), dim=1) == 1
gait_reward = torch.min(torch.where(single_stance.unsqueeze(-1), in_mode_time, 0.0), dim=1)[0]
gait_reward = torch.clamp(gait_reward, max=0.3)  # Cap to prevent over-optimization
```

**3. FORCE-BASED SLIDING DETECTION:**
```python
# Use contact forces for accurate sliding detection
contact_forces = contact_sensor.data.net_forces_w_history[:, :, foot_ids, :].norm(dim=-1).max(dim=1)[0]
contacts = contact_forces > 50.0
foot_velocities = robot.data.body_lin_vel_w[:, foot_ids, :2]
slide_penalty = torch.sum(foot_velocities.norm(dim=-1) * contacts, dim=1)
```

**THESE PATTERNS PRODUCE NATURAL WALKING WITH PROPER AIR TIME**

