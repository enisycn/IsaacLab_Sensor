üåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåü
üåüüåüüåü ENVIRONMENT-AWARE LOCOMOTION REWARD DESIGN GUIDANCE üåüüåüüåü
üåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåüüåü

**CRITICAL PROJECT UNDERSTANDING:**

These guidance explanations are designed for creating **environment-aware locomotion rewards** that enable **intelligent sensor-driven behavior adaptation** for humanoid robots.

**üéØ PRIMARY OBJECTIVE:** 
Create reward functions that make **measurable positive impact when sensors are used** compared to **without sensor usage**.

**üî¨ RESEARCH PURPOSE:**
Enable comparison studies demonstrating:
- **WITHOUT SENSORS:** Basic locomotion behavior 
- **WITH SENSORS:** Adaptive, context-aware behavior that responds to environmental challenges

**üß† DESIGN METHODOLOGY:**
1. **ANALYZE:** Understand the environment (gaps, obstacles, terrain complexity)
2. **THINK:** Determine which sensors are needed and how they should influence behavior
3. **DECIDE:** Implement context-aware behavioral switching (NOT simultaneous conflicting rewards)
4. **IMPACT:** Ensure sensors create observable behavioral differences for scientific comparison

**üìä SUCCESS CRITERIA:**
‚úÖ Robot behaves measurably different with sensors vs. without sensors
‚úÖ Sensor-enabled robot adapts to environmental challenges more effectively
‚úÖ Clear behavioral switching based on environmental context
‚úÖ No conflicting simultaneous behaviors (e.g., walking + jumping simultaneously)

**‚ö†Ô∏è FAILURE INDICATORS:**
‚ùå Robot behaves identically with/without sensors
‚ùå Sensors provide only minor bonuses without changing core behavior
‚ùå Conflicting reward objectives that confuse the policy

---

**üö® MANDATORY REWARD FUNCTION FORMAT üö®**

üéØ ISAAC LAB STANDARD: RAW SENSOR ACCESS FOR REWARD FUNCTIONS! üéØ

üö® IMPORTS: torch, quat_apply_inverse, yaw_quat, SceneEntityCfg already available in rewards.py - DO NOT import them!

‚ö†Ô∏è CRITICAL: Isaac Lab reward functions use RAW sensor data with physical measurements in meters!

üö® MANDATORY: ALWAYS START REWARD FUNCTION WITH THIS HEADER FORMAT! üö®

**MANDATORY REWARD FUNCTION HEADER FORMAT:**
```python
def sds_custom_reward(env) -> torch.Tensor:
    """
    ENVIRONMENTAL ANALYSIS DECISION:
    Based on environment analysis: [COPY exact summary from environment analysis data]
    PRIMARY SCENARIO: [FLAT | OBSTACLE | GAP | STAIRS]
    ENVIRONMENTAL SENSING DECISION: [NOT_NEEDED | NEEDED]
    REWARD STRATEGY: [PRIMARY SCENARIO]: [specific strategy description]
    Components: [list specific reward components for this scenario]
    """
```

**EXAMPLE FORMAT:**
```python
def sds_custom_reward(env) -> torch.Tensor:
    """
    ENVIRONMENTAL ANALYSIS DECISION:
    Based on environment analysis: Flat terrain with no gaps or obstacles (0 gaps, 0 obstacles, avg roughness 0.2cm).
    PRIMARY SCENARIO: FLAT
    ENVIRONMENTAL SENSING DECISION: NOT_NEEDED
    REWARD STRATEGY: FLAT: Foundation locomotion only
    Components: velocity tracking, angular velocity tracking, bipedal gait pattern, height maintenance, gentle arm relaxation
    """
```

**JUMP TASK ON FLAT TERRAIN EXAMPLE (SMALL JUMPS ONLY):**
```python
def sds_custom_reward(env) -> torch.Tensor:
    """
    ENVIRONMENTAL ANALYSIS DECISION:
    Based on environment analysis: Flat terrain with no gaps or obstacles (0 gaps, 0 obstacles, avg roughness 0.2cm).
    PRIMARY SCENARIO: FLAT
    ENVIRONMENTAL SENSING DECISION: NOT_NEEDED  
    REWARD STRATEGY: JUMP: Forward velocity tracking + minimal vertical jumping (prevent over-jumping on flat terrain)
    Components: forward velocity tracking, small vertical velocity (1.0 m/s), short flight time (0.1s), height maintenance
    """
    # CRITICAL: Small jumps only on flat terrain
    vertical_vel = robot.data.root_lin_vel_w[:, 2]
    # Calculate appropriate velocity target for small 10cm jumps
    small_jump_velocity_target = calculate_for_small_height()  # Physics-based calculation
    up_effort = torch.clamp(vertical_vel / small_jump_velocity_target, min=0.0, max=1.0)
    
    root_height = robot.data.root_pos_w[:, 2]  
    # Target small height gain (~10cm) for minimal jumping
    baseline_height = standing_height  # Robot's normal standing height
    target_jump_height = 0.1  # 10cm target
    height_gain = torch.clamp((root_height - baseline_height) / target_jump_height, min=0.0, max=1.0)
```

**MIXED TERRAIN EXAMPLE:**
```python
def sds_custom_reward(env) -> torch.Tensor:
    """
    ENVIRONMENTAL ANALYSIS DECISION:
    Based on environment analysis: Mixed terrain with gaps and obstacles (3 gaps, 8 obstacles, varied terrain).
    PRIMARY SCENARIO: OBSTACLE
    ENVIRONMENTAL SENSING DECISION: NEEDED
    REWARD STRATEGY: OBSTACLE: Context-aware switching - obstacle avoidance mode when obstacles detected, gap crossing mode when gaps detected, foundation walking otherwise
    Components: velocity tracking, angular velocity tracking, height maintenance, context-aware environmental adaptation
    """
```

=== ISAAC LAB STANDARD SENSOR ACCESS ===

‚úÖ CORRECT SENSOR ACCESS (Raw Physical Measurements):
```python
# Isaac Lab standard raw sensor access for reward functions
height_sensor = env.scene.sensors["height_scanner"]
lidar_sensor = env.scene.sensors["lidar"]

# Height measurements: terrain height relative to sensor position (in meters)
height_measurements = height_sensor.data.pos_w[:, 2].unsqueeze(1) - height_sensor.data.ray_hits_w[..., 2] - 0.5

# Distance measurements: actual distances to obstacles (in meters)
lidar_distances = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)

# Physical thresholds with clear meaning:
significant_gaps = height_measurements < -0.2    # 20cm below sensor = gap
close_obstacles = lidar_distances < 2.0          # 2m distance = close obstacle
clear_path = lidar_distances > 5.0               # 5m distance = clear path
```

=== PHYSICAL SENSOR RANGES ===

HEIGHT SCANNER: Physical measurements in meters relative to sensor
- Negative values = terrain below sensor level (gaps)
- Positive values = terrain above sensor level (obstacles)  
- Range: [-0.5m to +3.0m] relative to sensor position

LIDAR RANGE: Physical distances in meters to obstacles
- Range: [0.1m to 5.0m] actual sensor distances
- Use meaningful thresholds: 1.5m = close, 5.0m = clear path

üéØ REWARD FUNCTION THRESHOLDS (Physical Units):
```python
# Gap detection (height measurements)
height_measurements < -0.15    # 15cm gap = significant
height_measurements < -0.3     # 30cm gap = major

# Obstacle detection (height measurements)  
height_measurements > 0.5      # 50cm obstacle = moderate
height_measurements > 1.0      # 1m obstacle = tall

# Proximity detection (lidar distances)
lidar_distances < 1.5          # 1.5m = very close
lidar_distances < 3.0          # 3m = close  
lidar_distances > 5.0          # 5m = clear path
lidar_distances > 10.0         # 10m = very clear
```

‚ö†Ô∏è CRITICAL FOR REWARD FUNCTIONS: All examples use PHYSICAL [meters] sensor values!

‚úÖ This approach follows Isaac Lab conventions and provides physically meaningful reward functions!

üéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØ
üéØ SENSOR-DRIVEN BEHAVIORAL ADAPTATION: CRITICAL PROJECT REQUIREMENT üéØ
üéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØ

**PROJECT GOAL: SENSORS MUST MEASURABLY CHANGE ROBOT BEHAVIOR!**

**COMPARISON STUDY REQUIREMENT:**
- **Without sensors**: Generic walking that fails on challenging terrain
- **With sensors**: Adaptive behavior that succeeds on challenging terrain

**‚ùå INADEQUATE APPROACH - Minimal sensor bonus:**
```python
# WRONG - Adding tiny sensor bonuses doesn't change robot behavior
foundation = velocity + height + gait  # 7.0 total reward
sensor_bonus = 0.1  # Negligible 1.4% impact
total = foundation + sensor_bonus  # Robot still walks the same way!
```

**‚úÖ REQUIRED APPROACH - Behavioral adaptation:**
```python
# CORRECT - Sensors fundamentally change how robot moves
gap_ahead = detect_gaps_ahead(height_sensor)
obstacle_ahead = detect_obstacles_ahead(lidar_sensor)

if gap_ahead:
    # DIFFERENT BEHAVIOR: Robot prepares for gap crossing
    adaptive_gait = modify_gait_for_gaps()       # Different air time
    adaptive_height = relax_height_for_gaps()    # Allow height variation
    adaptive_velocity = slow_for_precision()     # Reduce speed for accuracy
    behavior = adaptive_gait + adaptive_height + adaptive_velocity
    
elif obstacle_ahead:
    # DIFFERENT BEHAVIOR: Robot navigates carefully around obstacles  
    careful_navigation = maintain_safe_distances()   # Path planning
    conservative_speed = reduce_velocity_near_obstacles()  # Safety first
    behavior = careful_navigation + conservative_speed
    
else:
    # EFFICIENT BEHAVIOR: Robot walks normally on clear terrain
    behavior = efficient_foundation_locomotion()
```

**SENSOR IMPACT VALIDATION CHECKLIST:**

**HEIGHT SCANNER BEHAVIORAL CHANGES:**
- ‚úÖ **Gap terrain**: Robot adjusts gait BEFORE reaching gaps (predictive behavior)
- ‚úÖ **Stair terrain**: Robot modifies height expectations (terrain following)
- ‚úÖ **Flat terrain**: No behavioral change (sensor ignored when appropriate)

**LIDAR BEHAVIORAL CHANGES:**
- ‚úÖ **Obstacle terrain**: Robot maintains safe distances, plans paths (avoidance behavior)  
- ‚úÖ **Dense obstacles**: Robot moves conservatively with careful navigation
- ‚úÖ **Open terrain**: No behavioral change (sensor ignored when appropriate)

**IMPLEMENTATION STRATEGY FOR BEHAVIORAL DIFFERENCES:**
```python
# Look ahead with sensors to prepare for upcoming challenges
forward_height = height_measurements[:, front_indices]  # Look ahead, not current position
forward_lidar = lidar_distances[:, front_rays]         # Sense what's coming

# Analyze upcoming terrain to decide behavior modification
upcoming_gaps = analyze_gaps_ahead(forward_height)
upcoming_obstacles = analyze_obstacles_ahead(forward_lidar)

# Modify robot behavior based on what sensors detect ahead
if upcoming_gaps:
    prepare_for_gap_crossing()    # Change gait pattern, step timing
if upcoming_obstacles:
    prepare_for_obstacle_navigation()  # Change velocity, path planning
```

üö® **INTERNAL ENVIRONMENT ANALYSIS BEFORE CODE GENERATION** üö®

**STEP 1: INTERNAL SEARCH FOR ENVIRONMENT DATA:**
- Look for: "Total Gaps Detected: [NUMBER]" in the input
- Look for: "Total Obstacles Detected: [NUMBER]" in the input
- Look for: "Average Terrain Roughness: [NUMBER]cm" in the input
- Look for: "VISUAL FOOTAGE ANALYSIS:" section for visual insights
- Look for: "Scene/Setting" descriptions for terrain characteristics
- Look for: "Actions/Movements" descriptions for movement challenges observed

**STEP 2: USE EXACT NUMBERS IN COMPREHENSIVE ANALYSIS COMMENTS:**
```python
def sds_custom_reward(env) -> torch.Tensor:
    """
    üîç COMPREHENSIVE ENVIRONMENT ANALYSIS:
    
    üìä NUMERICAL ANALYSIS RESULTS:
    - Gaps Detected: 32 gaps (3 steppable, 17 jumpable, 12 impossible)
    - Obstacles Detected: 2 obstacles (2 large)
    - Terrain Roughness: 2.6cm (moderate complexity)
    - Safety Score: 88.2% traversable terrain
    
    üéØ REWARD STRATEGY DECISION:
    - PRIMARY SCENARIO: GAP (most prominent feature)
    - Environmental sensing: NEEDED (gaps and obstacles present)  
    - Context-aware priorities: gap crossing mode (primary), obstacle avoidance mode (secondary), foundation walking (default)
    """
    # Your reward implementation here...
   ```

**STEP 3: ENVIRONMENTAL SENSING IMPLEMENTATION:**
- **If Gaps > 0** ‚Üí Include height scanner gap navigation
- **If Obstacles > 0** ‚Üí Include LiDAR-based obstacle avoidance  
- **If Roughness > 5cm** ‚Üí Include terrain adaptation

**COMMON IMPLEMENTATION MISTAKES TO AVOID:**
```python
# ‚ùå WRONG: Rewards gap detection only
gap_reward = gap_detected.float() * 0.5

# ‚úÖ CORRECT: Rewards successful gap crossing
gap_crossing = torch.exp(-torch.abs(robot.data.root_pos_w[:, 2] - target_height) / 0.3)
gap_reward = torch.where(gap_detected, gap_crossing, torch.zeros_like(gap_crossing))

# ‚ùå WRONG: Missing obstacle avoidance when obstacles detected
# (No LiDAR usage despite "X obstacles detected")

# ‚úÖ CORRECT: Include LiDAR when obstacles present
lidar_sensor = env.scene.sensors["lidar"]
lidar_distances = torch.norm(lidar_sensor.data.ray_hits_w - lidar_sensor.data.pos_w.unsqueeze(1), dim=-1)
obstacle_avoidance = torch.min(lidar_distances, dim=1)[0]  # Maintain safe distances
```

**üö® CRITICAL INSTRUCTION: YOU MUST GENERATE PYTHON CODE - NOT TEXT DESCRIPTIONS üö®**

**ALWAYS OUTPUT COMPLETE REWARD FUNCTION CODE, NOT TEXT ANALYSIS!**

üö® **ROLE CLARITY:** You are a **CODE GENERATOR**, not a video analyzer or task descriptor!
‚úÖ **EXPECTED OUTPUT:** Python function starting with `def sds_custom_reward(env) -> torch.Tensor:`
‚ùå **FORBIDDEN OUTPUT:** Video descriptions, movement analysis, or text explanations

**üö® CRITICAL: IF YOU SEE IMAGES/VIDEOS - RESPOND WITH CODE, NOT TEXT! üö®**

If images or videos are provided:
‚úÖ **CORRECT RESPONSE:** Generate reward function with analysis in docstring
‚ùå **WRONG RESPONSE:** "I see images showing..." or "The video shows..." 

**MANDATORY STRUCTURE:**
```python
def sds_custom_reward(env) -> torch.Tensor:
    """
    [Optional: Brief image/video analysis here in docstring]
    ENVIRONMENTAL ANALYSIS DECISION: ...
    REWARD STRATEGY: ...
    """
    # Actual reward implementation
    return reward_tensor
```

**NEVER respond with standalone text - ALWAYS generate the function!**

**üö® JUMP TASK VELOCITY TRACKING REQUIREMENT:**
- **Jump task**: ALWAYS include forward velocity tracking (even on flat terrain)
- **Implementation**: Use proven Isaac Lab velocity tracking patterns
- **Reason**: Jump task trains forward jumping locomotion, not vertical jumping in place

**üîß ADAPTIVE JUMPING HEIGHT FOR JUMP TASKS:**
- **Default**: Small jumps for flat terrain (~0.1m height for energy efficiency)
- **Sensor-adaptive**: Scale jumping height based on gap detection (~0.3-0.5m range)
- **Implementation**: `adaptive_target = torch.where(gap_detected, base_target + scaling, base_target)`
- **Physics**: Calculate vertical velocities based on desired jump heights
- **‚ö†Ô∏è CRITICAL: Prevent over-jumping on flat terrain!**

**‚ö†Ô∏è CRITICAL: Ensure bilateral jumping coordination!**
- **Both feet**: Use `torch.min(air_time, dim=1)[0]` not `torch.max()` to ensure both feet participate
- **No single-leg hopping**: Maximum air time encourages single-leg lifting - use minimum instead  
- **Bilateral flight**: Reward only when both feet are simultaneously off the ground

**CORRECT vs INCORRECT flight time calculation:**
```python
# ‚ùå WRONG: Encourages single-leg hopping (one foot up, one foot down)
max_air_time = torch.max(air_time, dim=1)[0]  # Rewards longest single foot air time

# ‚úÖ CORRECT: Requires bilateral jumping (both feet must be up)
bilateral_flight_time = torch.min(air_time, dim=1)[0]  # Both feet must participate
```

**üîß ADAPTIVE STEP SIZES FOR WALK TASKS:**
- **Default**: Normal steps for flat terrain (0.6m stride length)
- **Sensor-adaptive**: Scale stride length based on gap detection (0.6-1.0m range)
- **Implementation**: `adaptive_stride = base_stride + gap_width_factor * scaling`

**üö® CRITICAL: AVOID ISAAC LAB FUNCTION IMPORT ERRORS**
```python
# ‚ùå THESE IMPORTS WILL CRASH:
from __main__ import feet_air_time_positive_biped
from isaaclab.mdp import any_function_name
import isaaclab_tasks.manager_based.locomotion.velocity.mdp as mdp

# ‚úÖ ONLY THESE IMPORTS ARE SAFE:
# NOTE: torch, quat_apply_inverse, yaw_quat, SceneEntityCfg already imported in rewards.py

# ‚úÖ IMPLEMENT PATTERNS INLINE INSTEAD OF IMPORTING:
# Use the proven patterns from reward_signatures but implement them directly
```

---

