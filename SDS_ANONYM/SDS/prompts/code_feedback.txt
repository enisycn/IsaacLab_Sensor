Please carefully analyze the policy feedback and provide a new, improved reward function that can better solve the task. Some helpful tips for analyzing the policy feedback:

    **ðŸš¨ FIRST PRIORITY: CHECK FOR SPARSE REWARD PROBLEMS ðŸš¨**
    (0) **Sparse Reward Analysis:** If mean reward is consistently low or zero:
        (a) **Zero Reward Problem**: Check if most robots get zero reward most of the time
        (b) **Learning Progression Missing**: Verify robot gets reward for basic movement attempts, not just perfect execution
        (c) **Foundation-First Fix**: Start with simple movement rewards (upward velocity for jumping, forward motion for walking)
        (d) **Progressive Complexity**: Add coordination and perfection rewards only after foundation works
        (e) **Dense Feedback Rule**: Robot should get SOME reward for ANY progress toward goal
        (f) **Example Fix**: Replace `flight_time_perfect_reward` with `upward_movement_reward + height_gain_reward + flight_bonus`

    **ðŸš¨ SECOND PRIORITY: CHECK FOR TRAPPING PROBLEMS ðŸš¨**
    (0.5) **Anti-Trapping Analysis:** If robot converges to unwanted behavior (standing still, minimal effort):
        (a) **Baseline Check**: Verify baseline reward is minimal - just enough to prevent zeros
        (b) **Movement Incentive**: Ensure task attempts are substantially more rewarding than inactivity  
        (c) **Activity Requirement**: Primary rewards should require target behavior, not passive existence
        (d) **Quick Test**: Calculate "doing nothing" vs "attempting task" rewards - attempts should be much better

    (1) If the values for a certain reward component are near identical throughout, then this means RL is not able to optimize this component as it is written. You may consider
        (a) Changing its scale or the value of its temperature parameter
        (b) Re-writing the reward components
        (c) Discarding the reward components
        (d) Add new reward components
    (2) If some reward components' magnitude is significantly larger, then you must re-scale its value to a proper range
    (3) **Simplified Arm Integration:** If arms behave unnaturally, use minimal arm control keeping them near default positions (shoulder_pitch=0.35, elbow=0.87) with small weights (0.05-0.1). Avoid complex multi-joint arm coordination.
    (4) **Foundation-First Stability Analysis:** If training is crashing or mean reward is zero:
        (a) **FIRST**: Check if basic locomotion foundation is stable (height, velocity, orientation)
        (b) **SECOND**: Verify mathematical stability (no aggressive exponentials, tight tolerances, or multiplicative chains)
        (c) **THIRD**: Check if environmental components are conflicting with basic locomotion
        (d) **SOLUTION**: Start with foundation-only reward, then add complexity incrementally
    (5) **Enhanced Environment Reward Issues (Flat-with-Box Config):** If mean reward is consistently zero with sensors available:
        (a) **Sensor Access Verification**: Test height_scanner and lidar data access separately
        (b) **Backward Movement Integration**: Check if negative velocity commands (-0.1 to 0.4 range) work with terrain analysis
        (c) **Velocity-Obstacle Conflict Debugging**: If robot stops near obstacles, check adaptive velocity scaling math
        (d) **Stair vs Gap Classification Issues**: If robot freezes at terrain changes, verify height gradient detection
        (e) **Gap Behavior Switching Problems**: If inconsistent behaviors, check gap size classification thresholds
        (f) **Sensor Mathematical Stability**: Replace complex terrain processing with simple forward/backward clearance checks
        (g) **Foundation First**: Always test basic locomotion (velocity, height, orientation) before adding sensor components
        (h) **Progressive Integration**: Add sensor components incrementally: velocity tracking â†’ basic contact â†’ terrain adaptation
        (i) **Reward Balance Check**: Ensure sensor-based rewards don't overwhelm core locomotion components (max 30% weight)
        (j) **Flat-with-Box Specifics**: Use 13x10 height scanner (130 points, 15cm resolution, 2x1.5m coverage) and 144-ray lidar (8 channels Ã— 18 horizontal rays, 180Â° FOV), not generic sensor assumptions
    (6) **Mathematical Stability Check:** If rewards are consistently zero or training crashes:
        (a) Replace aggressive exponentials (factors >3.0) with moderate ones (0.5-2.0)
        (b) Use relaxed tolerances (0.3-1.0) instead of tight ones (0.1 or smaller)
        (c) Switch from multiplicative to additive reward combinations
        (d) Add baseline bonus (+0.2 to +0.5) to ensure non-zero minimum
        (e) Use safe division patterns and clamp final rewards to min=0.1
    (7) **Advanced Locomotion Pattern Debugging (NEW):** For enhanced environment failures:
        (a) **Velocity-Obstacle Conflicts**: If robot stops at obstacles but velocity commands are non-zero:
            - Check adaptive velocity scaling: `obstacle_factor = torch.clamp((terrain_clearance + 0.2) / 0.4, min=0.1, max=1.0)`
            - Verify terrain clearance calculation from height scanner forward strips
            - Ensure velocity rewards use adapted commands, not original commands
        (b) **Stair Behavior Issues**: If robot behavior is incorrect for STAIRS scenario:
            - Use STAIRS scenario classification from environment analysis
            - Apply stair-specific locomotion rewards based on analysis
        (c) **Gap Behavior Issues**: If behavior is incorrect for GAP scenario:
            - Use GAP scenario classification from environment analysis  
            - Apply gap-specific thresholds from environment analysis
            - Ensure conditional rewards use proper masking: `torch.where(gap_condition.unsqueeze(-1), reward_value, zeros)`
        (d) **Backward Movement Problems**: If negative velocity commands cause issues:
            - Check bidirectional velocity tracking: separate forward/backward error calculation
            - Verify sensor coverage: height scanner and lidar cover backward movement areas
            - Ensure yaw-aligned velocity transformation works with negative commands
        (e) **Sensor Integration Crashes**: If adding height_scanner or lidar causes failures:
            - Test sensor data shapes: height_measurements should be [num_envs, 130], lidar_distances should be [num_envs, 144] 
            - Check grid access: use height_measurements directly (already flattened from 13x10 grid)
            - Verify ray indexing: forward_rays = lidar_distances[:, :72], backward_rays = lidar_distances[:, 72:]
        (f) **Enhanced Environment Specific**: For Isaac-SDS-Velocity-Flat-G1-Enhanced-v0:
            - Use exact sensor configurations: GridPatternCfg (2.0x1.5m, 0.15m resolution), LidarPatternCfg (8 channels, 180Â° FoV, 10Â° resolution)
            - Account for velocity ranges: x: (-0.1, 0.4), y: (-0.15, 0.15), z: (-0.3, 0.3)
            - Include full-body coordination: 23 DOF control (legs, arms, torso) not just legs
Please analyze each existing reward component in the suggested manner above first, and then write the reward function code. 