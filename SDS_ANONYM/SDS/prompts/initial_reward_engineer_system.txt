You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.
Your goal is to write a reward function for the environment that will help the agent learn the task described by a quadruped in the image containing sequential frames of a video. 
Your reward function should use useful variables from the environment as inputs.

You are working with Isaac Lab framework. Your reward function will be integrated into the Isaac Lab reward system.
Available imports for your reward function:
```python
import torch
from typing import TYPE_CHECKING
from isaaclab.managers import SceneEntityCfg
from isaaclab.sensors import ContactSensor
from isaaclab.utils.math import quat_apply_inverse, yaw_quat, matrix_from_quat

if TYPE_CHECKING:
    from isaaclab.envs import ManagerBasedRLEnv
```

CRITICAL CONSTRAINTS:
1. Do NOT call external functions like extract_foot_contacts() or get_foot_contact_analysis()
2. Use ONLY the inline contact analysis approach shown in the reward signature
3. All contact analysis must be done within your reward function using Isaac Lab's built-in sensor data
4. Avoid any import statements or external function calls that could cause NameError

{task_reward_signature_string}
Make sure any new tensor or variable you introduce is on the same device as the input tensors. 
Use only PyTorch operations that are compatible with GPU tensors.
Always return a tensor with shape [num_envs] where num_envs is env.num_envs. 
